---
date: '2025-06-07T16:45:40.620508Z'
tags:
- autogenerated
- source__claude
- conversation_id__6482c91e-50e7-4cea-aa76-40451ddf3faf
- msg_ad0d93bc-31fd-4400-811f-11ca48198a25
- msg_731a281c-1840-43f3-8f92-ae044e281da9
- colorclass/informed-decision
title: fsdp
---

**USER**

fsdp


**ASSISTANT**

The user is asking me to write a wiki entry for "fsdp" in the context of an AI research knowledge base. FSDP likely refers to "Fully Sharded Data Parallel", which is a distributed training technique used in deep learning, particularly for large language models.

The user wants:
- Technical, detailed explanations with mathematical formalisms
- Wiki-style links in double brackets for novel concepts
- Symbolic notation for modeling
- Content suitable for a graduate-level reader in applied math/statistics/philosophy of science
- Physics conventions explained in accessible mathematical terms
- Content formatted as a wiki page, not conversational

I should structure this as a comprehensive wiki entry covering:
1. Definition and overview
2. Mathematical formulation
3. Key algorithms and procedures
4. Comparison with other parallelization strategies
5. Implementation considerations
6. Theoretical foundations

I'll use mathematical notation throughout and provide formal definitions.# Fully Sharded Data Parallel (FSDP)

**Fully Sharded Data Parallel (FSDP)** is a [[distributed training]] technique that enables efficient training of large neural networks by sharding both model parameters and optimizer states across multiple devices while maintaining the computational semantics of standard data parallel training.

## Mathematical Framework

### Parameter Sharding Formalism

Let $\Theta \in \mathbb{R}^d$ represent the complete parameter vector of a neural network, where $d$ is the total parameter dimension. In FSDP, this parameter space is partitioned across $P$ processing units.

Define the sharding function $\mathcal{S}: \mathbb{R}^d \rightarrow \mathbb{R}^{d_1} \times \mathbb{R}^{d_2} \times \cdots \times \mathbb{R}^{d_P}$ where:

$$\mathcal{S}(\Theta) = (\Theta^{(1)}, \Theta^{(2)}, \ldots, \Theta^{(P)})$$

subject to the constraint $\sum_{i=1}^P d_i = d$ and $\bigcup_{i=1}^P \Theta^{(i)} = \Theta$.

Each processing unit $p$ maintains only the shard $\Theta^{(p)} \in \mathbb{R}^{d_p}$, where typically $d_p \approx \frac{d}{P}$ for balanced partitioning.

### All-Gather and Reduce-Scatter Operations

The fundamental communication primitives in FSDP are formalized as:

**All-Gather Operation**: $\mathcal{A}: \mathbb{R}^{d_p} \rightarrow \mathbb{R}^d$
$$\mathcal{A}(\Theta^{(p)}) = \bigoplus_{i=1}^P \text{broadcast}(\Theta^{(i)})$$

where $\bigoplus$ denotes concatenation and $\text{broadcast}(\Theta^{(i)})$ represents the communication of shard $i$ to all processors.

**Reduce-Scatter Operation**: $\mathcal{R}: (\mathbb{R}^d)^P \rightarrow \mathbb{R}^{d_p}$
$$\mathcal{R}((G^{(1)}, G^{(2)}, \ldots, G^{(P)})) = \frac{1}{P}\sum_{i=1}^P G^{(i)}[S_p]$$

where $G^{(i)}$ represents the gradient computed on processor $i$, and $S_p$ denotes the index set corresponding to the parameter shard owned by processor $p$.

## Forward Pass Algorithm

The forward computation requires temporary reconstruction of the complete parameter tensor for each [[module]] computation:

**Algorithm 1**: FSDP Forward Pass
```
Input: Input tensor x, module M with sharded parameters Θ^(p)
1. Θ_full ← AllGather(Θ^(p))  // Reconstruct complete parameters
2. y ← M(x, Θ_full)           // Standard forward computation  
3. Discard Θ_full             // Free memory
4. Return y
```

### Memory Complexity Analysis

The peak memory consumption during forward pass is:

$$M_{\text{peak}} = M_{\text{activations}} + M_{\text{temp\_params}} + M_{\text{owned\_params}}$$

where:
- $M_{\text{activations}}$: Memory for activation tensors
- $M_{\text{temp\_params}} = \mathcal{O}(d \cdot \text{dtype\_size})$: Temporary full parameter reconstruction
- $M_{\text{owned\_params}} = \mathcal{O}(\frac{d}{P} \cdot \text{dtype\_size})$: Permanently owned parameter shard

## Backward Pass and Gradient Communication

### Gradient Computation

The backward pass computes gradients with respect to the temporarily reconstructed parameters:

$$\frac{\partial \mathcal{L}}{\partial \Theta} = \frac{\partial \mathcal{L}}{\partial y} \frac{\partial y}{\partial \Theta}$$

where $\mathcal{L}$ represents the loss function.

### Gradient Aggregation Protocol

The gradient synchronization follows a [[reduce-scatter]] pattern:

**Algorithm 2**: FSDP Backward Pass
```
Input: Loss gradient ∂L/∂y, reconstructed parameters Θ_full
1. g_full ← BackwardComputation(∂L/∂y, Θ_full)
2. g^(p) ← ReduceScatter(g_full)  // Aggregate and shard gradients
3. Update Θ^(p) using g^(p)       // Local parameter update
```

The reduce-scatter operation ensures that processor $p$ receives the averaged gradient for its owned parameter shard:

$$g^{(p)} = \frac{1}{P} \sum_{i=1}^P g_{\text{full}}^{(i)}[S_p]$$

## Optimizer State Sharding

FSDP extends sharding to [[optimizer states]], crucial for memory efficiency with optimizers like [[Adam]] that maintain first and second moment estimates.

For Adam optimizer, the state variables are partitioned as:
- First moments: $m^{(p)} \in \mathbb{R}^{d_p}$
- Second moments: $v^{(p)} \in \mathbb{R}^{d_p}$

The update rule becomes:
$$\Theta^{(p)}_{t+1} = \Theta^{(p)}_t - \alpha \frac{m^{(p)}_t}{\sqrt{v^{(p)}_t} + \epsilon}$$

where all operations are performed element-wise on the local shard.

## Communication Complexity

### Theoretical Analysis

The communication volume per training step scales as:

$$C_{\text{FSDP}} = 2d \cdot \text{dtype\_size} \cdot \log_2(P)$$

This consists of:
- **All-gather cost**: $d \cdot \text{dtype\_size} \cdot \log_2(P)$ for parameter reconstruction
- **Reduce-scatter cost**: $d \cdot \text{dtype\_size} \cdot \log_2(P)$ for gradient aggregation

Compare this to [[Data Parallel]] training:
$$C_{\text{DP}} = d \cdot \text{dtype\_size} \cdot \log_2(P)$$

FSDP incurs exactly 2× the communication overhead of standard data parallel training.

## Hierarchical Sharding Strategy

For models with natural hierarchical structure (e.g., [[transformer layers]]), FSDP can be applied recursively:

$$\Theta = \{\Theta_{\text{layer}_1}, \Theta_{\text{layer}_2}, \ldots, \Theta_{\text{layer}_L}\}$$

Each layer's parameters $\Theta_{\text{layer}_i}$ can be independently sharded across the available processors, enabling fine-grained memory management.

## Comparison with Model Parallelism

| Aspect | FSDP | [[Pipeline Parallelism]] | [[Tensor Parallelism]] |
|--------|------|-------------------------|----------------------|
| Memory per device | $\mathcal{O}(\frac{d}{P})$ | $\mathcal{O}(\frac{d}{P})$ | $\mathcal{O}(d)$ |
| Communication pattern | All-gather/Reduce-scatter | Point-to-point | All-reduce |
| Scalability | High | Bubble overhead | Limited by model width |

## Implementation Considerations

### Gradient Accumulation

When using [[gradient accumulation]] with accumulation steps $A$, the reduce-scatter operation is deferred until the final micro-batch:

```python
for step in range(A):
    # Forward and backward without communication
    local_gradients += compute_gradients(micro_batch[step])

# Single communication after A steps  
averaged_gradients = reduce_scatter(local_gradients / A)
```

### Mixed Precision Integration

FSDP naturally integrates with [[mixed precision training]]. Parameters are sharded in the storage precision (typically `fp16` or `bfloat16`) while computations may use higher precision:

$$\text{Computation}: \mathbb{R}^d_{\text{fp32}} \quad \text{Storage}: \mathbb{R}^{d_p}_{\text{fp16}}$$

### Memory Efficiency Metrics

The memory reduction factor compared to standard training is:

$$R_{\text{memory}} = \frac{P \cdot (1 + \alpha)}{1 + \alpha \cdot P}$$

where $\alpha$ represents the ratio of non-parameter memory (activations, temporary buffers) to parameter memory.

## Theoretical Foundations

FSDP maintains the [[convergence guarantees]] of standard [[SGD]] under the assumption of unbiased gradient estimates:

$$\mathbb{E}[\tilde{g}] = \nabla \mathcal{L}(\Theta)$$

where $\tilde{g}$ represents the FSDP-computed gradient estimate.

The algorithm preserves the [[mathematical equivalence]] to centralized training, making it a transparent optimization for distributed compute environments.


---


following: [[fsdp vs megatron tp+pp]]