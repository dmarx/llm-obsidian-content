---
date: 1718466059.664166
tags:
- autogenerated
- source__oai
- conversation_id__7010348a-395b-49c2-88fd-31ccbe12ed86
- msg_aaa2af8c-181e-45ee-a179-144bb09b65ad
- msg_ff140ad6-734e-4d3a-85b4-f1b2f34ea55b
- colorclass/construct-validity
title: distractorsfoils in selected-response items
---

**USER**

distractors/foils in selected-response items


**ASSISTANT**

In the context of selected-response items, such as multiple-choice questions, distractors (or foils) are the incorrect options provided alongside the correct answer. The quality and effectiveness of these distractors are crucial for the overall validity and reliability of the assessment. Here, we'll delve into the principles of creating effective distractors, their role in assessment, and methods for analyzing their performance.

## Principles of Creating Effective Distractors

### 1. **Plausibility**

Distractors should be plausible to ensure that the item discriminates effectively between students who have mastered the content and those who have not. Implausible distractors can make an item too easy and fail to differentiate between different levels of understanding.

### 2. **Relevance**

Distractors should be related to the content being assessed. They should represent common misconceptions or errors that students might make, thereby providing insight into their understanding and thinking processes.

### 3. **Homogeneity**

All options, including the correct answer and distractors, should be homogeneous in content and structure. This includes having similar lengths, grammatical structures, and complexity levels to prevent test-wise students from identifying the correct answer through non-content-related cues.

### 4. **Mutual Exclusivity**

Distractors should be mutually exclusive, meaning that only one option should be the correct answer. Overlapping options can confuse students and reduce the clarity and effectiveness of the assessment item.

### 5. **Consistency**

The format of distractors should be consistent with the stem (the question or prompt). For example, if the stem is a complete sentence, the distractors should also be complete sentences.

### 6. **Balanced Positioning**

The correct answer should be randomly placed among the distractors to avoid patterns that students might detect. Ideally, the correct answers should be evenly distributed across all positions (e.g., A, B, C, D) throughout the test.

## Role of Distractors in Assessment

### 1. **Diagnostic Information**

Effective distractors can provide valuable diagnostic information by revealing specific misconceptions or gaps in knowledge. Analyzing which distractors are frequently chosen can help educators identify common areas of misunderstanding.

### 2. **Discrimination**

Good distractors contribute to the itemâ€™s ability to discriminate between high- and low-performing students. They ensure that only students who understand the content can consistently choose the correct answer.

### 3. **Reliability and Validity**

The overall reliability and validity of an assessment depend significantly on the quality of its distractors. Well-designed distractors contribute to more reliable and valid measures of student knowledge and skills.

## Methods for Analyzing Distractor Performance

### 1. **Distractor Analysis**

Distractor analysis involves examining the frequency with which each distractor is chosen. Effective distractors should attract a reasonable proportion of students, particularly those who do not know the correct answer. Key metrics include:

- **Distractor Functioning**: Evaluates whether each distractor is functioning as intended. A distractor is functioning well if it is chosen by some students but not so frequently as to indicate a problem with the item.
- **Non-Functioning Distractors**: Distractors that are rarely or never chosen by students may indicate that they are implausible or too easy to eliminate.

### 2. **Difficulty and Discrimination Indices**

- **Item Difficulty (P-value)**: The proportion of students who answered the item correctly. The ideal range for most items is between 0.3 and 0.7.
  $$
  P = \frac{\text{Number of correct responses}}{\text{Total number of responses}}
  $$

- **Discrimination Index (D-value)**: Measures how well an item differentiates between high-performing and low-performing students. Items with a high discrimination index have distractors that effectively attract lower-performing students.
  $$
  D = \frac{P_{\text{high}} - P_{\text{low}}}{2}
  $$
  where \( P_{\text{high}} \) is the proportion of correct responses from the top 27% of students and \( P_{\text{low}} \) is the proportion of correct responses from the bottom 27% of students.

### 3. **Point-Biserial Correlation**

The point-biserial correlation coefficient measures the relationship between performance on a specific item and overall test performance. Effective distractors should have negative point-biserial correlations, indicating that choosing a distractor is associated with lower overall test performance.

$$
r_{pb} = \frac{\overline{X}_1 - \overline{X}_0}{s_X} \sqrt{\frac{pq}{n}}
$$

where:
- \( \overline{X}_1 \) = mean total score for students who chose the correct answer
- \( \overline{X}_0 \) = mean total score for students who chose the distractor
- \( s_X \) = standard deviation of total scores
- \( p \) = proportion of students choosing the correct answer
- \( q \) = proportion of students choosing the distractor
- \( n \) = total number of students

## Example of Effective Distractor Design

### Stem: What is the capital of France?

#### Options:
A. Berlin
B. Madrid
C. Paris
D. Rome

### Analysis:

- **Plausibility**: Berlin, Madrid, and Rome are all plausible options because they are well-known capitals of other European countries.
- **Relevance**: Each distractor is a capital city, which maintains relevance to the question about a capital.
- **Homogeneity**: All options are single-word names of capital cities, ensuring structural similarity.
- **Mutual Exclusivity**: Only one option, Paris, is correct.
- **Consistency**: The format of all options matches the question.
- **Balanced Positioning**: The correct answer (Paris) is placed among the distractors without any discernible pattern.

### Distractor Performance:

During field testing, the following selection rates are observed:

- A (Berlin): 15%
- B (Madrid): 20%
- C (Paris): 55%
- D (Rome): 10%

- **Item Difficulty (P-value)**: 0.55 (indicating moderate difficulty).
- **Distractor Analysis**: Each distractor attracts a reasonable proportion of students, indicating they are plausible and functioning well.
- **Discrimination Index (D-value)**: Calculated based on performance data, showing how well the item differentiates between high- and low-performing students.

In summary, the design of effective distractors is critical for the quality of selected-response items. By ensuring plausibility, relevance, homogeneity, mutual exclusivity, consistency, and balanced positioning, and by conducting thorough distractor analysis, educators can create assessments that accurately measure student understanding and provide valuable diagnostic information.


---

preceding: [[assessment design]]  


following: [[point-biserial correlation]]