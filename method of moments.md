---
date: 1716439613.377604
tags:
- autogenerated
- source__oai
- conversation_id__a10832fe-8f28-4883-86b7-f9c9cdc2ba9b
- msg_aaa2d599-1dca-47c5-a25a-e06011eafefb
- msg_461cba48-cd96-48d6-8b51-060d92fab24a
- colorclass/critical-thinking
title: method of moments
---

**USER**

method of moments


**ASSISTANT**

The **method of moments** is a technique used in statistics to estimate the parameters of a probability distribution. It involves equating sample moments to theoretical moments and solving the resulting equations for the parameters. This method can be applied to a wide range of distributions and is especially useful because it provides a simple and intuitive approach to parameter estimation.

### Steps in the Method of Moments

1. **Identify the moments**:
   - The \(k\)-th moment of a random variable \(X\) about the origin is defined as:
     $$\mu_k' = \mathbb{E}[X^k]$$
   - The \(k\)-th central moment is defined as:
     $$\mu_k = \mathbb{E}[(X - \mu)^k]$$
     where \(\mu = \mathbb{E}[X]\) is the mean of \(X\).

2. **Compute sample moments**:
   - The \(k\)-th sample moment about the origin is:
     $$m_k' = \frac{1}{n} \sum_{i=1}^n X_i^k$$
   - The \(k\)-th sample central moment is:
     $$m_k = \frac{1}{n} \sum_{i=1}^n (X_i - \bar{X})^k$$
     where \(\bar{X} = \frac{1}{n} \sum_{i=1}^n X_i\) is the sample mean.

3. **Equate sample moments to theoretical moments**:
   - For a distribution with parameters \(\theta_1, \theta_2, \ldots, \theta_p\), the theoretical moments \(\mu_k'(\theta_1, \theta_2, \ldots, \theta_p)\) are functions of these parameters. The method of moments involves setting the first \(p\) sample moments equal to the first \(p\) theoretical moments:
     $$m_k' = \mu_k'(\theta_1, \theta_2, \ldots, \theta_p) \quad \text{for} \; k = 1, 2, \ldots, p$$

4. **Solve the system of equations**:
   - Solve the resulting system of equations to obtain the estimates \(\hat{\theta}_1, \hat{\theta}_2, \ldots, \hat{\theta}_p\).

### Example: Estimating Parameters of a Normal Distribution

Suppose we have a sample \(X_1, X_2, \ldots, X_n\) from a normal distribution \(\mathcal{N}(\mu, \sigma^2)\). The goal is to estimate the parameters \(\mu\) (mean) and \(\sigma^2\) (variance).

1. **Theoretical moments**:
   - The first theoretical moment (mean) is \(\mu_1' = \mu\).
   - The second theoretical moment about the origin is \(\mu_2' = \mu^2 + \sigma^2\).

2. **Sample moments**:
   - The first sample moment (sample mean) is \(m_1' = \bar{X}\).
   - The second sample moment about the origin is \(m_2' = \frac{1}{n} \sum_{i=1}^n X_i^2\).

3. **Equate sample moments to theoretical moments**:
   - Set the sample mean equal to the population mean:
     $$\bar{X} = \mu$$
   - Set the second sample moment equal to the second theoretical moment:
     $$\frac{1}{n} \sum_{i=1}^n X_i^2 = \mu^2 + \sigma^2$$

4. **Solve for parameters**:
   - From the first equation, we get the estimate of \(\mu\):
     $$\hat{\mu} = \bar{X}$$
   - Substituting \(\hat{\mu}\) into the second equation, we get:
     $$\frac{1}{n} \sum_{i=1}^n X_i^2 = \bar{X}^2 + \sigma^2$$
     Solving for \(\sigma^2\), we get:
     $$\hat{\sigma}^2 = \frac{1}{n} \sum_{i=1}^n X_i^2 - \bar{X}^2$$

Thus, the method of moments estimates for the parameters of a normal distribution are \(\hat{\mu} = \bar{X}\) and \(\hat{\sigma}^2 = \frac{1}{n} \sum_{i=1}^n X_i^2 - \bar{X}^2\).

### General Properties

- **Simplicity**: The method is straightforward and easy to implement.
- **Bias**: Estimates obtained through the method of moments can be biased, especially for small sample sizes.
- **Consistency**: Under certain conditions, moment estimates are consistent, meaning they converge to the true parameter values as the sample size increases.

The method of moments is a foundational technique in statistics and forms the basis for more complex estimation methods such as the [[Generalized Method of Moments|GMM]].

See also: [[shifted gompertz distribution]]


See also: [[introduction to the weibull distribution]]