---
date: 1716103907.195341
tags:
- autogenerated
- source__oai
- conversation_id__6be115d5-5609-405d-88c9-249134e0c706
- msg_fdb45799-a8e2-4d20-b6a9-87d8f14685a6
- msg_aaa25a87-eed9-424e-8e04-a89d2dbf7886
- msg_86882388-34e1-4c39-8583-7b93ff08c40f
- colorclass/information-theory
title: dually flat structure of binary choice models
---

**ASSISTANT**

## Dually Flat Structure of Binary Choice Models

### Introduction

This study explores the geometry of parametric binary choice models through the lens of [[information geometry]]. We demonstrate that these models form a dually flat manifold characterized by dual connections, derived from the [[Fisher information metric]]. A key result is the equivalence of the canonical divergence and the Kullback–Leibler (KL) divergence for the logit model, distinguishing it uniquely among binary choice models.

### Geometry of the Binary Choice Model

Consider a binary choice model where the dependent variable $y$ takes values in $\{0, 1\}$. Let $x$ be a $d$-dimensional random vector and $\epsilon$ be a random term independent of $x$. The choice probability is given by:
$$
P(y = 1 | x) = F(\beta^T x)
$$
where $F$ is a distribution function. Common examples are the probit model ($F$ is the standard normal CDF) and the logit model ($F$ is the logistic function).

The model set $\mathcal{M}$ is a $d$-dimensional manifold with parameter $\beta$ forming a coordinate system. The Fisher information matrix $I(\beta)$ is:
$$
I(\beta) = E\left[\frac{\partial \log P(y | x, \beta)}{\partial \beta} \frac{\partial \log P(y | x, \beta)}{\partial \beta}^T \right]
$$
This defines a Riemannian metric $g$ on $\mathcal{M}$:
$$
g_{ij}(\beta) = E\left[\frac{\partial^2 \log P(y | x, \beta)}{\partial \beta_i \partial \beta_j}\right]
$$

### Dually Flat Space

A manifold is dually flat if it admits dual affine coordinate systems $(\theta, \eta)$ connected by a potential function $\psi$ such that:
$$
\eta_i = \frac{\partial \psi(\theta)}{\partial \theta_i}
$$
and the metric $g$ is the Hessian of $\psi$:
$$
g_{ij}(\theta) = \frac{\partial^2 \psi(\theta)}{\partial \theta_i \partial \theta_j}
$$

For the binary choice model, we define $\theta = \beta$ and $\eta = E[x | y = 1] - E[x | y = 0]$. The potential function $\psi$ is chosen so that:
$$
g_{ij}(\theta) = \frac{\partial^2 \psi(\theta)}{\partial \theta_i \partial \theta_j}
$$
This structure allows us to introduce dual connections $\nabla$ and $\nabla^*$ such that the manifold is dually flat with respect to these connections.

### Canonical and KL Divergence

For a dually flat space, the canonical divergence $D(p \parallel q)$ is:
$$
D(p \parallel q) = \psi(p) + \psi^*(q) - \sum_{i} p_i \frac{\partial \psi^*(q)}{\partial q_i}
$$
where $\psi^*$ is the Legendre transform of $\psi$. For the logit model, it is shown that the canonical divergence equals the KL divergence:
$$
D_{KL}(p \parallel q) = \sum_{i} p_i \log \frac{p_i}{q_i}
$$
This equivalence holds if and only if $F$ is the logistic distribution.

### Maximum Likelihood Estimation

The maximum likelihood estimator (MLE) $\hat{\beta}$ for the binary choice model maximizes the log-likelihood:
$$
\ell(\beta) = \sum_{i=1}^n \log P(y_i | x_i, \beta)
$$
Geometrically, the MLE can be interpreted as the point where the empirical expectation of the score function is zero. For the logit model, this corresponds to solving:
$$
\sum_{i=1}^n \left( y_i - \frac{1}{1 + e^{-\beta^T x_i}} \right) x_i = 0
$$

### Conclusion

This study highlights the natural geometric structure of binary choice models, emphasizing the logit model's unique properties. The dual flatness and the equivalence of canonical and KL divergences underscore the logit model's suitability for various applications in econometrics and beyond.

### References

- Amari, S.: Differential geometry of curved exponential families-curvature and information loss. Ann. Stat. 10(2), 357–385 (1982).
- Amari, S.: Information Geometry and Its Applications. Springer Japan KK, Tokyo (2016).
- Amari, S., Nagaoka, H.: Methods of Information Geometry. Oxford University Press, Tokyo (2000).

These references provide foundational insights into the differential geometry applied in econometric modeling and information theory.


**USER**

what are the main takeaways wrt geometric intuition


**ASSISTANT**

### Main Takeaways Regarding Geometric Intuition in Binary Choice Models

1. **Dually Flat Manifold Structure**:
   - The set of parametric binary choice models forms a **dually flat manifold**. This means that the manifold has two sets of affine coordinate systems connected by a potential function. These dual coordinates simplify the understanding of the model's geometry.

2. **Affine Coordinates and Potential Function**:
   - The canonical coordinates (typically the parameter vector $\beta$) and their duals (related to the expected sufficient statistics) provide a natural framework for the binary choice models. The potential function $\psi(\beta)$ and its Legendre dual $\psi^*(\eta)$ establish this connection.
   - This duality means that one can use the gradients of these potential functions to move between the primal and dual coordinate systems.

3. **Fisher Information Metric as a Riemannian Metric**:
   - The Fisher information matrix $I(\beta)$ induces a Riemannian metric on the manifold. This metric measures the curvature of the statistical model space, providing a way to quantify the "distance" between different parameter values.

4. **Canonical Divergence and KL Divergence**:
   - In a dually flat space, the canonical divergence $D(p \parallel q)$ between two points $p$ and $q$ on the manifold can be directly related to the KL divergence.
   - For the logit model specifically, the canonical divergence coincides with the KL divergence. This is a unique property that distinguishes the logit model within the class of binary choice models.

5. **Geodesic Paths and Projections**:
   - Geodesic paths in the manifold represent the shortest paths between points in terms of the Fisher information metric. These paths help understand the flow of information and the structure of the parameter space.
   - Projections onto subspaces (such as constraints in hypothesis testing) can be interpreted geometrically, where the orthogonal projection in the Fisher metric corresponds to maximum likelihood estimates under linear constraints.

6. **Interpretation of Maximum Likelihood Estimation (MLE)**:
   - The MLE can be seen as finding the point in the parameter space where the empirical Fisher information is maximized. This point is where the observed data is most "aligned" with the parameterized model.
   - For the logit model, the MLE process is particularly straightforward due to the dual flatness, allowing a direct estimation of parameters without iterative numerical methods.

### Visualization and Intuition

1. **Flatness and Curvature**:
   - Despite the nonlinear appearance of some models, from an information geometric perspective, these models can be viewed as "flat" if the manifold they form has zero curvature with respect to the Fisher information metric. This flatness simplifies the analysis and interpretation of these models.

2. **Dual Connections**:
   - The dual connections $(\nabla, \nabla^*)$ provide a way to navigate the manifold efficiently. One can move in the space of parameters or their expectations using these connections, providing two complementary views of the statistical model.

3. **Affine Subspaces**:
   - When imposing linear constraints (e.g., in hypothesis testing), the geometry tells us that these constraints define affine subspaces. The MLE within these subspaces can be understood as projecting the unconstrained MLE onto these affine subspaces.

### Conclusion

Understanding the dually flat structure and associated geometric properties of binary choice models provides powerful insights into their behavior and simplifies many aspects of their analysis. This geometric intuition helps clarify why certain models (like the logit model) are particularly well-suited for various statistical tasks, offering both theoretical elegance and practical convenience.


---

preceding: [[skip to main content]]  


following: [[dually flat manifold structure in binary choice models]]

See also: [[information theory]]


See also: [[sufficient statistic]]


See also: [[orthogonal projection]]