---
date: 1713749240.882468
tags:
- autogenerated
- source__oai
- conversation_id__5105adc8-e608-40d7-b875-84f3846ef325
- msg_aaa22494-ff7c-449b-92c8-eb71dc624857
- msg_2b39206b-17e8-4001-ae46-2f5e8dcd9838
- colorclass/vector-space
title: positional embeddings
---

**USER**

positional embeddings


**ASSISTANT**

Positional embeddings are a type of embedding used primarily in the field of natural language processing (NLP), particularly in the context of models like the [[Transformer|Transformer model]]. These embeddings encode the positions of tokens within a sequence, providing the model with a way to incorporate the order of tokens into its processing, which is crucial because the Transformer architecture does not inherently process sequential data in order.

### Mathematical Formalism

To understand positional embeddings, it is helpful to examine the mechanism by which they are integrated into a model's input representations. Let $x_i$ denote the token embedding for the $i^{th}$ token in a sequence. The positional embedding for position $i$ is denoted by $p_i$. The input to the model at position $i$, then, is given by the sum of these two embeddings:

$$
e_i = x_i + p_i
$$

where $e_i$ is the final embedding vector representing both the token and its position in the sequence.

### Generating Positional Embeddings

There are generally two approaches to generating positional embeddings: 

1. **Learned Embeddings**: These embeddings are parameters learned by the model during training, similar to how token embeddings are learned. Each position index $i$ has a corresponding embedding vector $p_i$ which is optimized during the training process.

2. **Fixed (Sinusoidal) Embeddings**: Introduced in the original [[Transformer|Transformer paper]], these embeddings use fixed functions of sine and cosine to encode positions. The mathematical definition for each dimension $k$ of the positional encoding vector $p_i$ is:

$$
p_{i,2k} = \sin\left(\frac{i}{10000^{2k/d}}\right)
$$
$$
p_{i,2k+1} = \cos\left(\frac{i}{10000^{2k/d}}\right)
$$

where $d$ is the dimensionality of the embeddings and $k$ ranges over $0, 1, \ldots, d/2 - 1$. This formulation provides a balance between allowing similar embeddings for nearby positions and ensuring that the embeddings are unique for different positions.

### Importance in Models

Positional embeddings are critical in NLP models like the Transformer for several reasons:

- **Sequence Order**: Unlike recurrent neural networks (RNNs) or long short-term memory networks (LSTMs), the Transformer processes input data in parallel rather than sequentially. Positional embeddings reintroduce necessary information about the sequence order, which is crucial for tasks involving syntax and context.

- **Generalization to Longer Sequences**: Fixed embeddings, such as sinusoidal embeddings, allow the model to theoretically handle sequences longer than those seen during training, as the pattern can be extrapolated to any position index.

In the context of advanced machine learning models, positional embeddings enable the effective handling of sequence data in state-of-the-art architectures, ensuring that the positional dynamics are not lost in the shuffle of parallel processing. This mechanism leverages the deep mathematical principles of Fourier series (via sine and cosine) to encode relative and absolute token positions in a highly efficient manner.


---


following: [[rope embeddings]]

See also: [[tokenization and learned input embeddings in llms]]


See also: [[embedding]]


See also: [[how do positional embeddings work what is their purpose how do they satisfy that purpose what's the problem they solve feel free to treat this as a single question rather than several. i'd prefer a un]]