---
date: 1721144109.794701
tags:
- autogenerated
- source__oai
- conversation_id__4f78839e-971a-4476-b886-e0eef3bd4caf
- msg_aaa211bc-c77e-4725-a354-838bae47a462
- msg_a37c0c8c-168b-44f2-b285-0f3ad8851ff9
- colorclass/information-theory
title: jeffreys prior
---

**USER**

Jeffreys Prior


**ASSISTANT**

The Jeffreys prior, named after the British statistician Harold Jeffreys, is a type of non-informative prior used in Bayesian statistics. It is particularly notable for being invariant under reparameterization, which makes it a widely accepted choice in various statistical modeling contexts.

## Definition and Properties

The Jeffreys prior is derived from the Fisher information matrix. For a parameter $\theta$, the Fisher information is given by:

$$
I(\theta) = -\mathbb{E}\left[ \frac{\partial^2 \log L(\theta \mid x)}{\partial \theta^2} \right]
$$

where $L(\theta \mid x)$ is the likelihood function. The Jeffreys prior is then defined as:

$$
\pi(\theta) \propto \sqrt{I(\theta)}
$$

### Application to Specific Distributions

#### Binomial Distribution

For a binomial distribution with parameter $\theta$, representing the probability of success in a Bernoulli trial, the likelihood function for observing $x$ successes in $n$ trials is:

$$
L(\theta \mid x, n) = \theta^x (1 - \theta)^{n-x}
$$

The second derivative of the log-likelihood with respect to $\theta$ is:

$$
\frac{\partial^2 \log L(\theta \mid x)}{\partial \theta^2} = -\frac{x}{\theta^2} - \frac{n-x}{(1 - \theta)^2}
$$

The Fisher information is:

$$
I(\theta) = \mathbb{E}\left[-\frac{\partial^2 \log L(\theta \mid x)}{\partial \theta^2}\right] = \frac{n}{\theta(1 - \theta)}
$$

Thus, the Jeffreys prior for the binomial distribution is:

$$
\pi(\theta) \propto \sqrt{\frac{n}{\theta(1 - \theta)}} \propto \frac{1}{\sqrt{\theta(1 - \theta)}}
$$

This prior is improper because it does not integrate to a finite value over the interval $[0, 1]$. It is defined as:

$$
\pi(\theta) = \frac{1}{\sqrt{\theta(1 - \theta)}}
$$

### Posterior Distribution

When combined with the binomial likelihood, the Jeffreys prior leads to a posterior distribution that is a Beta distribution with parameters $\alpha = x + \frac{1}{2}$ and $\beta = n - x + \frac{1}{2}$:

$$
\pi(\theta \mid x, n) = \text{Beta}(\theta; x + \frac{1}{2}, n - x + \frac{1}{2})
$$

### Implications

- **Invariance**: One of the key properties of the Jeffreys prior is that it is invariant under reparameterization. This means that if we change variables from $\theta$ to some function of $\theta$, say $\phi = g(\theta)$, the Jeffreys prior for $\phi$ is the same as for $\theta$ transformed appropriately.
- **Non-Informative Prior**: The Jeffreys prior is considered non-informative because it aims to provide a neutral, objective prior that does not favor any specific values of the parameter.

### Comparison with Other Priors

- **Uniform Prior**: The uniform prior for $\theta$ in a binomial setting is $\pi(\theta) = 1$. Unlike the Jeffreys prior, it is not invariant under reparameterization and does not take the sample size $n$ into account.
- **Haldane Prior**: The [[Haldane prior]] is $\pi(\theta) \propto \frac{1}{\theta(1 - \theta)}$. While it is also improper and non-informative, it differs from the Jeffreys prior in how it weights different values of $\theta$ and is more extreme at the boundaries ($\theta = 0$ and $\theta = 1$).

### Mathematical Formalization

To formally derive the posterior distribution with the Jeffreys prior for a binomial experiment:

Given the likelihood:

$$
L(\theta \mid x, n) = \theta^x (1 - \theta)^{n-x}
$$

and the Jeffreys prior:

$$
\pi(\theta) \propto \frac{1}{\sqrt{\theta(1 - \theta)}}
$$

The posterior is proportional to:

$$
\pi(\theta \mid x, n) \propto \theta^x (1 - \theta)^{n-x} \cdot \frac{1}{\sqrt{\theta(1 - \theta)}} = \theta^{x - \frac{1}{2}} (1 - \theta)^{(n-x) - \frac{1}{2}}
$$

which is a Beta distribution:

$$
\pi(\theta \mid x, n) = \text{Beta}(\theta; x + \frac{1}{2}, n - x + \frac{1}{2})
$$

## Conclusion

The Jeffreys prior is a powerful tool in Bayesian statistics due to its invariance properties and its non-informative nature. It is particularly useful in applications where an objective prior is desired, and it provides a robust framework for parameter estimation across various statistical models.


---

preceding: [[haldane prior]]  


following: [[uninformative priors]]