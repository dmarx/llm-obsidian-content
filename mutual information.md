---
date: 1714768877.306152
tags:
- autogenerated
- source__oai
- conversation_id__212e3628-2bdc-43ef-a6d3-6e47e5e4dfec
- msg_aaa267ea-a898-4b55-ba53-b05416b69b7b
- msg_65e4f905-e32b-4f2f-9816-ae5f88b1fc67
- colorclass/information-theory
title: mutual information
---

**USER**

mutual information


**ASSISTANT**

Mutual Information (MI) is a fundamental concept in information theory that measures the amount of information shared between two random variables. It quantifies the reduction in uncertainty about one variable given the knowledge of another, providing a measure of the dependence between the variables. This measure is non-negative and symmetric, and it captures both linear and non-linear relationships.

### Definition

For two discrete random variables \(X\) and \(Y\), the mutual information \(I(X; Y)\) is defined as:
$$
I(X; Y) = \sum_{x \in \mathcal{X}} \sum_{y \in \mathcal{Y}} p(x, y) \log \frac{p(x, y)}{p(x) p(y)}
$$
where \(p(x, y)\) is the joint probability distribution function of \(X\) and \(Y\), and \(p(x)\) and \(p(y)\) are the marginal probability distributions of \(X\) and \(Y\) respectively.

For continuous random variables, the definition uses integrals instead of sums:
$$
I(X; Y) = \int_{\mathcal{X}} \int_{\mathcal{Y}} p(x, y) \log \frac{p(x, y)}{p(x) p(y)} \, dx \, dy
$$

### Interpretation and Properties

1. **Reduction of Uncertainty**: Mutual Information measures how much knowing one of the variables reduces uncertainty about the other. It is zero if and only if the variables are independent.

2. **Symmetry**: \(I(X; Y) = I(Y; X)\), indicating that the information \(X\) provides about \(Y\) is the same as \(Y\) provides about \(X\).

3. **Relation to Entropy**: Mutual Information can also be expressed in terms of entropy:
   $$
   I(X; Y) = H(X) + H(Y) - H(X, Y)
   $$
   where \(H(X)\) and \(H(Y)\) are the entropies of \(X\) and \(Y\), and \(H(X, Y)\) is the joint entropy. Alternatively, it can be expressed as:
   $$
   I(X; Y) = H(X) - H(X|Y) = H(Y) - H(Y|X)
   $$
   This shows that MI is the reduction in entropy of \(X\) due to the knowledge of \(Y\) and vice versa.

4. **Data-Driven Applications**: In machine learning, mutual information is used for feature selection by quantifying the amount of information gained about the target variable by knowing a feature. It is preferred over correlation in scenarios where non-linear relationships are significant.

5. **Bounds and Maximum Information**: Mutual Information is bounded by the entropies of the involved variables. Specifically, it is always less than or equal to the minimum of \(H(X)\) and \(H(Y)\), and it reaches its maximum when one variable is a function of the other.

### Applications

- **Machine Learning**: Used in feature selection, clustering, and building predictive models where it helps to understand and utilize dependencies between variables.
- **Biology and Neuroscience**: Applied to understand the information exchange in neural networks and between genes in gene regulatory networks.
- **Cryptography**: Helps in analyzing the information leakage in cryptographic systems.
- **Economics and Sociology**: Analyzes dependencies and interactions between economic indicators or social variables.

Mutual Information is a powerful and versatile tool that provides deeper insights into the relationships between variables across diverse scientific and engineering fields. It addresses limitations of correlation by capturing nonlinear interactions, making it invaluable for comprehensive data analysis.


---

preceding: [[geometric interpretation of [[conditional entropy]]]]  


following: [[geometric interpretation of mutual information]]

See also: [[information theory of complex systems]]


See also: [[phi in integrated information theory (iit)]]


See also: [[computational mechanics, a field that studies the structure and dynamics of information processing in physical systems]]


See also: [[lol what]]


See also: [[adaptive capacity]]


See also: [[tononi-sporns-edelman (tse) complexity]]


See also: [[triangulation protocol]]


See also: [[regulatory tension]]


See also: [[governance studies]]


See also: [[principle of subsidiarity]]


See also: [[geometric interpretation of mutual information]]


See also: [[situated agency]]


See also: [[rate-distortion theory]]


See also: [[informed consent]]


See also: [[information absorption efficiency coefficient analysis]]


See also: [[neuroethics]]


See also: [[conservative logic]]


See also: [[entropic learning theory]]


See also: [[non-separable scales]]


See also: [[empowerment maximization]]


See also: [[information bottleneck theory in deep learning]]


See also: [[intrinsic motivation]]


See also: [[right to mental integrity]]


See also: [[dimensionality reduction]]


See also: [[freedom from coercion and manipulation]]


See also: [[information conservation laws]]


See also: [[shannon's noisy channel coding theorem]]


See also: [[philosophy of information and deep learning training dynamics]]


See also: [[algorithmic information theory]]


See also: [[partial information decomposition]]


See also: [[instrumental convergence]]


See also: [[information flow in physical and statistical systems]]


See also: [[summarize this paper for me]]


See also: [[algorithmic information theory (ait)]]


See also: [[information-optimal neural architecture design]]


See also: [[good regulator theorem]]


See also: [[information theory]]


See also: [[large language models (llms) as cultural technologies]]


See also: [[thermodynamics of statistical learning]]


See also: [[information theory of deep learning]]


See also: [[bounded rationality]]


See also: [[intrinsic motivation and the kuramoto model]]


See also: [[self-awareness as monitoring and responding to cybernetic needs]]


See also: [[koopman operator theory]]


See also: [[has anyone used or studied this objective through the lens of quantifying some kind of informational density of a dataset]]


See also: [[expand on the relationship between mutual information and kl divergence]]


See also: [[empirical risk minimization phase of deep learning]]


See also: [[phase transitions in computation]]


See also: [[data processing inequality]]


See also: [[mental privacy]]


See also: [[intrinsic motivation in dynamical control systems]]


See also: [[stochastic thermodynamics]]


See also: [[channel capacity]]


See also: [[residual entropy]]


See also: [[gene regulatory networks]]


See also: [[human intelligence (humint)]]


See also: [[neural correlates of consciousness]]


See also: [[characteristic compute scale]]


See also: [[mechanistic interpretability]]


See also: [[freedom of thought]]


See also: [[computational epistemology]]


See also: [[i think my confusion fundamentally stems from this earlier bit of analysis]]


See also: [[provide an intuitive interpretation of a scaling parameter whose value is expressed in the units  flops  (parameter  token).]]


See also: [[tell me about partial conditional mutual information from mixed embeddings. be as technical as possible, i have graduate degrees in math and statistics]]


See also: [[random variables]]


See also: [[empowerment maximization and synchronization dynamics]]


See also: [[channel capacity in the context of empowerment]]


See also: [[free entropy theory]]


See also: [[luhmann's systems theory]]


See also: [[representation compression phase of deep learning]]


See also: [[information theory in compressive sensing]]


See also: [[information bottleneck theory]]


See also: [[empowerment maximization and the emergence of collective behavior]]


See also: [[information theory as a framework for understanding life]]


See also: [[emergence of synchronization of behaviors as a manifestation of empowerment maximization]]


See also: [[geometric interpretation]]


See also: [[mutual information formulation of absorption efficiency]]


See also: [[information theory of complex systems by carlos gershenson and nelson fernández (2012)]]


See also: [[fundamental results and theories at the intersection of spectral theory, rkhs theory, and deep learning theory]]


See also: [[rainer mühlhoff’s theory of sealed surfaces]]


See also: [[entropy in information theory]]


See also: [[human right to mental self-determination]]


See also: [[sagawa ueda relation]]


See also: [[thermodynamic depth]]


See also: [[process complexity]]


See also: [[algorithmic governance]]


See also: [[entropic cost of neural parameter acquisition]]


See also: [[opening the black box of deep neural networks via information theory]]


See also: [[empowerment maximization and free energy]]


See also: [[empowerment maximization in the context of complex dynamical systems]]