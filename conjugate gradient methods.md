---
date: 1717825955.464753
tags:
- autogenerated
- source__oai
- conversation_id__ac8c3253-a669-4989-bf7c-c535d52c8140
- msg_aaa2dd8b-f41e-461e-8952-ea4770589e5f
- msg_792e9ba0-8407-4bee-88a5-73909d03f167
- colorclass/vector-space
title: conjugate gradient methods
---

**USER**

conjugate gradient methods


**ASSISTANT**

Conjugate gradient methods are iterative algorithms for solving systems of linear equations, particularly those of the form $Ax = b$ where $A$ is a symmetric positive-definite matrix. These methods are especially useful for large-scale problems where direct methods like Gaussian elimination are computationally prohibitive.

## Overview

The conjugate gradient method is part of the broader class of [[Krylov Subspace Methods]], which build an approximation to the solution iteratively using the Krylov subspace generated by the matrix \(A\) and the initial residual \(r_0 = b - Ax_0\). The key idea behind the conjugate gradient method is to minimize the quadratic form \( f(x) = \frac{1}{2} x^T A x - x^T b \) iteratively along conjugate directions.

### Mathematical Formulation

Given the problem \(Ax = b\), the conjugate gradient method proceeds as follows:

1. **Initialization**:
   - Choose an initial guess \( x_0 \).
   - Compute the initial residual \( r_0 = b - Ax_0 \).
   - Set the initial direction \( p_0 = r_0 \).

2. **Iteration** (for \(k = 0, 1, 2, \ldots\)):
   - Compute the step size: $$ \alpha_k = \frac{r_k^T r_k}{p_k^T A p_k} $$
   - Update the solution: $$ x_{k+1} = x_k + \alpha_k p_k $$
   - Compute the new residual: $$ r_{k+1} = r_k - \alpha_k A p_k $$
   - Check for convergence (e.g., \( \|r_{k+1}\| < \epsilon \) for some tolerance \(\epsilon\)). If converged, exit the loop.
   - Compute the new direction: $$ \beta_k = \frac{r_{k+1}^T r_{k+1}}{r_k^T r_k} $$
   - Update the direction: $$ p_{k+1} = r_{k+1} + \beta_k p_k $$

### Convergence Properties

The conjugate gradient method converges in at most \( n \) iterations for an \( n \times n \) matrix \(A\), assuming exact arithmetic. However, in practical applications, it often converges much faster due to the clustering of eigenvalues.

### Advantages and Applications

- **Sparse Systems**: Particularly efficient for large, sparse systems because it avoids the fill-in that occurs in direct methods.
- **Memory Efficiency**: Requires storage of only a few vectors, making it suitable for problems with large dimensions.
- **Applications**: Widely used in scientific computing, including areas like [[Finite Element Analysis]], optimization problems, and machine learning (e.g., training large-scale linear models).

### Example

Consider solving the system \( Ax = b \) where:

$$ A = \begin{pmatrix} 4 & 1 \\ 1 & 3 \end{pmatrix}, \quad b = \begin{pmatrix} 1 \\ 2 \end{pmatrix}, \quad x_0 = \begin{pmatrix} 2 \\ 1 \end{pmatrix} $$

1. **Initialization**:
   - \( r_0 = b - A x_0 = \begin{pmatrix} 1 \\ 2 \end{pmatrix} - \begin{pmatrix} 4 & 1 \\ 1 & 3 \end{pmatrix} \begin{pmatrix} 2 \\ 1 \end{pmatrix} = \begin{pmatrix} -6 \\ -3 \end{pmatrix} \)
   - \( p_0 = r_0 = \begin{pmatrix} -6 \\ -3 \end{pmatrix} \)

2. **Iteration**:
   - Compute \( \alpha_0 \):
     $$ \alpha_0 = \frac{r_0^T r_0}{p_0^T A p_0} = \frac{(-6)^2 + (-3)^2}{\begin{pmatrix} -6 & -3 \end{pmatrix} \begin{pmatrix} 4 & 1 \\ 1 & 3 \end{pmatrix} \begin{pmatrix} -6 \\ -3 \end{pmatrix}} = \frac{45}{153} = \frac{5}{17} $$
   - Update \( x_1 \):
     $$ x_1 = x_0 + \alpha_0 p_0 = \begin{pmatrix} 2 \\ 1 \end{pmatrix} + \frac{5}{17} \begin{pmatrix} -6 \\ -3 \end{pmatrix} = \begin{pmatrix} \frac{14}{17} \\ \frac{12}{17} \end{pmatrix} $$
   - Compute \( r_1 \):
     $$ r_1 = r_0 - \alpha_0 A p_0 = \begin{pmatrix} -6 \\ -3 \end{pmatrix} - \frac{5}{17} \begin{pmatrix} 4 & 1 \\ 1 & 3 \end{pmatrix} \begin{pmatrix} -6 \\ -3 \end{pmatrix} = \begin{pmatrix} 0 \\ 0 \end{pmatrix} $$

Since \( r_1 = 0 \), the method converges, and the solution is \( x = \begin{pmatrix} \frac{14}{17} \\ \frac{12}{17} \end{pmatrix} \).

## Related Topics

- [[Preconditioned Conjugate Gradient Method]]: An extension to accelerate convergence by preconditioning the system.
- [[Lanczos Algorithm]]: Another Krylov subspace method, useful for eigenvalue problems.
- [[Gradient Descent]]: A general optimization method, of which the conjugate gradient is a specific variant for quadratic forms.

The conjugate gradient method remains a cornerstone algorithm in numerical linear algebra, celebrated for its efficiency and robustness in solving large-scale systems.


---


following: [[lanczos algorithm]]

See also: [[trust region policy optimization (trpo) in reinforcement learning]]


See also: [[quadratic forms]]


See also: [[gradient]]