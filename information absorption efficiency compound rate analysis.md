---
date: '2024-11-21T20:51:19.840195Z'
tags:
- autogenerated
- source__claude
- conversation_id__3192c2c3-bfbf-423b-8bab-29fb984f34a0
- msg_1d0ec60c-78d3-4ff2-9560-c548e000fa05
- msg_dcddf3e7-28ae-432a-9ad2-fe179e0620ed
- colorclass/information-theory
title: information absorption efficiency compound rate analysis
---

**USER**

relate η(C), learning rate, global batch size, throughput, gradient magnitude


**ASSISTANT**

# Information Absorption Efficiency: Compound Rate Analysis

## Fundamental Relationship

Let's construct the dimensional equation relating these quantities:

```
η(C) = λ · |∇L| · B · R / N
```

Where:
- η(C): Information absorption efficiency [1/FLOP]
- λ: Learning rate [dimensionless]
- |∇L|: Gradient magnitude [1/parameter]
- B: Global batch size [samples]
- R: Training throughput [samples/second]
- N: Number of parameters [parameters]

## Unit Analysis

Breaking down each term:

1. [[Learning Rate]] (λ):
   ```
   [λ] = [1]  # dimensionless scale factor
   ```

2. [[Gradient Magnitude]] (|∇L|):
   ```
   [|∇L|] = [1/parameter]  # change in loss per parameter
   ```

3. [[Global Batch Size]] (B):
   ```
   [B] = [samples]  # training examples per update
   ```

4. [[Training Throughput]] (R):
   ```
   [R] = [samples/second]  # processing rate
   ```

5. [[Parameter Count]] (N):
   ```
   [N] = [parameters]  # total learnable parameters
   ```

## Composite Analysis

### Information Flow Rate
```
[λ · |∇L| · B] = [1 · 1/parameter · samples]
                = [samples/parameter]
```

### Processing Rate
```
[R/N] = [samples/second/parameter]
```

### Combined Rate
```
[η(C)] = [samples/parameter · samples/second/parameter]
       = [1/FLOP]  # when normalized by compute per sample
```

## Physical Interpretation

The relationship suggests [[information absorption efficiency]] is:
1. Proportional to learning rate
2. Proportional to gradient magnitude
3. Proportional to batch size
4. Proportional to throughput
5. Inversely proportional to parameter count squared

## Optimization Implications

1. [[Batch Size Scaling]]:
   ```
   B_opt ∝ N^(1/2)  # to maintain constant η(C)
   ```

2. [[Learning Rate Scaling]]:
   ```
   λ_opt ∝ N^(-1/2)  # for stable absorption
   ```

3. [[Throughput Scaling]]:
   ```
   R_opt ∝ N  # to compensate parameter growth
   ```

⚠️ **META-NOTE**: This relationship suggests a fundamental connection between architectural choices (N), optimization parameters (λ, B), and system capabilities (R) in determining [[information absorption efficiency]].

## See Also
- [[Learning Rate Theory]]
- [[Batch Size Scaling Laws]]
- [[Training Throughput Analysis]]
- [[Gradient Flow Dynamics]]


---

preceding: [[dimensional analysis of [[information absorption efficiency]]]]  


following: [[large batch training efficiency analysis via statistical mechanics]]

See also: [[information absorption efficiency]]