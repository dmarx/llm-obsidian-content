---
date: 1713909005.652952
tags:
- autogenerated
- source__oai
- conversation_id__9f8c6c88-3adc-4b4b-af9c-e569eada9468
- msg_aaa2a772-a350-4ed8-9c75-5d2b69dae194
- msg_a37d8497-7fea-4d5f-b4cd-e76f14313bc0
- colorclass/vector-space
title: method of least squares
---

**USER**

method of least squares


**ASSISTANT**

The method of least squares is a fundamental statistical technique used to estimate the parameters of a model by minimizing the sum of the squared differences between observed values and the values predicted by the model. This method is extensively used in data fitting, particularly in linear regression, and has broad applications across sciences and economics where predictive modeling and trend analysis are required.

### Mathematical Foundation of Least Squares

The method of least squares finds the best-fitting curve or line by minimizing the sum of the squares of the residuals. The residuals are the differences between the observed values and those predicted by the model. In the context of linear regression with a model:

$$
y = \beta_0 + \beta_1 x_1 + \beta_2 x_2 + \dots + \beta_p x_p + \epsilon
$$

where \( y \) is the dependent variable, \( x_1, x_2, \dots, x_p \) are the independent variables, \( \beta_0, \beta_1, \dots, \beta_p \) are the parameters to be estimated, and \( \epsilon \) is the error term, the goal is to estimate the parameters \( \beta_i \) that minimize:

$$
S(\beta_0, \beta_1, \dots, \beta_p) = \sum_{i=1}^n (y_i - (\beta_0 + \beta_1 x_{i1} + \beta_2 x_{i2} + \dots + \beta_p x_{ip}))^2
$$

### Derivation of the Normal Equations

To find the parameters that minimize the sum of squared residuals, you take the partial derivatives of \( S \) with respect to each parameter \( \beta_j \), set them to zero, and solve for the \( \beta \)s. This process results in a system of equations known as the normal equations:

$$
X^T X \beta = X^T y
$$

where \( X \) is the matrix of input data (with each column representing a different predictor variable and rows representing observations), \( y \) is the vector of observed values, and \( \beta \) is the vector of parameters.

### Solution via Matrix Algebra

The solution to the normal equations provides the least squares estimates of the parameters:

$$
\beta = (X^T X)^{-1} X^T y
$$

This equation assumes that \( X^T X \) is non-singular (invertible), which generally requires that there be more observations than parameters and that the predictor variables are not perfectly collinear.

### Applications of Least Squares

- **Linear Regression**: This is the most direct application, where the method is used to fit a linear model to observed data.
- **Curve Fitting**: Beyond linear models, least squares can be used to fit nonlinear models, assuming the model can be linearized or approximately linearized around the parameters.
- **Signal Processing**: In signal processing, least squares is used to estimate signals and remove noise.
- **Control Systems**: Used for system identification in control engineering to model the behavior of dynamic systems.

### Advantages and Limitations

- **Advantages**:
  - Provides a closed-form solution under ideal conditions, making it computationally straightforward.
  - Robust to small deviations and noise in the data, provided the underlying assumptions of linearity and normality of errors are reasonably met.

- **Limitations**:
  - Sensitivity to outliers, which can disproportionately influence the model due to the squaring of residuals.
  - Assumes that the errors are normally distributed, homoscedastic (constant variance), and independent, which might not always be the case in real-world data.

### Conclusion

The method of least squares is a powerful and widely used tool for statistical estimation and data fitting, providing a simple yet effective way to determine the best fit for a wide range of models. Its application, from simple linear regression to complex engineering systems, highlights its versatility and fundamental importance in both theoretical and applied statistics.


---

preceding: [[similar to minimizing the moment (torque) about the center of mass in physics]]  


following: [[mean squared error]]

See also: [[applications of concepts from probability and statistics by phsicists prior to the formal invention of statistics as a mathetmatical subfield]]


See also: [[signal processing]]


See also: [[seminal works in the theory of probability]]


See also: [[partial derivatives]]