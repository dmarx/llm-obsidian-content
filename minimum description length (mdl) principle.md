---
date: 1732951217.234
tags:
- autogenerated
- source__oai
- conversation_id__674ab96e-0030-8008-811b-df1a12b1e598
- msg_aaa2984c-5c3c-4fc1-b59f-97ddb157c3dd
- msg_9a8953fd-6f86-4b56-b6f5-0f64bb365c7b
- colorclass/information-theory
title: minimum description length (mdl) principle
---

**USER**

Minimum Description Length Principle


**ASSISTANT**

### Minimum Description Length (MDL) Principle

The **Minimum Description Length (MDL) Principle** is a formalization of Occam's Razor within the framework of [[Algorithmic Information Theory]] and statistics. It provides a rigorous method for model selection by seeking a balance between the complexity of a model and its fit to the observed data. MDL was introduced by Jorma Rissanen in the late 1970s and has since become a cornerstone in information-theoretic approaches to inference.

---

### Core Idea

The MDL principle states that the best explanation for a dataset is the one that minimizes the total length of the description of:

1. **The Model**: The parameters or structure of the model (how complex the explanation is).
2. **The Data Given the Model**: How well the model fits the data (the residual error).

Formally, the MDL principle minimizes the total code length:
$$
L(M) + L(D \mid M),
$$
where:
- \( L(M) \) is the length of the description of the model \( M \),
- \( L(D \mid M) \) is the length of the description of the data \( D \) given the model \( M \).

This is closely related to [[Kolmogorov Complexity]] but adapted for practical use with computable approximations.

---

### Components

1. **Encoding the Model**
   - Models with more parameters or complex structures require longer descriptions.
   - The complexity of the model is captured by \( L(M) \), ensuring that overly complex models are penalized.

2. **Encoding the Data Given the Model**
   - A good model compresses the data efficiently, making \( L(D \mid M) \) smaller.
   - \( L(D \mid M) \) often corresponds to the likelihood of the data under the model.

3. **Trade-Off**
   - Simple models may have short descriptions (\( L(M) \)) but poor fit (\( L(D \mid M) \)).
   - Overly complex models fit the data well (\( L(D \mid M) \)) but have high complexity (\( L(M) \)).
   - The MDL principle identifies a balance by minimizing the total description length.

---

### Formalization

#### Two-Part Code
The MDL principle often uses a **two-part code**:
1. Encode the model \( M \) with length \( L(M) \).
2. Encode the data \( D \) given the model \( M \) with length \( L(D \mid M) \).

The total description length is:
$$
L(D, M) = L(M) + L(D \mid M).
$$

#### Universal Coding
In some cases, MDL employs **universal coding**, where the description length is derived using a universal probability distribution \( P_U \):
$$
L(D, M) = -\log P_U(D, M).
$$

This avoids the explicit two-part structure by encoding both model and data together.

---

### MDL in Model Selection

In practice, MDL is used to select the best model \( M^* \) from a set of candidate models:
$$
M^* = \arg \min_{M \in \mathcal{M}} \{ L(M) + L(D \mid M) \}.
$$

For parametric models, \( L(D \mid M) \) is often approximated by:
$$
L(D \mid M) \approx -\log P(D \mid M),
$$
where \( P(D \mid M) \) is the likelihood of the data under the model.

---

### Relation to Other Principles

1. **Akaike Information Criterion (AIC)**
   AIC minimizes:
   $$
   -2 \log P(D \mid M) + 2k,
   $$
   where \( k \) is the number of model parameters. AIC can be seen as a simplification of MDL for large sample sizes and when \( L(M) \) depends linearly on the number of parameters.

2. **Bayesian Information Criterion (BIC)**
   BIC minimizes:
   $$
   -2 \log P(D \mid M) + k \log n,
   $$
   where \( n \) is the sample size. BIC corresponds to an approximation of MDL under certain assumptions about priors.

3. **Kolmogorov Complexity**
   MDL is inspired by [[Kolmogorov Complexity]], but unlike \( K(D) \), which is incomputable, MDL is computable by focusing on specific model classes and encoding schemes.

---

### Applications

1. **Data Compression**
   MDL is directly connected to optimal coding schemes and is used in designing compression algorithms.

2. **Model Selection in Statistics and Machine Learning**
   MDL is applied in selecting models, such as regression models, clustering algorithms, or neural networks, balancing complexity and fit.

3. **Pattern Discovery**
   MDL identifies patterns in data by finding structures that lead to better compression.

4. **Time Series Analysis**
   MDL is used to select models for forecasting or identifying change points in time series data.

---

### Example

#### Data
Suppose the data \( D \) is a sequence of 1s and 0s: \( D = 1111111100000000 \).

#### Candidate Models
1. **Model 1**: The sequence is a repetition of 8 ones followed by 8 zeros. Description length:
   $$
   L(M_1) = \text{length to encode "repeated 8 ones, then 8 zeros"} = 10 \, \text{bits},
   $$
   and
   $$
   L(D \mid M_1) = 0 \, \text{bits} \, \text{(no additional information needed to describe the data)}.
   $$

2. **Model 2**: The sequence is completely random. Description length:
   $$
   L(M_2) = 0 \, \text{bits (no model)}.
   $$
   and
   $$
   L(D \mid M_2) = \text{length to encode the entire sequence explicitly} = 16 \, \text{bits}.
   $$

#### MDL Criterion
Total description lengths:
- Model 1: \( L(M_1) + L(D \mid M_1) = 10 + 0 = 10 \, \text{bits}. \)
- Model 2: \( L(M_2) + L(D \mid M_2) = 0 + 16 = 16 \, \text{bits}. \)

MDL selects **Model 1** as the better explanation.

---

### Limitations and Extensions

1. **Model Assumptions**
   MDL depends on the choice of model class \( \mathcal{M} \). If the true model is outside \( \mathcal{M} \), MDL may fail to select it.

2. **Approximation of Complexity**
   In practice, \( L(M) \) and \( L(D \mid M) \) are approximated, which may introduce biases.

3. **Computational Cost**
   Finding the model \( M^* \) that minimizes \( L(M) + L(D \mid M) \) can be computationally expensive for large or complex datasets.

4. **Extensions**
   - **Normalized MDL (NML):** Adjusts MDL for small sample sizes.
   - **MDL in Bayesian Settings:** Links to Bayesian inference by treating priors as a source of model description lengths.

MDL is a powerful, principled approach to inference, unifying ideas from information theory, statistics, and computation into a coherent framework for balancing simplicity and explanatory power.


---

preceding: [[kraft's inequality]]  


following: [[quantifying the information content of a dataset and interpreting the irreducible loss]]

See also: [[information theory]]