---
date: 1713752407.589268
tags:
- autogenerated
- source__oai
- conversation_id__5105adc8-e608-40d7-b875-84f3846ef325
- msg_aaa2f1e7-083f-4b20-a5fd-cf96f176bcee
- msg_f152eb48-ac02-463a-a63d-5a0ef3be0a28
- colorclass/vector-space
title: he initialization
---

**USER**

He initialization


**ASSISTANT**

He initialization, also known as He normal initialization, is a technique used to set the initial weights of neural network layers in a way that helps to manage the variance of activations throughout the network. This method is particularly useful when the network uses ReLU (Rectified Linear Unit) activation functions and is designed to prevent the vanishing and exploding gradient problems that can occur in deep networks. The technique was introduced by Kaiming He et al. in the paper "Delving Deep into Rectifiers: Surpassing Human-Level Performance on ImageNet Classification" (2015), which focused on improving the training of deep neural architectures.

### Why He Initialization?

Before the introduction of He initialization, Xavier (or Glorot) initialization was commonly used, but it was primarily optimized for networks using sigmoid or tanh activations. Since ReLU activations became popular due to their ability to mitigate vanishing gradients, a need arose for an initialization scheme better suited to them. ReLU units can potentially double the variance of activations, hence the need for adjustment in the weight initialization.

### How Does He Initialization Work?

The key idea behind He initialization is to set the initial weights of the network's layers such that the variance of the outputs from each layer is equal to the variance of its inputs. This balance helps maintain a stable gradient size during backpropagation, which is essential for the effective training of deep neural networks.

#### Mathematical Formulation

For a layer with ReLU activation, the weights are initialized from a Gaussian distribution \( N(0, \sigma^2) \) or a uniform distribution, where \( \sigma \) (the standard deviation) is given by:
$$
\sigma = \sqrt{\frac{2}{n}}
$$
Here, \( n \) represents the number of incoming connections (or fan-in) to the layer. This factor of 2 in the numerator is specifically chosen to account for the ReLU activation function, which only passes positive values and zeros out negative values, potentially halving the number of active inputs at any time.

### Benefits of He Initialization

1. **Stable Gradient Propagation**: By adapting the variance of the weights according to the number of input connections, He initialization helps in maintaining gradient magnitudes at a level that avoids vanishing and exploding gradients as the signal propagates forward and backward through the network.

2. **Faster Convergence**: Networks initialized with this method often converge faster because they start with weights that help avoid initial excessive saturation of activation functions, which can occur if the initial weights are too large or too small.

3. **Improved Training of Deep Networks**: He initialization is especially beneficial in training deeper networks with ReLU activations, supporting the training process to proceed smoothly and effectively from the beginning.

### Practical Application

He initialization has become a standard practice when working with neural networks that utilize ReLU or its variants like Leaky ReLU, Parametric ReLU (PReLU), and others. This method is readily available in most deep learning frameworks, and it can be specified as an option when defining layers in models built using libraries like TensorFlow, PyTorch, and Keras.

### Conclusion

He initialization addresses the specific needs of neural networks employing ReLU activations by providing a robust method for setting the initial weights. This helps in overcoming potential hurdles related to training dynamics, such as vanishing and exploding gradients, making it easier to train effective deep learning models, particularly those involving many layers. As such, it represents a key development in the toolbox of techniques available to neural network practitioners aiming to optimize the performance and convergence of their models.


---

preceding: [[vanishing gradients]]  


following: [[glorot initialization]]

See also: [[researchers like james crutchfield have applied concepts from chaos theory to neural networks, offering insights into their dynamic properties.]]


See also: [[unreasonable effectiveness of gradient descent]]


See also: [[weight initialization strategies inductive priors and solution feasibility]]