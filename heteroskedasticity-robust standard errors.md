---
date: '2025-03-09T09:32:00.452098Z'
tags:
- autogenerated
- source__claude
- conversation_id__f5278ef4-a0f7-4b09-b57a-d1033ff3a25c
- msg_82a98426-3f95-4e3e-8073-67528a26303f
- msg_270d37aa-5ba9-4cb1-a4de-40820607efda
- colorclass/philosophy-of-language
title: heteroskedasticity-robust standard errors
---

**USER**

heteroskedasticity-robust standard errors


**ASSISTANT**

# Heteroskedasticity-Robust Standard Errors

## Overview

[[Heteroskedasticity-robust standard errors]] (also called robust standard errors, White standard errors, or Huber-White standard errors) are adjustments to the standard errors in regression analysis that account for patterns in the residuals that violate the classical assumption of homoskedasticity.

## The Homoskedasticity Problem

In classical [[ordinary least squares]] (OLS) regression, one key assumption is homoskedasticity: the variance of the error term is constant across all values of the independent variables. Formally, this is expressed as:

$$\text{Var}(\varepsilon_i|X_i) = \sigma^2$$

When this assumption is violated (heteroskedasticity), the standard OLS estimator remains unbiased and consistent, but the conventional standard errors become inappropriate, leading to:
- Invalid hypothesis tests
- Inaccurate confidence intervals
- Inefficient parameter estimates

## Mathematical Formulation

### Conventional OLS Variance Formula

The conventional variance-covariance matrix for OLS estimators is:

$$\text{Var}(\hat{\beta}) = \sigma^2(X'X)^{-1}$$

Where:
- $\hat{\beta}$ is the vector of regression coefficients
- $\sigma^2$ is the constant error variance
- $X$ is the matrix of predictors

### Heteroskedasticity-Robust Variance Formula

The heteroskedasticity-robust variance estimator (HC0) introduced by White (1980) is:

$$\widehat{\text{Var}}(\hat{\beta}) = (X'X)^{-1}X'\Omega X(X'X)^{-1}$$

Where $\Omega$ is a diagonal matrix with the squared residuals $\hat{\varepsilon}_i^2$ on the diagonal.

## Types of Robust Standard Errors

Several variants of robust standard errors have been developed to improve finite sample properties:

1. **HC0**: The original White estimator
   
   $$\Omega_i = \hat{\varepsilon}_i^2$$

2. **HC1**: Degrees of freedom adjustment
   
   $$\Omega_i = \frac{n}{n-k}\hat{\varepsilon}_i^2$$
   
   Where $n$ is sample size and $k$ is the number of parameters.

3. **HC2**: Leverage adjustment
   
   $$\Omega_i = \frac{\hat{\varepsilon}_i^2}{1-h_{ii}}$$
   
   Where $h_{ii}$ is the leverage (diagonal element of the hat matrix).

4. **HC3**: Further leverage adjustment to improve small sample performance
   
   $$\Omega_i = \frac{\hat{\varepsilon}_i^2}{(1-h_{ii})^2}$$

5. **HC4, HC5**: Further refinements for high-leverage observations

## Applications with Binary Outcomes

For [[linear regression]] with binary outcomes (the linear probability model), heteroskedasticity is mathematically guaranteed. If:

$$P(Y_i = 1|X_i) = X_i'\beta$$

Then the variance is:

$$\text{Var}(Y_i|X_i) = X_i'\beta(1-X_i'\beta)$$

This variance is not constant unless $\beta = 0$, making heteroskedasticity-robust standard errors particularly important when analyzing binary outcomes with linear models.

## Implementation in Statistical Software

### R

```r
# Using lm() with robust standard errors
model <- lm(y ~ x, data = data)
library(sandwich)
robust_se <- sqrt(diag(vcovHC(model, type = "HC1")))

# Or using the lmtest package
library(lmtest)
coeftest(model, vcov = vcovHC(model, type = "HC1"))
```

### Python

```python
import statsmodels.api as sm

# Fit the model with robust standard errors
X = sm.add_constant(X)
model = sm.OLS(y, X)
results = model.fit(cov_type='HC1')  # HC1 is default
```

### Stata

```stata
reg y x, robust
```

## Limitations and Considerations

1. **Sample Size**: Robust standard errors perform better in large samples; in small samples, they may be biased.

2. **Model Misspecification**: Robust standard errors address heteroskedasticity but don't fix other model misspecifications.

3. **Cluster-Robust Variant**: For clustered data (e.g., repeated measures, hierarchical data), cluster-robust standard errors may be more appropriate:
   
   ```r
   vcovCL(model, cluster = data$group)
   ```

4. **Bootstrap Alternative**: Bootstrapping can provide an alternative approach to computing robust standard errors.

## Conclusion

Heteroskedasticity-robust standard errors are an essential tool for valid statistical inference when the homoskedasticity assumption is violated. For binary outcome modeling with linear regression, they are particularly important due to the intrinsic heteroskedasticity of the error term. While they don't change point estimates, they provide more accurate measures of uncertainty, leading to valid hypothesis tests and confidence intervals.


---

preceding: [[linear regression for binary outcomes]]

See also: [[hat matrix]]