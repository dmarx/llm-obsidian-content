---
date: 1724919413.154343
tags:
- autogenerated
- source__oai
- conversation_id__ad2150e2-e599-43ae-9dc4-259111e0c751
- msg_aaa22d5c-83a2-4bd0-a872-2b6970b53d8e
- msg_bb0db7af-561f-46ce-b6c0-637bc07c109c
- colorclass/information-theory
title: entropy in information theory
---

**USER**

Entropy in Information Theory


**ASSISTANT**

### Entropy in Information Theory

**Entropy** in the context of [[information theory]] is a measure of the uncertainty or unpredictability associated with a random variable. It quantifies the amount of "information" or "surprise" contained in a random variable's possible outcomes. The concept was first introduced by [[Claude Shannon]] in his 1948 paper, "A Mathematical Theory of Communication," and is foundational to the field of information theory.

#### Definition of Entropy

For a discrete random variable $X$ with a set of possible outcomes $\{x_1, x_2, \ldots, x_n\}$ and associated probabilities $\{p(x_1), p(x_2), \ldots, p(x_n)\}$, the entropy $H(X)$ is defined as:

$$
H(X) = -\sum_{i=1}^n p(x_i) \log p(x_i).
$$

In this formula:
- $p(x_i)$ is the probability of the outcome $x_i$.
- The logarithm is usually taken in base 2 (resulting in entropy measured in **bits**) or in base $e$ (resulting in **nats**).

The entropy $H(X)$ represents the expected amount of information (in bits or nats) required to describe the outcome of $X$ when the probability distribution of $X$ is known.

For a **continuous random variable** with probability density function $p(x)$, the entropy is defined analogously as:

$$
H(X) = -\int_{-\infty}^{\infty} p(x) \log p(x) \, dx.
$$

This definition assumes that $p(x) \log p(x)$ is integrable, and it generalizes the discrete case to a continuous domain.

#### Interpretation of Entropy

Entropy can be interpreted in several ways:

1. **Measure of Uncertainty**: Entropy quantifies the uncertainty associated with the outcome of a random variable. A higher entropy indicates more uncertainty, while lower entropy indicates less uncertainty. For example, if $X$ represents the outcome of a fair coin toss, then:

   $$
   H(X) = -\left(\frac{1}{2} \log \frac{1}{2} + \frac{1}{2} \log \frac{1}{2}\right) = \log 2 = 1 \text{ bit}.
   $$

   This result implies that there is one bit of uncertainty, or information, associated with predicting the outcome of a fair coin toss.

2. **Average Surprise**: Entropy also represents the average "surprise" of a random variable's outcomes. Outcomes that are very unlikely provide more "surprise" (or information) when they occur, while highly probable outcomes provide less surprise. 

3. **Optimal Encoding Length**: In the context of data compression, entropy provides a lower bound on the average number of bits required to encode a message drawn from a given distribution. This is related to Shannon's source coding theorem, which states that the entropy of a source represents the minimum number of bits needed on average to encode each symbol from that source without loss of information.

#### Properties of Entropy

Entropy has several key properties that make it a fundamental concept in information theory:

1. **Non-Negativity**: Entropy is always non-negative:

   $$
   H(X) \geq 0.
   $$

   Entropy is zero if and only if the outcome is certain (i.e., one of the probabilities $p(x_i)$ is 1 and all others are 0).

2. **Maximum Entropy**: For a discrete random variable with a fixed number of outcomes $n$, entropy is maximized when the distribution is uniform, meaning all outcomes are equally likely:

   $$
   H(X) \leq \log n.
   $$

   Equality holds when $p(x_i) = \frac{1}{n}$ for all $i$.

3. **Additivity for Independent Variables**: If $X$ and $Y$ are independent random variables, their joint entropy is the sum of their individual entropies:

   $$
   H(X, Y) = H(X) + H(Y).
   $$

   This property reflects the fact that independent variables provide information independently of each other.

4. **Chain Rule of Entropy**: The entropy of a pair of random variables $(X, Y)$ can be decomposed as:

   $$
   H(X, Y) = H(X) + H(Y|X),
   $$

   where $H(Y|X)$ is the **conditional entropy** of $Y$ given $X$, representing the uncertainty of $Y$ when $X$ is known. This rule generalizes to more than two variables and underpins much of the analysis in information theory.

5. **Invariance Under Change of Alphabet**: If you apply a one-to-one transformation to the random variable, the entropy remains unchanged. This property shows that entropy is a measure of the distribution of probabilities, not of the values themselves.

#### Conditional Entropy and Mutual Information

**Conditional entropy** measures the remaining uncertainty of a random variable $Y$ given that the outcome of another random variable $X$ is known:

$$
H(Y|X) = -\sum_{x \in \mathcal{X}} p(x) \sum_{y \in \mathcal{Y}} p(y|x) \log p(y|x).
$$

**Mutual information** quantifies the amount of information obtained about one random variable through the other. For random variables $X$ and $Y$, mutual information $I(X; Y)$ is defined as:

$$
I(X; Y) = H(X) - H(X|Y) = H(Y) - H(Y|X).
$$

Mutual information is always non-negative and symmetric ($I(X; Y) = I(Y; X)$). It is zero if and only if the random variables are independent.

#### Differential Entropy

For continuous random variables, **differential entropy** is a generalization of discrete entropy and is defined as:

$$
h(X) = -\int_{-\infty}^{\infty} p(x) \log p(x) \, dx.
$$

Differential entropy has different properties from its discrete counterpart. Notably, it can be negative, and it is not invariant under changes of variables (which means it is affected by transformations like scaling).

#### Entropy in Communication Systems

In the design of communication systems, entropy plays a crucial role in determining the efficiency of encoding schemes. Shannon's noisy-channel coding theorem, which builds on the concept of entropy, shows that reliable communication is possible up to a certain limit, called the channel capacity, which is the maximum mutual information between the input and output of a channel.

### Conclusion

**Entropy** in information theory provides a fundamental measure of the uncertainty or information content in a random variable. It is a versatile tool in both theoretical and practical applications, underpinning everything from data compression and error correction in communication systems to probabilistic modeling and machine learning. Its properties and interpretations make entropy a cornerstone concept in understanding the behavior of complex systems and the flow of information.


---

preceding: [[maximum entropy principle]]  


following: [[state function]]

See also: [[conditional entropy]]


See also: [[mutual information]]


See also: [[information theory]]


See also: [[channel capacity]]


See also: [[random variables]]


See also: [[boltzmann's entropy constant]]