---
date: '2024-12-01T06:03:45.646142Z'
tags:
- autogenerated
- source__claude
- conversation_id__89a0dda3-5667-4fb7-9fa7-fc583dbda261
- msg_e82bfe2c-ec93-42c3-872a-f5096c8b7199
- msg_d76ad0cd-5550-44de-9a2b-5e3680d2a41d
- colorclass/information-theory
title: μp for self-attention derivation
---

**USER**

demonstrate how to derive the muP extension for self-attention


**ASSISTANT**

# μP for Self-Attention: Derivation

## Problem Statement

We need to derive the [[Maximal Update Parameterization]] for [[self-attention mechanisms]] that maintains O(1) parameter updates during training while preserving the relative scale of attention outputs.

## Initial Setup

Standard self-attention is defined as:

Attention(Q,K,V) = softmax(QK^T/√d_k)V

where:
- Q = XW_Q
- K = XW_K
- V = XW_V
- X ∈ ℝⁿˣᵈ (input)
- W_Q,W_K,W_V ∈ ℝᵈˣᵈ_k (projection matrices)

## Derivation Steps

### 1. Analyze Update Magnitude

For parameter updates to remain O(1), we need:

||∂L/∂W_Q|| ∝ O(1)
||∂L/∂W_K|| ∝ O(1)
||∂L/∂W_V|| ∝ O(1)

### 2. Examine Gradient Flow

The gradient with respect to W_Q flows through:

∂L/∂W_Q = X^T · ∂L/∂Q
∂L/∂Q ∝ softmax'(QK^T/√d_k)K/√d_k · V

### 3. Scale Analysis

In standard self-attention:
- QK^T scales as O(d)
- The √d_k denominator makes QK^T/√d_k scale as O(√d)
- softmax(QK^T/√d_k) scales as O(1)
- The output scales as O(√d)

### 4. μP Adjustments

To achieve O(1) updates, we need:

Q̃ = W_Q/√d
K̃ = W_K/√d
Ṽ = W_V/√d

where d is the input dimension.

### 5. Modified Attention Formula

The μP-scaled attention becomes:

Attention(Q̃,K̃,Ṽ) = softmax(Q̃K̃^T)Ṽ

Note: The √d_k term is absorbed into the parameterization.

## Verification

### 1. Forward Pass Scaling

Under μP:
- Q̃K̃^T scales as O(1)
- softmax(Q̃K̃^T) scales as O(1)
- Final output scales as O(1)

### 2. Gradient Scaling

The gradients now scale as:

||∂L/∂W_Q|| ∝ ||X^T|| · ||∂L/∂Q̃|| · (1/√d) ∝ O(1)

Similarly for W_K and W_V.

### 3. Update Magnitude

Parameter updates during gradient descent:

ΔW_Q = -η · ∂L/∂W_Q ∝ O(1)

## Implementation

```python
def muP_attention(X, W_Q, W_K, W_V):
    d = X.shape[1]  # input dimension
    
    # μP scaling
    Q = X @ (W_Q / np.sqrt(d))
    K = X @ (W_K / np.sqrt(d))
    V = X @ (W_V / np.sqrt(d))
    
    # Attention computation
    attention_weights = softmax(Q @ K.T)
    output = attention_weights @ V
    
    return output
```

## Related Concepts
- [[Residual Stream Scale]]
- [[Attention Head Scaling]]
- [[Multi-head Attention]]
- [[Layer Normalization]]
- [[Gradient Flow Analysis]]

The derived μP extension for self-attention ensures stable training dynamics while preserving the mechanism's ability to learn meaningful attention patterns, regardless of model scale.


---

preceding: [[maximal update parameterization]]  


following: [[flexattention with μp parameterization]]