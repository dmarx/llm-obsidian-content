---
date: 1713153062.652002
tags:
- autogenerated
- source__oai
- conversation_id__5fa9dfe5-a59e-4ed4-a2e9-ff745c9a6df9
- msg_aaa27104-4df4-4710-ac6a-70e5a4fa6929
- msg_5376450b-9fb0-4f72-8850-5e25aa8a6e72
- colorclass/vector-space
title: clifford group equivariant neural networks
---

**USER**

Clifford Group Equivariant Neural Networks
David Ruhe
AI4Science Lab, AMLab, API
University of Amsterdam
david.ruhe@gmail.com
Johannes Brandstetter
Microsoft Research
AI4Science
brandstetter@ml.jku.at
Patrick Forré
AI4Science Lab, AMLab
University of Amsterdam
p.d.forre@uva.nl
Abstract
We introduce Clifford Group Equivariant Neural Networks: a novel approach
for constructing O(n)- and E(n)-equivariant models. We identify and study the
Clifford group: a subgroup inside the Clifford algebra tailored to achieve several favorable properties. Primarily, the group’s action forms an orthogonal automorphism
that extends beyond the typical vector space to the entire Clifford algebra while
respecting the multivector grading. This leads to several non-equivalent subrepresentations corresponding to the multivector decomposition. Furthermore, we prove
that the action respects not just the vector space structure of the Clifford algebra but
also its multiplicative structure, i.e., the geometric product. These findings imply
that every polynomial in multivectors, including their grade projections, constitutes
an equivariant map with respect to the Clifford group, allowing us to parameterize equivariant neural network layers. An advantage worth mentioning is that
we obtain expressive layers that can elegantly generalize to inner-product spaces
of any dimension. We demonstrate, notably from a single core implementation,
state-of-the-art performance on several distinct tasks, including a three-dimensional
n-body experiment, a four-dimensional Lorentz-equivariant high-energy physics
experiment, and a five-dimensional convex hull experiment. 
1 Introduction
Incorporating group equivariance to ensure symmetry constraints in neural networks has been a highly
fruitful line of research [CW16, WGTB17, CGKW18, KT18, WC19, WGW+18, Est20, BBCV21,
WFVW21, CLW22]. Besides translation and permutation equivariance [ZKR+17, SGT+08], rotation
equivariance proved to be vitally important for many graph-structured problems as encountered in,
e.g., the natural sciences. Applications of such methods include modeling the dynamics of complex
physical systems or motion trajectories [KFW+18, BHP+22]; studying or generating molecules,
proteins, and crystals [RDRL14, GLB+16, CTS+17, SSK+18, ZCD+20, TVS+21, AGB22]; and
point cloud analysis [WSK+15, UPH+19]. Note that many of these focus on three-dimensional
problems involving rotation, reflection, or translation equivariance by considering the groups O(3),
SO(3), E(3), or SE(3).
Such equivariant neural networks can be broadly divided into three categories: approaches that
scalarize geometric quantities, methods employing regular group representations, and those utilizing
irreducible representations, often of O(3) [HRXH22]. Scalarization methods operate exclusively on
Code is available at https://github.com/DavidRuhe/clifford-group-equivariant-neural-networks
37th Conference on Neural Information Processing Systems (NeurIPS 2023).
arXiv:2305.11141v5 [cs.LG] 22 Oct 2023
1
|{z}
Scalars
e1 + e2 + e3
| {z }
Vectors
e12 + e13 + e23
| {z }
Bivectors
e123
| {z }
Trivectors
ρ(w)
⟳
ρ(w)
⟳
ϕ
ϕ
Figure 1: CGENNs (represented with ϕ) are able to operate on multivectors (elements of the Clifford
algebra) in an O(n)- or E(n)-equivariant way. Specifically, when an action ρ(w) of the Clifford
group, representing an orthogonal transformation such as a rotation, is applied to the data, the model’s
representations corotate. Multivectors can be decomposed into scalar, vector, bivector, trivector, and
even higher-order components. These elements can represent geometric quantities such as (oriented)
areas or volumes. The action ρ(w) is designed to respect these structures when acting on them.
scalar features or manipulate higher-order geometric quantities such as vectors via scalar multiplication [SSK+18, CCG18, KGG20, KKN20, SHW21, JES+21, GBG21, SUG21, DLD+21, HHR+22,
TF22]. They can be limited by the fact that they do not extract all directional information. Regular
representation methods construct equivariant maps through an integral over the respective group
[CW16, KT18]. For continuous Lie groups, however, this integral is intractable and requires coarse
approximation [FSIW20, Bek19]. Methods of the third category employ the irreducible representations of O(3) (the Wigner-D matrices) and operate in a steerable spherical harmonics basis
[TSK+18, AHK19, FWFW20, BHP+22, BMS+22]. This basis allows a decomposition into type-l
vector subspaces that transform under Dl
: the type-l matrix representation of O(3) [Len90, FA+91].
Through tensor products decomposed using Clebsch-Gordan coefficients (Clebsch-Gordan tensor
product), vectors (of different types) interact equivariantly. These tensor products can be parameterized using learnable weights. Key limitations of such methods include the necessity for an alternative
basis, along with acquiring the Clebsch-Gordan coefficients, which, although they are known for
unitary groups of any dimension [KR67], are not trivial to obtain [AKHvD11].
We propose Clifford Group Equivariant Neural Networks (CGENNs): an equivariant parameterization
of neural networks based on Clifford algebras. Inside the algebra, we identify the Clifford group
and its action, termed the (adjusted) twisted conjugation, which has several advantageous properties.
Unlike classical approaches that represent these groups on their corresponding vector spaces, we
carefully extend the action to the entire Clifford algebra. There, it automatically acts as an orthogonal
automorphism that respects the multivector grading, enabling nontrivial subrepresentations that
operate on the algebra subspaces. Furthermore, the twisted conjugation respects the Clifford algebra’s
multiplicative structure, i.e. the geometric product, allowing us to bypass the need for explicit tensor
product representations. As a result, we obtain two remarkable properties. First, all polynomials in
multivectors generate Clifford group equivariant maps from the Clifford algebra to itself. Additionally,
grade projections are equivariant, allowing for a denser parameterization of such polynomials. We
then demonstrate how to construct parameterizable neural network layers using these properties.
Our method comes with several advantages. First, instead of operating on alternative basis representations such as the spherical harmonics, CGENNs (similarly to scalarization methods) directly
transform data in a vector basis. Second, multivector representations allow a (geometrically meaningful) product structure while maintaining a finite dimensionality as opposed to tensor product
representations. Through geometric products, we can transform vector-valued information, resulting
in a more accurate and nuanced interpretation of the underlying structures compared to scalarization
methods. Further, we can represent exotic geometric objects such as pseudovectors, encountered
in certain physics problems, which transform in a nonstandard manner. Third, our method readily
generalizes to orthogonal groups regardless of the dimension or metric signature of the space, thereby
attaining O(n)- or E(n)-equivariance. These advantages are demonstrated on equivariance bench2
marks of different dimensionality. Note that specialized tools were developed for several of these
tasks, while CGENNs can be applied more generally.
2 The Clifford Algebra
Clifford algebras (also known as geometric algebras) are powerful mathematical objects with applications in various areas of science and engineering. For a complete formal construction, we
refer the reader to Appendix D. Let V be a finite-dimensional vector space over a field F equipped
with a quadratic form q : V → F. The Clifford algebra Cl(V, q) is the unitary, associative, noncommutative algebra generated by V such that for every v ∈ V the relation v
2 = q(v) holds, i.e.,
vectors square to scalars. This simple property solely generates a unique mathematical theory
that underpins many applications. Note that every element x of the Clifford algebra Cl(V, q) is a
linear combination of (formal, non-commutative) products of vectors modulo the condition that
every appearing square v
2 gets identified with the scalar square q(v): x =
P
i∈I
ci
· vi,1 · · · vi,ki
2
.
Here, the index set I is finite, ci ∈ F, k ∈ N0, vi,j ∈ V . The Clifford algebra’s associated bilinear form b(v1, v2) := 1
2
(q(v1 + v2) − q(v1) − q(v2)) yields the fundamental Clifford identity:
v1v2 + v2v1 = 2b(v1, v2) for v1, v2 ∈ V (Lemma D.3). In this context, the quantity v1v2 represents the geometric product, which is aptly named for its ability to compute geometric properties
and facilitate various transformations. Note that when v1, v2 are orthogonal (e.g., for orthogonal
basis vectors), b(v1, v2) = 0, in which case v1v2 = −v2v1. The dimensionality of the algebra
is 2
n, where n := dim V (Theorem D.15). Let e1, . . . , en be an orthogonal basis of V . The tuple (eA)A⊆[n]
, [n] := {1, . . . , n}, is an orthogonal basis for Cl(V, q), where for all such A the
product eA := Q<
i∈A ei
is taken in increasing order (Theorem D.26). We will see below that we
can decompose the algebra into vector subspaces Cl(m)
(V, q), m = 0, . . . , n, called grades, where
dim Cl(m)
(V, q) =  n
m

. Elements of grade m = 0 and m = 1 are scalars (Cl(0)(V, q) = F) and
vectors (Cl(1)(V, q) = V ), respectively, while elements of grade m = 2 and m = 3 are referred to as
bivectors and trivectors. Similar terms are used for elements of even higher grade. These higher-order
grades can represent (oriented) points, areas, and volumes, as depicted in Figure 1.
Clifford algebras provide tools that allow for meaningful algebraic representation and manipulation
of geometric quantities, including areas and volumes [RDK21, DM02, DFM09]. In addition, they
offer generalizations such as extensions of the exterior and Grassmannian algebras, along with the
natural inclusion of complex numbers and Hamilton’s quaternions [Gra62, Ham66]. Applications
of Clifford algebras can be found in robotics [BCRLZE06, HZBC08], computer graphics [WCL05,
BTH22], signal processing [Hit12, BMQQS+21], and animation [HP10, CC12]. In the context of
machine learning, Clifford algebras and hypercomplex numbers have been employed to improve the
performance of algorithms in various tasks. For example, [MFW21] learn an equivariant embedding
using geometric neurons used for classification tasks. Further, [Spe21] introduce geometric algebra
attention networks for point cloud problems in physics, chemistry, and biology. More recently,
[BBWG22] introduce Clifford neural layers and Clifford Fourier transforms for accelerating solving
partial differential equations. [RGdK+23] continue this direction, strengthening the geometric
inductive bias by the introduction of geometric templates. Concurrently with this work, [BDHBC23]
develop the geometric algebra transformer. Further, [LK14, PRM+18, ZXXC18] introduce complexvalued and quaternion-valued networks. Finally, [KIdHN23] define normalizing flows on the group
of unit quaternions for sampling molecular crystal structures


**ASSISTANT**

The introduction of Clifford Group Equivariant Neural Networks (CGENNs) represents a significant advancement in the field of neural networks, particularly for tasks requiring the handling of complex geometric transformations and maintaining symmetries inherent in the data. This concept leverages the rich mathematical framework of Clifford algebras to create neural network architectures that are naturally equivariant to orthogonal and Euclidean transformations.

### Overview of Clifford Group Equivariant Neural Networks

CGENNs are designed to utilize the full structure of Clifford algebras, not just as a computational tool but as an integral part of the network's architecture. This approach allows the networks to inherently respect the geometric properties of data, such as orientations, rotations, and reflections.

#### Key Features and Mathematical Foundations

1. **Clifford Algebra**: At the heart of CGENNs is the Clifford algebra, which provides a way to handle multivectors—elements that can represent points, lines, planes, and volumes. Clifford algebras are generated from vector spaces equipped with a quadratic form, and they extend the scalar and vector concepts to higher dimensions through the geometric product.

2. **Clifford Group Action**: The Clifford group, a subgroup within the Clifford algebra, acts through what's described as twisted conjugation. This action is an orthogonal automorphism of the algebra that respects the grading of multivectors. The group action ensures that transformations preserve the geometric relationships encoded by the multivectors, making it ideal for tasks where data is represented in these terms.

3. **Equivariance**: The CGENNs architecture ensures that the neural network is equivariant under the action of the Clifford group. This means that the output of the network transforms predictably under the action of the group on the input, which is crucial for tasks involving symmetrical properties or where the orientation and position of data points are significant.

#### Applications and Benefits

- **Dimensional Generality**: CGENNs can handle data spaces of various dimensions without the need for reparameterization, making them extremely versatile and scalable across different kinds of applications, from 3D physical simulations to higher-dimensional spaces.

- **Handling Complex Geometries**: By leveraging the multivector structure and geometric product of Clifford algebras, CGENNs can efficiently manage data representing complex geometric properties like volumes and orientations. This is particularly beneficial in fields like physics, robotics, and computer graphics.

- **Performance**: Initial implementations and tests show that CGENNs can achieve state-of-the-art performance on various tasks that benefit from understanding the geometric and symmetrical structure of the data, such as n-body simulations, high-energy physics experiments, and convex hull calculations in higher dimensions.

### Conclusion

Clifford Group Equivariant Neural Networks mark a substantial progress in the integration of advanced mathematical concepts into machine learning frameworks. By directly incorporating the algebraic and geometric properties of Clifford algebras, CGENNs provide a powerful tool for processing and analyzing data in ways that conventional neural network models cannot, particularly in tasks where the geometric structure and symmetries are key. This approach not only enhances the capability and accuracy of neural networks in specific applications but also opens new avenues for research into equivariant machine learning models.


---

preceding: [[clifford algebras]]  


following: [[theoretical framework of clifford group equivariant neural networks]]

See also: [[signal processing]]


See also: [[vector space]]


See also: [[normalizing flows]]


See also: [[clifford algebra]]


See also: [[geometric algebra]]


See also: [[theoretical framework of clifford group equivariant neural networks]]


See also: [[convex hull]]


See also: [[metric signature]]


See also: [[inductive bias]]


See also: [[methodology of clifford group equivariant neural networks (cgenns)]]


See also: [[partial differential equations]]


See also: [[spherical harmonics]]