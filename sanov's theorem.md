---
date: 1721687818.624802
tags:
- autogenerated
- source__oai
- conversation_id__a2d4a2dc-4888-4b9b-aa34-ef6394b04fb7
- msg_aaa2b5f8-ad6e-4a3c-bed9-53efdf40ee35
- msg_7518685f-d46c-4b06-b296-f139f98a39ce
- colorclass/information-theory
title: sanov's theorem
---

**USER**

Sanov's Theorem


**ASSISTANT**

## Sanov's Theorem

### Overview

Sanov's theorem is a fundamental result in large deviations theory, describing the probabilities of rare events for empirical measures of independent and identically distributed (i.i.d.) random variables. It provides a rigorous framework for understanding the exponential decay of probabilities associated with deviations of empirical distributions from the true distribution.

### Statement of the Theorem

Let \(\{X_i\}_{i=1}^n\) be a sequence of i.i.d. random variables taking values in a finite set \(\mathcal{X}\) with common probability distribution \(P\). The empirical measure \(\hat{P}_n\) is defined as:

$$
\hat{P}_n(x) = \frac{1}{n} \sum_{i=1}^n \delta_{X_i}(x),
$$

where \(\delta_{X_i}(x)\) is the Dirac delta function centered at \(X_i\). The empirical measure \(\hat{P}_n\) represents the fraction of times each element \(x \in \mathcal{X}\) appears in the sample.

#### Large Deviations Principle (LDP)

Sanov's theorem states that the sequence of empirical measures \(\{\hat{P}_n\}\) satisfies a [[large deviations principle]] (LDP) with the rate function given by the Kullback-Leibler divergence ([[relative entropy]]) \(D(Q||P)\) between the empirical measure \(Q\) and the true distribution \(P\):

$$
I(Q) = D(Q || P) = \sum_{x \in \mathcal{X}} Q(x) \log \frac{Q(x)}{P(x)}.
$$

#### Formal Statement

For any measurable set \(A \subset \mathcal{M}_1(\mathcal{X})\) (the space of probability measures on \(\mathcal{X}\)),

$$
-\inf_{Q \in A^\circ} D(Q||P) \leq \liminf_{n \to \infty} \frac{1}{n} \log \Pr(\hat{P}_n \in A) \leq \limsup_{n \to \infty} \frac{1}{n} \log \Pr(\hat{P}_n \in A) \leq -\inf_{Q \in \overline{A}} D(Q||P),
$$

where \(A^\circ\) is the interior of \(A\) and \(\overline{A}\) is the closure of \(A\).

### Intuition and Interpretation

Sanov's theorem essentially states that the probability that the empirical distribution \(\hat{P}_n\) deviates significantly from the true distribution \(P\) decays exponentially with the sample size \(n\), and the rate of this decay is determined by the [[relative entropy]] \(D(Q||P)\). The [[relative entropy]] measures the "distance" between the empirical measure \(Q\) and the true distribution \(P\).

### Applications

#### Statistical Mechanics

In statistical mechanics, Sanov's theorem can be applied to understand the distribution of microstates in the canonical ensemble. For example, it helps in quantifying the probability of observing a macroscopic state with an empirical distribution that deviates from the equilibrium distribution.

#### Information Theory

In information theory, Sanov's theorem is used to analyze the likelihood of observing atypical sequences in a data source. This has applications in coding theory, where it helps in deriving bounds on the probabilities of decoding errors.

#### Hypothesis Testing

Sanov's theorem is also used in hypothesis testing, particularly in the context of the likelihood ratio test. It provides a way to evaluate the probability of type I errors (false positives) by quantifying the likelihood of empirical distributions deviating from the null hypothesis.

### Examples

#### Example 1: Coin Tosses

Consider a sequence of \(n\) independent coin tosses, where each coin has a probability \(p\) of landing heads and \(1-p\) of landing tails. Let \(X_i \in \{0, 1\}\) denote the outcome of the \(i\)-th toss (1 for heads, 0 for tails). The empirical measure \(\hat{P}_n\) for heads is the fraction of heads observed in \(n\) tosses.

According to Sanov's theorem, the probability of observing a fraction \(q\) of heads that deviates from \(p\) satisfies:

$$
\Pr(\hat{P}_n \approx q) \sim \exp(-n D(q||p)),
$$

where \(D(q||p) = q \log \frac{q}{p} + (1-q) \log \frac{1-q}{1-p}\) is the Kullback-Leibler divergence between the empirical measure \(q\) and the true distribution \(p\).

#### Example 2: Multinomial Distribution

Consider a multinomial distribution with \(k\) categories, where the probability of observing category \(i\) is \(p_i\). For a sample of size \(n\), the empirical measure \(\hat{P}_n\) gives the observed frequencies of each category.

Sanov's theorem provides the large deviations rate function for the empirical distribution \(\hat{P}_n\). The probability of observing an empirical distribution \(Q\) that deviates from the true distribution \(P\) is given by:

$$
\Pr(\hat{P}_n \approx Q) \sim \exp(-n D(Q||P)),
$$

where \(D(Q||P) = \sum_{i=1}^k Q_i \log \frac{Q_i}{P_i}\).

### Further Reading

- [[Large Deviations Theory]]
- [[Gartner-Ellis Theorem]]
- [[Cram√©r's Theorem]]
- [[Legendre-Fenchel Transform]]
- [[Relative Entropy]]
- [[Spin Glass Models]]
- [[Sherrington-Kirkpatrick Model]]
- [[Replica Symmetry Breaking]]

Sanov's theorem is a powerful tool in large deviations theory, providing deep insights into the behavior of empirical measures and the probabilities of rare events. For a comprehensive exploration, refer to specialized texts in probability theory and statistical mechanics.


---

preceding: [[large deviations theory]]  


following: [[varadhan's lemma]]

See also: [[important results in stochastic processes theory]]


See also: [[girsanov's theorem]]


See also: [[probability measure]]


See also: [[feynman-kac formula]]


See also: [[probability theory]]


See also: [[information theory]]


See also: [[independent and identically distributed]]


See also: [[coding theory]]


See also: [[random variables]]


See also: [[martingales]]