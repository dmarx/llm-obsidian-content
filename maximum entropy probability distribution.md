---
date: 1724991940.497158
tags:
- autogenerated
- source__oai
- conversation_id__ad2150e2-e599-43ae-9dc4-259111e0c751
- msg_aaa2b139-dd81-4700-917b-b936dc3bb789
- msg_d6cf654a-da6c-4ca0-ba3c-213da0ad4aca
- colorclass/systems-theory
title: maximum entropy probability distribution
---

**USER**

Maximum entropy probability distribution


**ASSISTANT**

### Maximum Entropy Probability Distribution

The **maximum entropy probability distribution** is a fundamental concept in information theory and statistical mechanics. It is a probability distribution that maximizes entropy, given certain constraints. The **Maximum Entropy Principle (MaxEnt)**, introduced by [[Edwin Jaynes]], asserts that, when trying to determine a probability distribution, one should select the distribution that has the maximum entropy among all distributions that satisfy the known constraints. This principle ensures that no additional assumptions or biases are introduced beyond the available information.

#### Definition of Entropy

In information theory, the entropy $H(p)$ of a discrete random variable $X$ with a probability distribution $p(x)$ is defined as:

$$
H(p) = -\sum_{x \in \mathcal{X}} p(x) \log p(x),
$$

where:
- $\mathcal{X}$ is the set of all possible outcomes of $X$,
- $p(x)$ is the probability of the outcome $x$.

For continuous random variables with a probability density function $p(x)$, the entropy $H(p)$ is given by:

$$
H(p) = -\int_{\mathcal{X}} p(x) \log p(x) \, dx.
$$

The entropy measures the uncertainty or unpredictability of a probability distribution. A higher entropy value indicates a more uniform distribution, where each outcome is equally likely, while a lower entropy value indicates a more certain or peaked distribution.

#### Maximum Entropy Principle

The Maximum Entropy Principle states that the probability distribution that best represents the current state of knowledge about a system is the one with the maximum entropy, subject to the constraints imposed by the known information. This approach ensures that no extra assumptions are added, and only the information provided is used to derive the distribution.

To formalize this, we seek the distribution $p(x)$ that maximizes $H(p)$ subject to some constraints. The typical constraints are:
1. **Normalization**: The probabilities must sum to 1 for discrete distributions, or integrate to 1 for continuous distributions:

   $$
   \sum_{x \in \mathcal{X}} p(x) = 1, \quad \text{or} \quad \int_{\mathcal{X}} p(x) \, dx = 1.
   $$

2. **Expected Value Constraints**: The expected values of certain functions (e.g., the mean, variance) must match the known values. For example, if the mean $\mu$ of $X$ is known, we have:

   $$
   \sum_{x \in \mathcal{X}} x p(x) = \mu, \quad \text{or} \quad \int_{\mathcal{X}} x p(x) \, dx = \mu.
   $$

#### Derivation of Maximum Entropy Distributions

To derive the maximum entropy distribution under specific constraints, we use the **method of Lagrange multipliers** to maximize $H(p)$ while satisfying the constraints.

##### Example 1: Discrete Uniform Distribution

If no information is known about the outcomes other than the fact that they must sum to 1, the maximum entropy distribution is the **uniform distribution**. For a discrete set of $n$ possible outcomes $\{x_1, x_2, \ldots, x_n\}$:

- **Objective**: Maximize $H(p) = -\sum_{i=1}^n p(x_i) \log p(x_i)$,
- **Constraint**: $\sum_{i=1}^n p(x_i) = 1$.

Using a Lagrange multiplier $\lambda$, the Lagrangian $\mathcal{L}$ is:

$$
\mathcal{L}(p(x_1), \ldots, p(x_n), \lambda) = -\sum_{i=1}^n p(x_i) \log p(x_i) + \lambda \left( \sum_{i=1}^n p(x_i) - 1 \right).
$$

Taking the partial derivative with respect to each $p(x_i)$ and setting it to zero gives:

$$
-\log p(x_i) - 1 + \lambda = 0 \implies p(x_i) = e^{\lambda - 1}.
$$

Using the normalization constraint $\sum_{i=1}^n p(x_i) = 1$, we find that:

$$
p(x_i) = \frac{1}{n}.
$$

Thus, the maximum entropy distribution is the discrete uniform distribution, where each outcome is equally likely.

##### Example 2: Maximum Entropy with Known Mean (Exponential Distribution)

For a continuous random variable where the only known constraint is the expected value (mean) $\langle x \rangle = \mu$, the maximum entropy distribution is the **exponential distribution**.

- **Objective**: Maximize $H(p) = -\int_{0}^{\infty} p(x) \log p(x) \, dx$,
- **Constraints**:
  - Normalization: $\int_{0}^{\infty} p(x) \, dx = 1$,
  - Known mean: $\int_{0}^{\infty} x p(x) \, dx = \mu$.

The Lagrangian $\mathcal{L}$ is:

$$
\mathcal{L}(p(x), \lambda_0, \lambda_1) = -\int_{0}^{\infty} p(x) \log p(x) \, dx + \lambda_0 \left( \int_{0}^{\infty} p(x) \, dx - 1 \right) + \lambda_1 \left( \int_{0}^{\infty} x p(x) \, dx - \mu \right).
$$

Taking the functional derivative with respect to $p(x)$ and setting it to zero:

$$
\frac{\delta \mathcal{L}}{\delta p(x)} = -\log p(x) - 1 + \lambda_0 + \lambda_1 x = 0.
$$

Solving for $p(x)$, we get:

$$
p(x) = e^{-(1 - \lambda_0) - \lambda_1 x}.
$$

Letting $1 - \lambda_0 = \log Z$ (normalization constant) and $\lambda_1 = \beta$, we have:

$$
p(x) = \frac{1}{Z} e^{-\beta x}.
$$

Applying the normalization constraint:

$$
\int_{0}^{\infty} \frac{1}{Z} e^{-\beta x} \, dx = 1 \implies Z = \frac{1}{\beta}.
$$

Thus, the maximum entropy distribution with a known mean $\mu$ is:

$$
p(x) = \beta e^{-\beta x}, \quad \text{where} \quad \beta = \frac{1}{\mu}.
$$

This is the **exponential distribution** with parameter $\beta = 1/\mu$.

##### Example 3: Maximum Entropy with Known Mean and Variance (Normal Distribution)

When both the mean $\langle x \rangle = \mu$ and variance $\langle (x - \mu)^2 \rangle = \sigma^2$ of a continuous random variable are known, the maximum entropy distribution is the **normal distribution** (Gaussian distribution).

- **Objective**: Maximize $H(p) = -\int_{-\infty}^{\infty} p(x) \log p(x) \, dx$,
- **Constraints**:
  - Normalization: $\int_{-\infty}^{\infty} p(x) \, dx = 1$,
  - Known mean: $\int_{-\infty}^{\infty} x p(x) \, dx = \mu$,
  - Known variance: $\int_{-\infty}^{\infty} (x - \mu)^2 p(x) \, dx = \sigma^2$.

The Lagrangian $\mathcal{L}$ becomes:

$$
\mathcal{L}(p(x), \lambda_0, \lambda_1, \lambda_2) = -\int_{-\infty}^{\infty} p(x) \log p(x) \, dx + \lambda_0 \left( \int_{-\infty}^{\infty} p(x) \, dx - 1 \right) + \lambda_1 \left( \int_{-\infty}^{\infty} x p(x) \, dx - \mu \right) + \lambda_2 \left( \int_{-\infty}^{\infty} (x - \mu)^2 p(x) \, dx - \sigma^2 \right).
$$

Taking the functional derivative with respect to $p(x)$ and setting it to zero:

$$
\frac{\delta \mathcal{L}}{\delta p(x)} = -\log p(x) - 1 + \lambda_0 + \lambda_1 x + \lambda_2 (x - \mu)^2 = 0.
$$

Solving for $p(x)$, we have:

$$
p(x) = e^{-(1 - \lambda_0) - \lambda_1 x - \lambda_2 (x - \mu)^2}.
$$

Letting $1 - \lambda_0 = \log Z$ (normalization constant), $\lambda_1 = 0$, and $\lambda_2 = \frac{1}{2\sigma^2}$, we obtain:

$$
p(x) = \frac{1}{Z}


---

preceding: [[ludwig boltzmann's statistical interpretation of thermodynamics]]  


following: [[principle of maximum caliber]]

See also: [[information theory]]


See also: [[random variables]]