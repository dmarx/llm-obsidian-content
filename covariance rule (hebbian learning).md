---
date: 1716602248.231624
tags:
- autogenerated
- source__oai
- conversation_id__1da89a1a-bc3f-4b71-a716-88fe28b143ec
- msg_aaa22d44-cacd-46ee-9406-a03c91ac068c
- msg_8b074907-be53-46b0-a911-f96a35e860ef
- colorclass/vector-space
title: covariance rule (hebbian learning)
---

**USER**

covariance rule ([[hebbian learning]])


**ASSISTANT**

## Covariance Rule (Hebbian Learning)

### Introduction

The covariance rule is a modification of the basic Hebbian learning rule that accounts for the mean activity of neurons. This adjustment allows the rule to focus on the covariance between the activities of pre- and postsynaptic neurons, rather than just their raw activity levels. The covariance rule helps to ensure that synaptic changes are sensitive to deviations from the average activity, making it more robust to varying activity levels.

### Key Concepts

1. **Covariance**: Measures how two variables change together, focusing on deviations from their mean values.
2. **Hebbian Learning**: A principle where the synaptic weight is strengthened if the activities of pre- and postsynaptic neurons are correlated.
3. **Mean Activity**: The average activity level of a neuron over time, which is subtracted in the covariance rule to focus on fluctuations.

### Mathematical Formulation

The basic Hebbian learning rule can be expressed as:

$$
\Delta w_{ij} = \eta \, x_i \, y_j
$$

where:
- \( \Delta w_{ij} \) is the change in the synaptic weight between presynaptic neuron \( i \) and postsynaptic neuron \( j \),
- \( \eta \) is the learning rate,
- \( x_i \) is the activity of the presynaptic neuron \( i \),
- \( y_j \) is the activity of the postsynaptic neuron \( j \).

The covariance rule modifies this update to include the mean activities:

$$
\Delta w_{ij} = \eta \, (x_i - \langle x_i \rangle) \, (y_j - \langle y_j \rangle)
$$

where:
- \( \langle x_i \rangle \) is the mean activity of the presynaptic neuron \( i \),
- \( \langle y_j \rangle \) is the mean activity of the postsynaptic neuron \( j \).

This ensures that the synaptic weight changes are based on the deviations from the mean activities, emphasizing the covariance between the activities.

### Properties and Interpretation

1. **Centering**: By subtracting the mean activities, the rule centers the synaptic updates around zero, focusing on the covariance.
2. **Robustness**: The covariance rule is more robust to varying levels of neural activity, reducing the impact of overall activity levels on synaptic changes.
3. **Stability**: Centering helps in maintaining stability in the learning process by preventing runaway growth of synaptic weights.

### Numerical Example: Covariance Rule in a Simple Network

Below is a Python example demonstrating the covariance rule for Hebbian learning in a simple neural network:

```python
import numpy as np
import matplotlib.pyplot as plt

# Parameters
eta = 0.01  # Learning rate
n_neurons = 2  # Number of neurons
n_samples = 1000  # Number of samples

# Generate random input data with a specific covariance matrix
mean = [0, 0]
cov = [[1, 0.8], [0.8, 1]]
data = np.random.multivariate_normal(mean, cov, n_samples)

# Compute the mean activity of the input data
mean_activity = np.mean(data, axis=0)

# Initialize weights
weights = np.random.rand(n_neurons)

# Covariance rule for online learning
for sample in data:
    centered_sample = sample - mean_activity
    y = np.dot(weights, centered_sample)
    weights += eta * y * centered_sample

# Plot the input data and the learned principal component
plt.scatter(data[:, 0], data[:, 1], alpha=0.2, label='Input Data')
plt.quiver(0, 0, weights[0], weights[1], color='r', scale=5, label='Learned Component')
plt.xlabel('X1')
plt.ylabel('X2')
plt.legend()
plt.title('Covariance Rule for Hebbian Learning')
plt.show()
```

### Applications

1. **Neural Networks**: Training neural networks with robust and stable synaptic weight updates.
2. **Principal Component Analysis (PCA)**: Analyzing the principal components of input data by focusing on covariance.
3. **Signal Processing**: Enhancing signal decorrelation and feature extraction.
4. **Pattern Recognition**: Improving feature extraction for pattern recognition tasks.

### Conclusion

The covariance rule is a significant enhancement to the basic Hebbian learning rule, focusing on the covariance between pre- and postsynaptic neuron activities. This rule makes the learning process more robust and stable by centering synaptic updates around deviations from mean activities. It finds applications in neural networks, principal component analysis, signal processing, and pattern recognition. For further exploration, refer to the pages on [[Hebbian Learning|Hebbian learning]], [[Principal Component Analysis|principal component analysis]], and [[Neural Networks|neural networks]].


---

preceding: [[oja's rule]]  


following: [[oja's rule as online pca]]

See also: [[signal processing]]


See also: [[principal component analysis]]


See also: [[hebbian learning]]