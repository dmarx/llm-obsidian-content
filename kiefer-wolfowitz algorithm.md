---
date: 1717804954.155593
tags:
- autogenerated
- source__oai
- conversation_id__77e70e10-f02a-4f98-bd5c-6217b10122cf
- msg_aaa297de-6ac8-4a38-9f16-162000cac2f0
- msg_68b436af-fa54-43c4-b545-7803ce332f3e
- colorclass/information-theory
title: kiefer-wolfowitz algorithm
---

**USER**

Kiefer-Wolfowitz Algorithm


**ASSISTANT**

The Kiefer-Wolfowitz algorithm is a stochastic approximation method used for optimizing an objective function when the gradient of the function is not directly available. It is particularly useful in situations where only noisy measurements of the function values are obtainable. This algorithm is named after Jack Kiefer and Jacob Wolfowitz, who introduced it in 1952.

### Kiefer-Wolfowitz Algorithm

The Kiefer-Wolfowitz algorithm is a finite-difference method for estimating the gradient of an objective function and using this estimate to perform stochastic gradient descent. It is defined as follows:

Given an objective function \( f(\theta) \) where \(\theta \in \mathbb{R}^d \) is the parameter vector we aim to optimize, the algorithm updates the parameter vector iteratively using the following steps:

1. **Initialization**: Choose an initial guess \(\theta_0 \in \mathbb{R}^d \), a step size sequence \(\{\gamma_k\}\), and a perturbation sequence \(\{c_k\}\).

2. **Finite-Difference Gradient Estimate**:
   At iteration \( k \), perturb the parameter \(\theta_k\) by \(c_k e_j\) where \(e_j\) is the \(j\)-th unit vector in \(\mathbb{R}^d\), and evaluate the function at the perturbed points:
   \[
   \theta_k^{(j+)} = \theta_k + c_k e_j,
   \]
   \[
   \theta_k^{(j-)} = \theta_k - c_k e_j.
   \]
   Compute the noisy observations of the function values at these points:
   \[
   Y_k^{(j+)} = f(\theta_k^{(j+)}) + \zeta_k^{(j+)},
   \]
   \[
   Y_k^{(j-)} = f(\theta_k^{(j-)}) + \zeta_k^{(j-)},
   \]
   where \(\zeta_k^{(j+)}\) and \(\zeta_k^{(j-)}\) are noise terms.

3. **Gradient Estimate**:
   Estimate the gradient \(\nabla f(\theta_k)\) using the finite differences:
   \[
   \hat{\nabla} f(\theta_k)_j = \frac{Y_k^{(j+)} - Y_k^{(j-)}}{2c_k}.
   \]

4. **Parameter Update**:
   Update the parameter vector using the estimated gradient:
   \[
   \theta_{k+1} = \theta_k - \gamma_k \hat{\nabla} f(\theta_k).
   \]

5. **Iteration**:
   Repeat steps 2-4 for \( k = 0, 1, 2, \ldots \) until convergence.

### Convergence Conditions

For the Kiefer-Wolfowitz algorithm to converge to the true optimum, certain conditions must be satisfied:

1. **Step Size Sequence**:
   The step sizes \(\{\gamma_k\}\) should satisfy:
   \[
   \sum_{k=0}^{\infty} \gamma_k = \infty \quad \text{and} \quad \sum_{k=0}^{\infty} \gamma_k^2 < \infty.
   \]
   A common choice is \(\gamma_k = \frac{\gamma_0}{k}\) for some \(\gamma_0 > 0\).

2. **Perturbation Sequence**:
   The perturbation sequence \(\{c_k\}\) should decrease to 0, but not too quickly. A typical choice is \(c_k = \frac{c_0}{k^\alpha}\) with \(0.5 < \alpha \leq 1\).

3. **Noise Conditions**:
   The noise terms \(\zeta_k^{(j+)}\) and \(\zeta_k^{(j-)}\) should be zero-mean and have finite variances.

### Applications

The Kiefer-Wolfowitz algorithm is widely used in various applications where the gradient of the objective function is not directly available:

1. **Simulation Optimization**:
   Used in optimizing performance measures in simulations where gradients cannot be computed analytically.

2. **Machine Learning**:
   Applied in scenarios where the loss function is noisy, such as reinforcement learning and stochastic optimization.

3. **Control Systems**:
   Used for adaptive control where the system model is unknown or partially known, and only noisy measurements of the system output are available.

4. **Econometrics and Finance**:
   Employed in estimating models with noisy economic or financial data.

### Example

Consider optimizing a function \( f(\theta) = \theta^2 \) with noise \(\zeta_k\) added to the function evaluations. The update steps of the Kiefer-Wolfowitz algorithm are:

1. **Initialization**: Set \(\theta_0 = 1\), \(\gamma_k = \frac{1}{k+1}\), and \(c_k = \frac{1}{\sqrt{k+1}}\).

2. **Finite-Difference Gradient Estimate**:
   \[
   \theta_k^{(+)} = \theta_k + c_k,
   \]
   \[
   \theta_k^{(-)} = \theta_k - c_k.
   \]
   Evaluate the noisy function values:
   \[
   Y_k^{(+)} = (\theta_k + c_k)^2 + \zeta_k^{(+)} = \theta_k^2 + 2\theta_k c_k + c_k^2 + \zeta_k^{(+)},
   \]
   \[
   Y_k^{(-)} = (\theta_k - c_k)^2 + \zeta_k^{(-)} = \theta_k^2 - 2\theta_k c_k + c_k^2 + \zeta_k^{(-)}.
   \]

3. **Gradient Estimate**:
   \[
   \hat{\nabla} f(\theta_k) = \frac{Y_k^{(+)} - Y_k^{(-)}}{2c_k} = \frac{4\theta_k c_k + \zeta_k^{(+)} - \zeta_k^{(-)}}{2c_k} = 2\theta_k + \frac{\zeta_k^{(+)} - \zeta_k^{(-)}}{2c_k}.
   \]

4. **Parameter Update**:
   \[
   \theta_{k+1} = \theta_k - \gamma_k \hat{\nabla} f(\theta_k) = \theta_k - \frac{1}{k+1} \left( 2\theta_k + \frac{\zeta_k^{(+)} - \zeta_k^{(-)}}{2c_k} \right).
   \]

5. **Iteration**:
   Repeat the steps for \( k = 0, 1, 2, \ldots \) until convergence.

### Further Reading

For a comprehensive study of the Kiefer-Wolfowitz algorithm and related topics, consider the following references:

- "Stochastic Approximation and Recursive Algorithms and Applications" by Harold Kushner and G. George Yin.
- "Introduction to Stochastic Search and Optimization" by James C. Spall.
- "Optimization by Vector Space Methods" by David G. Luenberger.

These texts provide detailed theoretical analyses, proofs, and a wide range of applications of the Kiefer-Wolfowitz algorithm in various fields.


---

preceding: [[birkhoff's ergodic theorem]]  


following: [[martingale difference sequence]]

See also: [[reinforcement learning]]


See also: [[adaptive control]]