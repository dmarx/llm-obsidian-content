---
date: 1733004579.872
tags:
- autogenerated
- source__oai
- conversation_id__674b8ce2-60d4-8008-9194-5dd50dd403c7
- msg_aaa20099-bf7c-4310-8529-07bd50b6766a
- msg_b43679b9-4e5f-47ab-b097-85fc11b92501
- colorclass/systems-theory
title: gibbs entropy
---

**USER**

gibbs entropy


**ASSISTANT**

The **Gibbs entropy** is a generalization of the [[Boltzmann entropy]] that applies to systems where the probability distribution over microstates is non-uniform, such as systems not in equilibrium. It is a fundamental concept in [[statistical mechanics]] and is given by:

$$
S = -k_B \sum_i p_i \ln p_i
$$

or, in the continuous case, for a probability density function $\rho(x)$ over the phase space:

$$
S = -k_B \int \rho(x) \ln \rho(x) \, dx
$$

where:

- $S$ is the entropy of the system,
- $k_B$ is the [[Boltzmann constant]],
- $p_i$ is the probability of the system being in the $i$th microstate,
- $\rho(x)$ is the probability density function in phase space,
- $x$ represents a point in the phase space of the system.

---

### Key Features of Gibbs Entropy

#### 1. **Probabilistic Nature**
Unlike the [[Boltzmann entropy]], which assumes a uniform distribution over all accessible microstates (where $p_i = 1/\Omega$), the Gibbs entropy accounts for cases where the system has a non-uniform distribution of microstates. This makes it applicable to non-equilibrium systems and systems with partial information.

#### 2. **Connection to Boltzmann Entropy**
If the system is in equilibrium and all accessible microstates are equally probable, $p_i = 1/\Omega$, the Gibbs entropy reduces to the Boltzmann entropy:

$$
S = -k_B \sum_{i=1}^\Omega \frac{1}{\Omega} \ln\left(\frac{1}{\Omega}\right) = k_B \ln \Omega.
$$

Thus, the Boltzmann entropy is a special case of the Gibbs entropy.

---

### Applications of Gibbs Entropy

#### a. **Equilibrium Statistical Mechanics**
In equilibrium, the Gibbs entropy provides a connection to thermodynamic entropy. For the canonical ensemble, the probability $p_i$ of each state $i$ is given by the [[Boltzmann distribution]]:

$$
p_i = \frac{e^{-\beta E_i}}{Z},
$$

where:
- $\beta = 1/(k_B T)$ is the inverse temperature,
- $E_i$ is the energy of the $i$th microstate,
- $Z$ is the [[partition function]].

The Gibbs entropy then takes the form:

$$
S = k_B \ln Z + k_B \beta \langle E \rangle,
$$

where $\langle E \rangle$ is the average energy.

#### b. **Non-Equilibrium Systems**
The Gibbs entropy is particularly useful for systems out of equilibrium. For example, in information theory, it measures the uncertainty (or information content) of a system's microstate distribution.

#### c. **Quantum Systems**
In quantum statistical mechanics, the Gibbs entropy generalizes to the **von Neumann entropy**, defined in terms of the density matrix $\rho$:

$$
S = -k_B \operatorname{Tr}(\rho \ln \rho).
$$

---

### Maximization Principle and Second Law of Thermodynamics
The Gibbs entropy naturally increases over time for isolated systems, reflecting the [[second law of thermodynamics]]. This is consistent with the fact that the system's probability distribution tends to spread out over time, maximizing entropy at equilibrium.

---

### Interpretation in Information Theory
The form of the Gibbs entropy is closely related to the [[Shannon entropy]] in information theory:

$$
H = -\sum_i p_i \log p_i,
$$

where the base of the logarithm depends on the units of information (e.g., bits for base 2 or nats for base $e$). This analogy highlights the deep connection between statistical mechanics and information theory.


---

preceding: [[boltzmann entropy]]  


following: [[shannon-mcmillan-breiman theorem]]

See also: [[information theory]]