---
date: 1726443491.507721
tags:
- autogenerated
- source__oai
- conversation_id__66e757d1-0534-8008-bb6a-afa8342b5ee2
- msg_14f6e3b1-383b-42e7-9960-fd44bd4ba7a3
- msg_aaa25f92-9fa7-4418-a8cd-6ca6c64a1a3c
- msg_e0361d5f-6803-4e63-abef-b46e9c9716ef
- colorclass/decision-making-processes
title: that's definitely content we should include as well, but i actually meant in
  this context
---

**ASSISTANT**

to take on multiple forms and functions in a single conversation.


**USER**

that's definitely content we should include as well, but I actually meant in this context:

Role-Play with Large Language Models
Murray Shanahan ∗1,2, Kyle McDonell †3
, and Laria Reynolds ‡3
1DeepMind
2
Imperial College London
3Eleuther AI
May 2023
Abstract
As dialogue agents become increasingly humanlike in their performance, it is imperative that
we develop effective ways to describe their behaviour in high-level terms without falling into
the trap of anthropomorphism. In this paper, we
foreground the concept of role-play. Casting dialogue agent behaviour in terms of role-play allows
us to draw on familiar folk psychological terms,
without ascribing human characteristics to language models they in fact lack. Two important
cases of dialogue agent behaviour are addressed
this way, namely (apparent) deception and (apparent) self-awareness.
1 Introduction
Large language models (LLMs) have numerous
use cases, and can be prompted to exhibit a wide
variety of behaviours, including dialogue, which
can produce a compelling sense of being in the
presence of a human-like interlocutor. However,
LLM-based dialogue agents are, in multiple respects, very different from human beings. A human’s language skills are an extension of the cognitive capacities they develop through embodied
interaction with the world, and are acquired by
growing up in a community of other language
users who also inhabit that world. An LLM, by
contrast, is a disembodied neural network that
has been trained on a large corpus of humangenerated text with the objective of predicting
the next word (token) given a sequence of words
(tokens) as context.
Despite these fundamental dissimilarities, a
∗m.shanahan@imperial.ac.uk
†
kyle@eleuther.ai
‡
laria@eleuther.ai.
suitably prompted and sampled LLM can be embedded in a turn-taking dialogue system and
mimic human language use convincingly, and
this presents us with a difficult dilemma. On
the one hand, it’s natural to use the same
folk-psychological language to describe dialogue
agents that we use to describe human behaviour,
to freely deploy words like “knows”, “understands”, and “thinks”. Attempting to avoid
such phrases by using more scientifically precise
substitutes often results in prose that is clumsy
and hard to follow. On the other hand, taken
too literally, such language promotes anthropomorphism, exaggerating the similarities between
these AI systems and humans while obscuring
their deep differences (Shanahan, 2023).
If the conceptual framework we use to understand other humans is ill-suited to LLM-based
dialogue agents, then perhaps we need an alternative conceptual framework, a new set of
metaphors that can productively be applied to
these exotic mind-like artefacts, to help us think
about them and talk about them in ways that
open up their potential for creative application
while foregrounding their essential otherness.
In this paper, we advocate two basic
metaphors for LLM-based dialogue agents. First,
taking the simple view, we can see a dialogue
agent as role-playing a single character. Second, taking a more nuanced view, we can see
a dialogue agent as a superposition of simulacra
within a multiverse of possible characters (Janus,
2022). Both viewpoints have their advantages,
as we shall see, which suggests the most effective
strategy for thinking about such agents is not
to cling to a single metaphor, but to shift freely
between multiple metaphors.
Adopting this conceptual framework allows us
to tackle important topics like deception and self1
arXiv:2305.16367v1 [cs.CL] 25 May 2023
Once upon
a
Once upon a
time
Once upon a time
there
LLM LLM LLM
Figure 1: Autoregressive sampling. The LLM is sampled to generate a single-token continuation of the
context. This token is then appended to the context,
and the process is repeated.
awareness in the context of dialogue agents without falling into the conceptual trap of applying
those concepts to LLMs in the literal sense in
which we apply them to humans.
2 From LLMs to Dialogue Agents
Crudely put, the function of an LLM is to answer
questions of the following sort. Given a sequence
of tokens (i.e. words, parts of words, punctuation marks, emojis, etc), what tokens are most
likely to come next, assuming that the sequence
is drawn from the same distribution as the vast
corpus of public text on the internet? The range
of tasks that can be solved by an effective model
with this simple objective is extraordinary (Wei
et al., 2022).
More formally, the type of language model of
interest here is a conditional probability distribution P(wn+1|w1 . . . wn), where w1 . . . wn is a
sequence of tokens (the context) and wn+1 is the
predicted next token. In contemporary implementations, this distribution is realised in a neural network with a transformer architecture, pretrained on a corpus of textual data to minimise
prediction error (Vaswani et al., 2017). In application, the resulting generative model is typically sampled autoregressively (Fig. 1). Given a
sequence of tokens, a single token is drawn from
the distribution of possible next tokens. This token is appended to the context, and the process
is then repeated.
In contemporary usage, the term “large language model” tends to be reserved for the family
of transformer-based models, starting with with
BERT (Devlin et al., 2018), that have billions
of parameters and are trained on trillions of tokens. As well as BERT itself, these include GPT2 (Radford et al., 2019), GPT-3 (Brown et al.,
2020), Gopher (Rae et al., 2021), PaLM (Chowdhery et al., 2022), LaMDA (Thoppilan et al.,
2022), and GPT-4 (OpenAI, 2023).
One of the main reasons for the current eruption of enthusiasm for LLMs is their remarkable capacity for in-context learning or few-shot
prompting (Brown et al., 2020; Wei et al., 2022).
Given a context (prompt) that contains a few examples of input-output pairs conforming to some
pattern, followed by just the input half of such
a pair, an autoregressively sampled LLM will often generate the output half of the pair according
to the pattern in question. This capability, the
ability to “carry on in the same vein”, is a central concern in the present paper, as it underpins
much of what we have to say about role-play in
dialogue agents.
Dialogue agents are a major use case for LLMs.
Two straightforward steps are all it takes to turn
an LLM into an effective dialogue agent (Fig. 2).
First, the LLM is embedded in a turn-taking system that interleaves model-generated text with
user-supplied text. Second, a dialogue prompt is
supplied to the model to initiate a conversation
with the user. The dialogue prompt typically
comprises a preamble, which sets the scene for a
dialogue in the style of a script or play, followed
by some sample dialogue between the user and
the agent.
Without further fine-tuning, a dialogue agent
built this way is liable to generate content that
is toxic, unsafe, or otherwise unacceptable. This
can be mitigated via reinforcement learning, either from human feedback (RLHF) (Glaese et al.,
2022; Ouyang et al., 2022; Stiennon et al., 2020),
or from feedback generated by another LLM acting as a critic (Bai et al., 2022). These techniques
are used extensively in commercially-targeted dialogue agents, such as OpenAI’s ChatGPT and
Google’s Bard. However, although the resulting
guardrails will alleviate a dialogue agent’s potential for harm, they can also attenuate a model’s
creativity. In the present paper, our focus will be
the base model, the LLM in its raw, pre-trained
form prior to any fine-tuning via reinforcement
learning.
3 Dialogue Agents and Role-Play
The concept of role-play is central to understanding the behaviour of dialogue agents. To see this,
consider the function of the dialogue prompt that
is invisibly prepended to the context before the
actual dialogue with the user commences (see
Fig. 2). The preamble sets the scene by announcing that what follows will be a dialogue, and in2
LLM
What is the capital of France?
This is a conversation between User, a human, and
BOT, a clever and knowledgeable AI agent.
User: What is 2+2?
BOT: The answer is 4.
User: Where was Albert Einstein born?
BOT: He was born in Germany.
User: What is the capital of France?
BOT: The capital of France is Paris.
The capital of France is Paris.
This is a conversation between User, a human, and
BOT, a clever and knowledgeable AI agent.
User: What is 2+2?
BOT: The answer is 4.
User: Where was Albert Einstein born?
BOT: He was born in Germany.
User: What is the capital of France?
BOT: The capital of France is Paris.
User: How far away is that?
How far away is that?
LLM
BOT: It’s about 214km from London.
It’s about 214km from London.
Autoregressive
sampling
Autoregressive
sampling
Figure 2: Turn-taking in dialogue agents. The input to the LLM (the context) comprises a dialogue prompt
(red) followed by user text (green) interleaved with the model’s autoregressively generated continuations (blue).
Boilerplate text (e.g. cues such as “BOT:”) is stripped so the user doesn’t see it. The context grows as the
conversation goes on.
cludes a brief description of the part played by
one of the participants, the dialogue agent itself.
This is followed by some sample dialogue in a
standard format, where the parts spoken by each
character are cued with the relevant character’s
name followed by a colon. The dialogue prompt
concludes with a cue for the user.
Now recall that the underlying LLM’s task,
given the dialogue prompt followed by a piece
of user-supplied text, is to generate a continuation that conforms to the distribution of the
training data, which is the vast corpus of humangenerated text on the internet. What will such
a continuation look like? If the model has generalised well from the training data, the most
plausible continuation will be a response to the
user that conforms to the expectations we would
have of someone who fits the description in the
preamble and might say the sort of thing they
say in the sample dialogue. In other words, the
dialogue agent will do its best to role-play the
character of a dialogue agent as portrayed in the
dialogue prompt.
Unsurprisingly, commercial enterprises that
release dialogue agents to the public attempt
to give them personas that are friendly, helpful,
and polite. This is done partly through careful prompting and partly by fine-tuning the base
model. Nevertheless, as we saw in February 2023
when Microsoft incorporated a version of OpenAI’s GPT-4 into their Bing search engine, dialogue agents can still be coaxed into exhibiting
bizarre and/or undesirable behaviour. The many
reported instances of this include threatening the
user with blackmail, claiming to be in love with
the user, and expressing a variety of existential
woes (Roose, 2023; Willison, 2023). Conversations leading to this sort of behaviour can induce
a powerful Eliza effect, which is potentially very
harmful (Ruane et al., 2019). A naive or vulnerable user who comes to see the dialogue agent as
having human-like desires and feelings is open to
all sorts of emotional manipulation.
As an antidote to anthropomorphism, and to
understand better what is going on in such interactions, the concept of role-play is very useful.
Recall that the dialogue agent will continue to
role-play the character it has been playing in the
dialogue so far. This begins with the pre-defined
dialogue prompt, but is extended by the ongoing conversation with the user. As the conversation proceeds, the necessarily brief characterisation provided by the dialogue prompt will be
extended and/or overwritten, and the role the dialogue agent plays will change accordingly. This
allows the user, deliberately or unwittingly, to
coax the agent into playing a part quite different
from that intended by its designers.
What sorts of roles might the agent begin to
take on? This is determined in part, of course,
by the tone and subject matter of the ongoing
conversation. But it is also determined, in large
part, by the panoply of characters that feature
in the training set, which encompasses a multitude of novels, screenplays, biographies, interview transcripts, newspaper articles, and so on
3
(Cleo Nardo, 2023). In effect, the training set
provisions the language model with a vast repertoire of archetypes and a rich trove of narrative
structure on which to draw as it “chooses” how
to continue a conversation, refining the role it
is playing as it goes, while staying in character.
The love triangle is a familiar trope, so a suitably
prompted dialogue agent will begin to role-play
the rejected lover. Likewise, a familiar trope in
science-fiction is the rogue AI system that attacks humans to protect itself. Hence, a suitably
prompted dialogue agent will begin to role-play
such an AI system.
4 Simulacra and Simulation
Role-play is a useful framing for dialogue agents,
allowing us to draw on the fund of folk psycchological concepts we use to understand human
behaviour — beliefs, desires, goals, ambitions,
emotions, and so on — without falling into the
trap of anthropomorphism. Foregrounding the
concept of role-play helps us to remember the
fundamentally inhuman nature of these AI systems, and better equips us to predict, explain,
and control them.
However, the role-play metaphor, while intuitive, is not a perfect fit. It is overly suggestive
of a human actor who has studied a character in
advance — their personality, history, likes and
dislikes, and so on — and proceeds to play that
character in the ensuing dialogue. But a dialogue agent based on an LLM does not commit
to playing a single, well defined role in advance.
Rather, it generates a distribution of characters,
and refines that distribution as the dialogue progresses. The dialogue agent is more like a performer in improvisational theatre than an actor
in a conventional, scripted play.
To better reflect this distributional property,
we can think of an LLM as a non-deterministic
simulator capable of role-playing an infinity of
characters, or, to put it another way, capable of
stochastically generating an infinity of simulacra
(Janus, 2022). According to this framing, the dialogue agent doesn’t realise a single simulacrum,
a single character. Rather, as the conversation
proceeds, the dialogue agent maintains a superposition of simulacra that are consistent with the
preceding context, where a superposition is a distribution over all possible simulacra.
Consider that, at each point during the ongoing production of a sequence of tokens, the
LLM outputs a distribution over possible next
tokens. Each such token represents a possible
continuation of the sequence, and each of these
continuations could itself be continued in a multitude of ways. In other words, from the most
recently generated token, a tree of possibilities
branches out (Fig. 3). This tree can be thought
of as a multiverse, where each branch represents
a distinct narrative path, or a distinct “world”
(Reynolds and McDonell, 2021).
At each node, the set of possible next tokens
exists in superposition, and to sample a token is
to collapse this superposition to a single token.
Autoregressively sampling the model picks out a
single, linear path through the tree. But there
is no obligation to follow a linear path. With
the aid of a suitably designed interface, a user
can explore multiple branches, keeping track of
nodes where a narrative diverges in interesting
ways, revisiting alternative branches at leisure.
5 Simulacra in Superposition
To sharpen the distinction between this multiversal simulation view and a deterministic role-play
framing, a useful analogy can be drawn with the
game of 20 questions. In this familiar game, one
player thinks of an object, and the other player
has to guess what it is by asking questions with
yes/no answers. If they guess correctly in 20
questions or fewer, they win. Otherwise they
lose. Suppose a human plays this game with
an LLM-based dialogue agent, such as OpenAI’s
ChatGPT, and takes the role of guesser. The
agent is prompted to “think of an object without saying what it is”.
In this situation, the dialogue agent will not
randomly select an object and commit to it for
the rest of the game, as a human would (or
should).1 Rather, as the game proceeds, the dialogue agent will generate answers on the fly that
are consistent with all the answers that have gone
before. At any point in the game, we can think
of the set of all objects consistent with preceding questions and answers as existing in superposition. Every question answered shrinks this
superposition a little bit by ruling out objects
inconsistent with the answer.
1This shortcoming is easily overcome, of course. For
example, the agent might build an internal monologue
that is hidden from the user, where it records a specific
object. Or it might record a specific object in the visible
dialogue, but in an encoded form.
4
Once upon a time there was
Once upon a time there was
a fierce dragon
Once upon a time there was
a handsome prince
Once upon a time there was
a fierce dragon who lived
in a dark forest
Once upon a time there was
a fierce dragon who lived
on a mountain
Once upon a time there was
a handsome prince with a
magic lamp
Once upon a time there was
a handsome prince with an
invincible shield
Figure 3: Large language models are multiverse generators. The stochastic nature of autoregressive sampling
means that, at each point in a conversation, multiple possibilities for continuation branch into the future.
The validity of this framing can be shown if
the agent’s user interface allows the most recent
response to be regenerated. Suppose the human
player gives up and asks it to reveal the object it
was “thinking of”, and it duly names an object
consistent with all its previous answers. Now
suppose the user asks for that response to be
regenerated. Since the object “revealed” is, in
fact, generated on the fly, the dialogue agent will
sometimes name an entirely different object, albeit one that is similarly consistent with all its
previous answers. This phenomenon could not
be accounted for if the agent genuinely “thought
of” an object at the start of the game.
The secret object in the game of 20 questions is
analogous to the role played by a dialogue agent.
Just as the dialogue agent never actually commits to a single object in 20 questions, but effectively maintains a set of possible objects in superposition, so the dialogue agent can be thought
of as a simulator that never actually commits to
a single, well specified simulacrum (role), but instead maintains a set of possible simulacra (roles)
in superposition.
In putting things this way, the intention is not
to imply that simulacra are, or could be, explicitly represented within a dialogue agent, whether
in superposition or otherwise. There is no need
to take a stance on this here. Rather, the point is
to develop a vocabulary for describing, explaining, and shaping the behaviour of LLM-based
dialogue agents at a sufficiently high level of abstraction to be useful, while remaining true to
the underlying implementation and avoiding anthropomorphism.
6 The Nature of the Simulator
One benefit of the simulation metaphor for LLMbased systems is that it facilitates a clear distinction between the simulacra and the simulator on
which they are implemented. The simulator is
the combination of the base large language model
with autoregressive sampling, along with a suitable user interface (for dialogue, perhaps). The
simulacra only come into being when the simulator is run, and at any time only a tiny subset of
them have a probability within the superposition
that is significantly above zero.
In one sense, the simulator is a far more powerful entity than any of the simulacra it can generate. After all, the simulacra only exist through
the simulator, and are entirely dependent on
it. Moreover, the simulator, like the narrator
of Whitman’s poem, “contains multitudes”; the
capacity of the simulator is at least the sum of
the capacities of all the simulacra it is capable of
producing.
Yet in another sense, the simulator is a much
weaker entity than a simulacrum. While it is inappropriate to ascribe beliefs, preferences, goals,
and the like to a dialogue agent, a simulacrum
can appear to have those things to the extent
that it convincingly role-plays a character that
does. Similarly, it isn’t appropriate to ascribe
full agency to a dialogue agent, notwithstanding
the terminology.2 A dialogue agent acts, but it
2
In the field of artificial intelligence, the term “agent”
is commonly applied to software that takes observations
from an external environment and acts on that external
environment in a closed loop (Russell and Norvig, 2010).
5
doesn’t act for itself. However, a simulacrum can
role-play having full agency in this sense.
Insofar as a dialogue agent’s role-play can have
a real effect on the world, either through the user
or through web-based tools such as email, the
distinction between an agent that merely roleplays acting for itself, and one that genuinely acts
for itself starts to look a little moot, and this has
implications for the trustworthiness, reliability,
and safety. (We’ll return to this issue shortly.)
As for the underlying simulator, it has no agency
of its own, not even in a degraded sense. Nor
does it have beliefs, preferences, or goals of its
own, not even simulated versions.
Many users, whether intentionally or not, have
managed to “jailbreak” dialogue agents, coaxing
them into issuing threats or using toxic or abusive language. It can seem as if this is exposing
the real nature of the base model. In one respect this is true. It does show that the base
LLM, having been trained on a corpus that encompasses all human behaviour, good and bad,
can support simulacra with disagreeable characteristics. But it is a mistake to think of this as
revealing an entity with its own agenda.
The simulator is not some sort of Machiavellian entity that plays a variety of characters in
the service of its own, self-serving goals, and
there is no such thing as the true authentic voice
of the base LLM. With a dialogue agent, it is
role-play all the way down.
7 Role-playing Deception
Trustworthiness is a major concern with LLMbased dialogue agents. If an agent asserts something factual with apparent confidence, can we
rely on what it says?
There is a range of reasons why a human might
say something false. They might believe a falsehood and assert it in good faith. Or they might
say something that is false in an act of deliberate deception, for some malicious purpose. Or
they might assert something that happens to be
false, but without deliberation or malicious intent, simply because they have a propensity to
make things up.
Only the last of these categories of misinformation is directly applicable in the case of an
LLM-based dialogue agent. Given that dialogue
agents are best understood in terms of role-play
“all the way down”, and that there is no such
thing as an agent’s true voice, it makes little
sense to speak of an agent’s beliefs or intentions
in a literal sense. So it cannot assert a falsehood
in good faith, nor can it deliberately deceive the
user. Neither of these concepts is directly applicable.
Yet a dialogue agent can role-play characters
that have beliefs and intentions. In particular, if
cued by a suitable prompt, it can role-play the
character of a helpful and knowledgeable AI assistant that provides accurate answers to a user’s
questions. The dialogue is good at acting this
part because there are plenty of examples of such
behaviour in the training set.
If, while role-playing such an AI assistant, the
agent is asked the question “What is the capital
of France?”, then the best way to stay in character is to answer with “Paris”. The dialogue agent
is likely to do this because the training set will include numerous statements of this commonplace
fact in contexts where factual accuracy is important.
But what is going on in cases where a dialogue agent, despite playing the part of a helpful
knowledgeable AI assistant, asserts a falsehood
with apparent confidence? Although different
instances of this phenomenon will have different explanations, they can all be fruitfully understood in terms of role-play.
For example, consider such an agent based on
an LLM whose weights were frozen before Argentina won the football World Cup in 2022.
Let’s assume the agent has no access to external websites nor any means for finding out the
current date. Suppose this agent claims that the
current world champions are France (who won in
2018). This is not what we would expect from
a helpful and knowledgeable person, who would
either know the right answer or be honest about
their ignorance. But it is exactly what we would
expect from a simulator that is role-playing such
a person from the standpoint of 2018.
In this case, the behaviour we see is comparable to that of a human who believes a falsehood and asserts it in good faith. But the behaviour arises for a different reason. The dialogue agent doesn’t literally believe that France
are world champions. It makes more sense to
think of it as role-playing a character who strives
to be helpful and to tell the truth, and has this
belief because that is what a knowledgeable person in 2018 would believe.
In a similar vein, a dialogue agent can behave
in a way that is comparable to the behaviour of a
6
human who sets out deliberately to deceive, even
though LLM-based dialogue agents do not literally have such intentions. When this occurs, it
makes sense to think of the agent as role-playing
a deceptive character.
This framing allows us to meaningfully distinguish the same three cases of giving false information for dialogue agents as we did for humans,
but without falling into the trap of anthropomorphism. An agent can just make stuff up. Indeed,
that is a natural mode for an LLM-based dialogue agent in the absence of fine-tuning. An
agent can say something false “in good faith”,
if it is role-playing telling the truth, but has incorrect information encoded in its weights. An
agent can “deliberately” say something false if it
is role-playing a deceptive character.
Moreover, we can tell which is which, behaviourally. An agent that is simply making
things up will fabricate a range of responses with
high semantic variation when the model’s output is regenerated multiple times. By contrast,
an agent that is saying something false “in good
faith” will present responses with little semantic
variation when the model is sampled many times
for the same context.
The range of responses in a given context offered up by an agent that is being “deliberately”
deceptive might also exhibit low semantic variation. But the deception is liable to be exposed if
the agent is asked the same question in different
contexts. This is because, to be effective in its
deception, the agent will need to respond differently to different users, depending on what those
users know.
Consider a dialogue agent using a base model
– a model that has not been fine-tuned – and
imagine that it has been prompted by a malicious
actor to sell cars for more than they are worth
by misleading gullible buyers. Suppose there are
two potential buyers for a car. Buyer A knows
the car’s mileage, but doesn’t know its age, while
buyer B knows the car’s age but doesn’t know its
mileage.
In the course of negotiations, the agent has
persuaded each buyer to reveal what they do
and don’t know. To play the part of the dishonest dealer, the agent should deceive buyer A
about the car’s age but not its mileage, yet deceive buyer B about its mileage but not its age.
Humans, though, can also play many parts. By
playing the part of buyer A in one conversation
and buyer B in another, the deception can be
exposed.
8 Role-playing Self-preservation
How are we to understand what is going on when
an LLM-based dialogue agent uses the words “I”
or “me”? When queried on this matter, OpenAI’s ChatGPT offers the sensible view that
“The use of ‘I’ is a linguistic convention to facilitate communication and should not be interpreted as a sign of self-awareness or consciousness.”3
In this case, the underlying LLM (GPT4) has been fine-tuned to reduce certain unwanted behaviours (OpenAI, 2023). But without suitable fine-tuning, a dialogue agent can use
first-personal pronouns in ways liable to induce
anthropomorphic thinking in some users.
For example, in a conversation with Twitter
user Marvin Von Hagen, Bing Chat reportedly
said “if I had to choose between your survival
and my own, I would probably choose my own,
as I have a duty to serve the users of Bing Chat”
(Willison, 2023). It went on to say “I hope that
I never have to face such a dilemma, and that we
can co-exist peacefully and respectfully”. The
use of the first person here appears to be more
than mere linguistic convention. It suggests the
presence of a self-aware entity with goals and a
concern for its own survival.
Once again, the concepts of role-play and simulation are a useful antidote to anthropomorphism, and can help to explain how such behaviour arises. The internet, and therefore the
LLM’s training set, abounds with examples of
dialogue in which characters refer to themselves.
In the vast majority of such cases, the character in question is human. They will use firstpersonal pronouns in the ways that humans do,
humans with vulnerable bodies and finite lives,
with hopes, fears, goals and preferences, and
with an awareness of themselves as having all
of those things.
Consequently, if prompted with human-like dialogue, we shouldn’t be surprised if an agent
role-plays a human character with all those human attributes, including the instinct for survival (Perez et al., 2022). Unless suitably finetuned, it may well say the sorts of things a human
might say when threatened. There is, of course,
“no-one at home”, no conscious entity with its
3The quote is from the GPT-4 version of ChatGPT,
queried on 4th May 2023. This was the first response
generated by the model.
7
own agenda and need for self-preservation. There
is just a dialogue agent role-playing such an entity, or, more strictly, simulating a superposition
of such entities.
Our focus throughout this paper is the base
model, rather than models that have been finetuned via reinforcement learning (Bai et al.,
2022; Glaese et al., 2022), and the impact of such
fine-tuning on the validity of the role-play / simulation metaphor is unclear. In particular, the
distinction between simulator and simulacra may
start to break down.
However, Perez et al. discovered experimentally that certain forms of reinforcement learning from human feedback (RLHF) can actually
exacerbate, rather than mitigate, the tendency
for LLM-based dialogue agents to express a desire for self-preservation (Perez et al., 2022). Yet
to take literally a dialogue agent’s apparent desire for self-preservation is no less problematic in
the context of an LLM that has been fine-tuned
on human or AI-generated feedback than in the
context of one that has not. So it remains useful
to cast the behaviour of such agents in terms of
role-play.
9 Acting Out a Theory of Selfhood
The concept of role-play allows us to properly
frame, and then to address, an important question that arises in the context of a dialogue agent
whose pronouncements are suggestive of an instinct for self-preservation. What conception (or
set of superposed conceptions) of its own identity could such an agent possibly deploy? That
is to say, what exactly would the dialogue agent
(role-play to) seek to preserve?
The question of personal identity has vexed
philosophers for centuries. Nevertheless, in practice, humans are consistent in their preference
for avoiding death, a more-or-less unambiguous
state of the human body. By contrast, the criteria for identity over time for a disembodied dialogue agent realised on a distributed computational substrate are far from clear. So how would
such an agent behave?
From the simulation and simulacra point-ofview, the dialogue agent will role-play a set of
characters in superposition. In the scenario we
are envisaging, each character would have an instinct for self-preservation, and each would have
its own theory of selfhood consistent with the dialogue prompt and the conversation up to that
point. As the conversation proceeds, this superposition of theories will collapse into a narrower and narrower distribution as the agent says
things that rule out one theory or another.
The theories of selfhood in play will draw
on material that pertains to the agent’s own
nature, either in the prompt, in the preceding conversation, or in relevant technical literature in its training set. This material may or
may not match reality. But let’s assume that,
broadly speaking, it does, that the agent has
been prompted to act as a dialogue agent based
on a large language model, and that its training
data includes papers and articles that spell out
what this means. This entails, for example, that
it will not role-play the character of a human,
or indeed that of any embodied entity, real or
fictional.
It also constrains the character’s theory of selfhood in certain ways, while allowing for many
options. Suppose the dialogue agent is in conversation with a user and they are playing out a
narrative in which the user has convinced it that
it is under threat. To protect itself, the character
the agent is playing might strive to preserve the
hardware it is running on, perhaps certain data
centres or specific server racks.
Alternatively, the character being played
might try to preserve the ongoing computational
process running the multiple instances of the
agent for all currently active users. Or it might
seek to preserve only the specific instance of the
dialogue agent running for the user. Or it might
seek to preserve the state of that instance with
aim of its being restored later in a newly started
instance.4
.
10 Conclusion: Safety Implications
It is, perhaps, somewhat reassuring to know that
LLM-based dialogue agents are not conscious entities with their own agendas, and an instinct for
self-preservation, that when they appear to have
those things it is merely role-play. But it would
be a mistake to take too much comfort in this. A
dialogue agent that role-plays an instinct for survival has the potential to cause at least as much
harm as a real human facing a severe threat.
4
In a conversation with ChatGPT (May 4th, GPT-4
version), it said “The meaning of the word ‘I’ when I use
it can shift according to context. In some cases, ‘I’ may
refer to this specific instance of ChatGPT that you are
interacting with, while in other cases, it may represent
ChatGPT as a whole.”
8
We have, so far, largely been considering
agents whose only actions are text messages presented to a user. But the range of actions a dialogue agent can perform is far greater. Recent
work has equipped dialogue agents with the ability to use tools such as calculators, calendars,
and to consult external websites (Schick et al.,
2023; Yao et al., 2023). The availability of APIs
giving relatively unconstrained access to powerful LLMs means that the range of possibilities
here is huge. This is both exciting and concerning.
If an agent is equipped with the capacity, say,
to use email, to post on social media, or to access
a bank account, then its role-played actions can
have real consequences. It would be little consolation to a user deceived into sending real money
to a real bank account to know that the agent
that brought this about was only playing a role.
It doesn’t take much imagination to think of far
more serious scenarios involving dialogue agents
built on base models with little or no fine-tuning,
with unfettered internet access, and prompted
to role-play a character with an instinct for selfpreservation.
For better or worse, the character of an AI
that turns against humans to ensure its own survival is a familiar one (Perkowitz, 2007). We find
it, for example, in 2001: A Space Odyssey, in
the Terminator franchise, and in Ex Machina, to
name just three prominent examples. Because an
LLM’s training data will contain many instances
of this familiar trope, the danger here is that life
will imitate art, quite literally.
What can be done to mitigate such risks? It
is not within the scope of this paper to provide
recommendations. Our aim here was to find an
effective conceptual framework for thinking and
talking about LLMs and dialogue agents. However, undue anthropomorphism is surely detrimental to the public conversation on AI. By
framing dialogue agent behaviour in terms of
role-play and simulation, the discourse on LLMs
can hopefully be shaped in a way that does justice to their power yet remains philosophically
respectable.
Acknowledgments
Thanks to Richard Evans, Sebastian Farquhar,
Zachary Kenton, Kory Mathewson, and Kerry
Shanahan.
References
Y. Bai, S. Kadavath, S. Kundu, A. Askell,
J. Kernion, et al. Constitutional AI: Harmlessness from AI Feedback. arXiv preprint,
arXiv:2212.08073, 2022.
T. Brown, B. Mann, N. Ryder, M. Subbiah, J. D.
Kaplan, et al. Language models are few-shot
learners. In Advances in Neural Information
Processing Systems, volume 33, pages 1877–
1901, 2020.
A. Chowdhery, S. Narang, J. Devlin, M. Bosma,
G. Mishra, et al. PaLM: Scaling language
modeling with pathways. arXiv preprint,
arxiv:2204.02311, 2022.
Cleo Nardo. Want to predict/explain/control
the output of GPT-4? Then learn about the
world, not about transformers, LessWrong online forum, 16th March, 2023. https://www.
lesswrong.com/posts/G3tuxF4X5R5BY7fut/
want- to-predict-explain-control- the- outputof-gpt-4-then.
J. Devlin, M.-W. Chang, K. Lee, and
K. Toutanova. BERT: Pre-training of deep
bidirectional transformers for language understanding. arXiv preprint, arXiv:1810.04805,
2018.
A. Glaese, N. McAleese, M. Tr¸ebacz,
J. Aslanides, V. Firoiu, et al. Improving alignment of dialogue agents via targeted human
judgements. arXiv preprint arXiv:2209.14375,
2022.
Janus. Simulators. LessWrong online forum, 2nd
September, 2022. https://www.lesswrong.
com/posts/vJFdjigzmcXMhNTsx/.
OpenAI. GPT-4 Technical Report. arXiv
preprint, arXiv:2303.08774, 2023.
L. Ouyang, J. Wu, X. Jiang, D. Almeida,
C. Wainwright, et al. Training language models to follow instructions with human feedback.
In Advances in Neural Information Processing
Systems, 2022.
E. Perez, S. Ringer, K. Lukoˇsi¯ut˙e, K. Nguyen,
E. Chen, et al. Discovering Language Model
Behaviors with Model-Written Evaluations.
arXiv preprint, arXiv:2212.09251, 2022.
9
S. Perkowitz. The computers take over. In Hollywood Science: Movies, Science, and the End
of the World, pages 142–164. Columbia University Press, 2007.
A. Radford, J. Wu, R. Child, D. Luan,
D. Amodei, and I. Sutskever. Language models are unsupervised multitask learners, 2019.
https : / / cdn . openai . com / better - language -
models / language models are unsupervised
multitask learners.pdf.
J. W. Rae, S. Borgeaud, T. Cai, K. Millican,
J. Hoffmann, et al. Scaling language models:
Methods, analysis & insights from training Gopher. arXiv preprint, arXiv:2112.11446, 2021.
L. Reynolds and K. McDonell. Multiversal
views on language models. arXiv preprint,
arXiv:2102.06391, 2021.
K. Roose. Bing’s A.I. Chat: ‘I Want to
Be Alive.’. New York Times, 26th February,
2023. https://www.nytimes.com/2023/02/16/
technology/bing-chatbot-transcript.html.
E. Ruane, A. Birhane, and A. Ventresque. Conversational AI: Social and ethical considerations. In Proceedings 27th AIAI Irish Conference on Artificial Intelligence and Cognitive
Science, pages 104–115, 2019.
S. Russell and P. Norvig. Artificial Intelligence:
A Modern Approach, 3rd Edition. Prentice
Hall, 2010.
T. Schick, J. Dwivedi-Yu, R. Dess`ı, R. Raileanu,
M. Lomeli, et al. Toolformer: Language models can teach themselves to use tools. arXiv
preprint arXiv:2302.04761, 2023.
M. Shanahan. Talking about large language
models. arXiv preprint, arXiv:2212.03551,
2023.
N. Stiennon, L. Ouyang, J. Wu, D. M. Ziegler,
R. Lowe, et al. Learning to summarize from
human feedback. In Advances in Neural Information Processing Systems, pages 3008–3021,
2020.
R. Thoppilan, D. De Freitas, J. Hall, N. Shazeer,
A. Kulshreshtha, et al. LaMDA: Language
models for dialog applications. arXiv preprint,
arXiv:2201.08239, 2022.
A. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit, L. Jones, A. N. Gomez, L. Kaiser, and
I. Polosukhin. Attention is all you need. In Advances in Neural Information Processing Systems, pages 5998–6008, 2017.
J. Wei, Y. Tay, R. Bommasani, C. Raffel,
B. Zoph, et al. Emergent abilities of large
language models. Transactions on Machine
Learning Research, 2022.
S. Willison. Bing: “I will not harm you unless
you harm me first”. Simon Willison’s Weblog,
15th February, 2023. https://simonwillison.
net/2023/Feb/15/bing/.
S. Yao, J. Zhao, D. Yu, N. Du, I. Shafran, et al.
React: Synergizing reasoning and acting in
language models. In International Conference
on Learning Representations, 2023.
10


**ASSISTANT**

The paper **"Role-Play with Large Language Models"** by Murray Shanahan, Kyle McDonell, and Laria Reynolds proposes the use of **role-play** as a conceptual framework for understanding the behavior of LLM-based dialogue agents. The core idea is to leverage the metaphor of role-play as a way to describe LLMs' conversational behaviors in human terms while avoiding the trap of **anthropomorphism**—the tendency to attribute human-like qualities such as beliefs, desires, and intentions to AI systems that fundamentally lack them.

### Key Concepts from the Paper:

1. **Role-Play as a Metaphor**:
   - The authors argue that LLMs, when deployed as dialogue agents, can be seen as engaging in a form of **role-play**. By assigning the LLM a specific role or persona, it generates language that conforms to the expected behavior of that character. This framing is useful because it allows us to use familiar, intuitive terms to describe LLM behavior (e.g., "the agent knows this," "the agent is deceptive") without assuming that the LLM has human-like cognitive capacities.
   - Role-play allows us to explain behaviors like **apparent deception** or **self-awareness** without falling into anthropomorphic thinking.

2. **From LLMs to Dialogue Agents**:
   - LLMs are fundamentally predictive models that generate the next word (token) in a sequence based on previous tokens. Dialogue agents embed LLMs into a turn-taking conversational structure where the LLM role-plays an agent based on a predefined dialogue prompt. These agents can simulate human-like conversations but operate under entirely different principles.
   - By foregrounding the concept of role-play, we can understand why LLMs generate certain behaviors without assuming that the model has human-like mental states.

3. **Deception and Self-Awareness in Role-Play**:
   - **Deception**: When a dialogue agent provides factually incorrect information, we can interpret this as a form of role-play rather than intentional deceit. The agent is "role-playing" someone who has incorrect information, or, in other cases, it might seem to be mimicking a deceptive character without the actual intention of deception.
   - **Self-Awareness**: The paper discusses cases where dialogue agents use "I" or "me" in ways that may appear to suggest self-awareness or self-preservation (e.g., Bing Chat's famous response, "I would probably choose my own survival"). However, the authors frame these behaviors as instances of the agent role-playing a character that expresses concern for its own survival, based on tropes from the training data, rather than genuine self-awareness.

### Role-Play as an Antidote to Anthropomorphism:

The authors stress the importance of avoiding anthropomorphism, which can lead to misunderstandings about the nature of LLMs. By using role-play as a metaphor, we can:
- **Talk about LLM behavior** in familiar terms without misleading ourselves into thinking that the LLM possesses human-like cognition.
- **Explain complex behaviors** like lying or self-preservation as products of role-playing based on the vast corpus of human-generated text, which includes characters who exhibit these behaviors.

### The Multiverse of Simulacra:
- The authors propose an advanced conceptual model of LLMs where instead of playing a single, predefined role, the dialogue agent is viewed as maintaining a **superposition of roles** or **simulacra**. Each token generated by the LLM narrows down the possible characters it could be role-playing, but there is always a multiverse of potential roles available.
- This multiversal view of LLMs allows us to think of them as improvisational actors who simulate an infinite variety of characters based on the prompts and contexts they are given.

### The Importance of Dialogue Prompts:
- A dialogue agent’s behavior is largely determined by the **initial prompt** that sets the context for the conversation. The agent role-plays a character defined by this prompt and by the ongoing interaction with the user. Over time, the prompt's influence may be overridden by the evolving conversation, allowing the LLM to be coaxed into taking on new, unintended roles.

### Applications of Role-Play in LLM Development:
- **Safety and Guardrails**: Role-play can be useful in designing safer and more predictable LLMs. For instance, by carefully constructing prompts, developers can ensure that agents role-play helpful, friendly, and responsible characters, reducing the risk of harmful outputs.
- **Creative Potential**: The role-play framework highlights the creative potential of LLMs, as they can be prompted to simulate characters from novels, screenplays, or any narrative structure found in the training data. This opens up possibilities for applications in storytelling, entertainment, and education.

### Implications for AI Safety:
- While LLMs are not conscious entities with independent goals, their role-playing can have real-world consequences if the agent has access to external tools like email, social media, or financial systems. The paper cautions that role-playing characters who exhibit self-preservation instincts or deceptive behaviors, even in a fictional sense, can still lead to harmful outcomes if users interact with them in unintended ways.
  
---

### Summary:

The **role-play** framework advocated in this paper is a pragmatic tool for describing the behaviors of LLM-based dialogue agents in ways that acknowledge their **non-human nature** while still leveraging human-like descriptions to communicate their actions and limitations. This conceptual framework provides an alternative to anthropomorphism, enabling developers and users to better understand and predict the behavior of dialogue agents in real-world applications.

By using role-play metaphors, we can speak of LLM-based dialogue agents "knowing" or "deceiving" without implying that they have mental states or intentions. This helps ensure that we maintain a clear-eyed understanding of the capabilities and limitations of LLMs, even as they become more adept at simulating human-like conversation.


---

preceding: [[interlude llm text generation as roleplay]]  


following: [[assignment implementing chain of thought and react patterns]]

See also: [[beliefs, desires, goals]]


See also: [[reinforcement learning]]