---
date: 1721688160.217839
tags:
- autogenerated
- source__oai
- conversation_id__a2d4a2dc-4888-4b9b-aa34-ef6394b04fb7
- msg_aaa2d1f1-5865-4ba5-b413-0b4ebb037028
- msg_6121fe98-a973-42d8-a574-5ba1517e9140
- colorclass/information-theory
title: lagrangian duality
---

**USER**

Lagrangian Duality


**ASSISTANT**

## Lagrangian Duality

### Overview

Lagrangian duality is a fundamental concept in optimization theory, particularly in convex optimization. It provides a powerful framework for solving constrained optimization problems by transforming them into dual problems. This duality approach not only offers insights into the structure of the original problem but also often simplifies the problem, making it easier to solve.

### Primal and Dual Problems

#### Primal Problem

Consider a general optimization problem (the primal problem) in the following form:

$$
\begin{aligned}
& \min_{x \in \mathcal{X}} & & f(x) \\
& \text{subject to} & & g_i(x) \leq 0, \quad i = 1, \ldots, m, \\
& & & h_j(x) = 0, \quad j = 1, \ldots, p,
\end{aligned}
$$

where \(f(x)\) is the objective function, \(g_i(x) \leq 0\) are inequality constraints, \(h_j(x) = 0\) are equality constraints, and \(\mathcal{X}\) is the feasible region.

#### Lagrangian Function

The Lagrangian function \(L(x, \lambda, \nu)\) combines the objective function and the constraints using Lagrange multipliers (dual variables) \(\lambda_i \geq 0\) for the inequality constraints and \(\nu_j\) for the equality constraints:

$$
L(x, \lambda, \nu) = f(x) + \sum_{i=1}^m \lambda_i g_i(x) + \sum_{j=1}^p \nu_j h_j(x).
```

#### Dual Function

The dual function \(g(\lambda, \nu)\) is defined as the infimum of the Lagrangian over the primal variable \(x\):

$$
g(\lambda, \nu) = \inf_{x \in \mathcal{X}} L(x, \lambda, \nu).
```

This function provides a lower bound on the optimal value of the primal problem for any \(\lambda \geq 0\) and \(\nu\).

#### Dual Problem

The dual problem is then formulated as a maximization problem over the dual variables \(\lambda\) and \(\nu\):

$$
\begin{aligned}
& \max_{\lambda \geq 0, \nu} & & g(\lambda, \nu).
\end{aligned}
```

### Weak and Strong Duality

#### Weak Duality

Weak duality states that the optimal value of the dual problem provides a lower bound on the optimal value of the primal problem. Specifically, if \(x^*\) is a feasible solution to the primal problem and \((\lambda^*, \nu^*)\) is a feasible solution to the dual problem, then:

$$
f(x^*) \geq g(\lambda^*, \nu^*).
```

#### Strong Duality

Strong duality holds when the optimal values of the primal and dual problems are equal. This means that if \(x^*\) is an optimal solution to the primal problem and \((\lambda^*, \nu^*)\) is an optimal solution to the dual problem, then:

$$
f(x^*) = g(\lambda^*, \nu^*).
```

Strong duality does not always hold, but it is guaranteed under certain conditions, such as convexity of the primal problem and satisfaction of constraint qualifications (e.g., Slater's condition).

### KKT Conditions

The Karush-Kuhn-Tucker (KKT) conditions provide necessary conditions for a solution to be optimal in a constrained optimization problem. For a solution \(x^*\), \(\lambda^*\), and \(\nu^*\) to be optimal, they must satisfy:

1. **Primal Feasibility**:
    $$
    g_i(x^*) \leq 0, \quad i = 1, \ldots, m,
    $$
    $$
    h_j(x^*) = 0, \quad j = 1, \ldots, p.
    ```

2. **Dual Feasibility**:
    $$
    \lambda_i^* \geq 0, \quad i = 1, \ldots, m.
    ```

3. **Stationarity**:
    $$
    \nabla f(x^*) + \sum_{i=1}^m \lambda_i^* \nabla g_i(x^*) + \sum_{j=1}^p \nu_j^* \nabla h_j(x^*) = 0.
    ```

4. **Complementary Slackness**:
    $$
    \lambda_i^* g_i(x^*) = 0, \quad i = 1, \ldots, m.
    ```

### Examples

#### Example 1: Linear Programming

Consider the linear programming (LP) problem:

**Primal Problem:**

$$
\begin{aligned}
& \min_{x} & & c^T x \\
& \text{subject to} & & Ax \leq b \\
& & & x \geq 0,
\end{aligned}
```

where \(c \in \mathbb{R}^n\), \(A \in \mathbb{R}^{m \times n}\), and \(b \in \mathbb{R}^m\).

**Dual Problem:**

$$
\begin{aligned}
& \max_{y} & & b^T y \\
& \text{subject to} & & A^T y \leq c \\
& & & y \geq 0,
\end{aligned}
```

where \(y \in \mathbb{R}^m\) are the dual variables.

In linear programming, strong duality always holds, meaning the optimal values of the primal and dual problems are equal.

#### Example 2: Quadratic Programming

Consider the quadratic programming (QP) problem:

**Primal Problem:**

$$
\begin{aligned}
& \min_{x} & & \frac{1}{2} x^T Q x + c^T x \\
& \text{subject to} & & Ax \leq b,
\end{aligned}
```

where \(Q \in \mathbb{R}^{n \times n}\) is a symmetric positive definite matrix.

**Dual Problem:**

The dual problem for a QP can be derived similarly to the LP case, leading to another QP involving the dual variables.

### Applications

#### Economics

In economics, Lagrangian duality is used to analyze consumer behavior, production optimization, and cost minimization. The dual variables often have economic interpretations, such as shadow prices or marginal costs.

#### Engineering

In engineering, [[duality theory]] is applied to solve optimization problems in control systems, signal processing, and network design. It helps in designing efficient algorithms for resource allocation and system optimization.

#### Machine Learning

In machine learning, [[duality theory]] is used in support vector machines (SVMs) and other optimization-based learning algorithms. The dual problem formulation often leads to more efficient algorithms for training and inference.

### Further Reading

- [[Convex Optimization]]
- [[Large Deviations Theory]]
- [[Gartner-Ellis Theorem]]
- [[Cramér's Theorem]]
- [[Legendre-Fenchel Transform]]
- [[Karush-Kuhn-Tucker Conditions]]
- [[Optimization Problems in Spin Glasses]]

Lagrangian duality provides a powerful and elegant framework for understanding and solving optimization problems. For a comprehensive exploration, refer to specialized texts in convex analysis, optimization theory, and operations research.


---

preceding: [[cramér's theorem]]  


following: [[relative entropy]]

See also: [[signal processing]]


See also: [[consumer behavior]]


See also: [[support vector machines]]


See also: [[operations research]]


See also: [[resource allocation]]