---
date: 1739751993.181862
tags:
- autogenerated
- source__oai
- conversation_id__67b26c18-8610-8008-8f24-866d5f3c6ed8
- msg_32ba50a9-9c89-490b-90e8-c47d272ce529
- msg_19f76489-24b8-459a-9ac9-e0ae16d31280
- colorclass/information-theory
title: rate of convergence in the glivenko-cantelli theorem via the dkw inequality
---

**USER**

> A stronger version of the theorem gives a rate of convergence using the DKW inequality


**ASSISTANT**

### **Rate of Convergence in the Glivenko-Cantelli Theorem via the DKW Inequality**

A stronger quantitative version of the **Glivenko-Cantelli theorem** provides a **rate of convergence** for the empirical distribution function $\hat{F}_n(x)$ to the true cumulative distribution function (CDF) $F(x)$. This rate is often derived using the **Dvoretzky-Kiefer-Wolfowitz (DKW) inequality**.

---

## **1. Dvoretzky-Kiefer-Wolfowitz (DKW) Inequality**
The **DKW inequality** gives a probabilistic bound on the uniform deviation between $\hat{F}_n(x)$ and $F(x)$. Specifically, for any $\epsilon > 0$,

\[
P\left( \sup_{x \in \mathbb{R}} |\hat{F}_n(x) - F(x)| > \epsilon \right) \leq 2 e^{-2n\epsilon^2}.
\]

This result is **distribution-free**, meaning it holds for all probability distributions $F$.

---

## **2. Rate of Convergence**
Using the DKW inequality, we can derive an **order of magnitude** estimate for the convergence rate. Setting the right-hand side to a small probability level $\delta$ (e.g., $\delta = 0.05$ for a 95% confidence bound), we solve for $\epsilon$:

\[
2 e^{-2n\epsilon^2} = \delta.
\]

Solving for $\epsilon$, we get:

\[
\epsilon = \sqrt{\frac{\log(2/\delta)}{2n}}.
\]

Thus, with probability at least $1 - \delta$,

\[
\sup_{x \in \mathbb{R}} |\hat{F}_n(x) - F(x)| \leq \sqrt{\frac{\log(2/\delta)}{2n}}.
\]

This gives an explicit **finite-sample bound**, which is useful in practical applications.

### **Asymptotic Rate**
From the bound above, we see that the uniform convergence rate satisfies:

\[
\sup_{x \in \mathbb{R}} |\hat{F}_n(x) - F(x)| = O_p\left(\sqrt{\frac{\log n}{n}}\right).
\]

- The $\sqrt{1/n}$ dependence follows from classical **central limit theorem** scaling.
- The additional $\sqrt{\log n}$ term accounts for the **supremum over all $x$**, since we are dealing with uniform convergence rather than pointwise convergence.

---

## **3. Interpretation and Implications**
- The rate $O_p(\sqrt{\log n / n})$ is **slower** than the typical pointwise **law of large numbers** rate $O_p(1/\sqrt{n})$ because of the **supremum over all $x$**.
- The bound is **distribution-free**, meaning it holds for all probability distributions.
- This result is a precursor to **more refined empirical process bounds**, such as those using VC theory for function classes beyond just indicator functions.

---

## **4. Application: Confidence Bands for the Empirical CDF**
Using the DKW bound, we can construct **uniform confidence bands** for the true CDF $F(x)$:

\[
F(x) \in \left[\hat{F}_n(x) - \epsilon_n, \hat{F}_n(x) + \epsilon_n\right] \quad \text{for all } x
\]

with high probability, where $\epsilon_n = \sqrt{\frac{\log(2/\delta)}{2n}}$.

This forms the basis for **Kolmogorov-Smirnov tests**, where the maximum deviation $\sup_x |\hat{F}_n(x) - F(x)|$ is used as a test statistic.

---

## **5. Connection to Empirical Process Theory**
The Glivenko-Cantelli theorem states that $\sup_x |\hat{F}_n(x) - F(x)| \to 0$ almost surely. The DKW inequality quantifies this by giving a **finite-sample bound**.

For more advanced results:
- **Donskerâ€™s Theorem** strengthens this to a **functional central limit theorem**, showing that $\sqrt{n} (\hat{F}_n - F)$ converges in distribution to a **Brownian bridge**.
- **VC theory** generalizes this type of uniform convergence to function classes beyond just indicator functions.

---

Would you like a deeper dive into the **Donsker theorem**, VC-theoretic extensions, or applications to statistical learning?


---

preceding: [[glivenko-cantelli theorem]]

See also: [[glivenko-cantelli theorem]]