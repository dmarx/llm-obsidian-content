---
date: '2025-04-19T16:40:48.869072Z'
tags:
- autogenerated
- source__claude
- conversation_id__a632fe1f-d15e-4da4-814d-fdcec2bbd584
- msg_ebde47d3-3e46-4a05-af28-b7d980f606d2
- msg_cb56368e-6144-463d-965d-df2a960868c0
- colorclass/information-theory
title: kraskov o-information
---

**USER**

Kraskov O-information


**ASSISTANT**

# Kraskov O-information

## Definition

The [[Kraskov O-information]] is a model-free, non-parametric [[information-theoretic measure]] that quantifies the balance between [[redundancy]] and [[synergy]] in multivariate systems. Named after Alexander Kraskov, who developed influential estimators for mutual information, the O-information extends these techniques to higher-order interactions.

For a set of random variables $X_1, X_2, ..., X_n$, the O-information is defined as:

$$\Omega(X_1, X_2, ..., X_n) = (n-2) \cdot H(X_1, X_2, ..., X_n) - \sum_{i=1}^{n} (n-1) \cdot H(X_{-i})$$

where:
- $H(X_1, X_2, ..., X_n)$ is the [[joint entropy]] of all variables
- $H(X_{-i})$ is the joint entropy of all variables except $X_i$
- $n$ is the number of variables

## Alternative Formulations

The O-information can be equivalently expressed in terms of [[mutual information]] as:

$$\Omega(X_1, X_2, ..., X_n) = \sum_{i=1}^{n} I(X_i; X_{-i}) - I(X_1; X_2; ...; X_n)$$

where:
- $I(X_i; X_{-i})$ is the mutual information between variable $X_i$ and the set of all other variables $X_{-i}$
- $I(X_1; X_2; ...; X_n)$ is the [[multivariate mutual information]] (also called the [[total correlation]] or [[multi-information]])

## Interpretation

The O-information provides critical insight into the nature of interactions in multivariate systems:

- $\Omega > 0$: The system exhibits net [[redundancy]], indicating overlapping information and potentially distributed representations
- $\Omega < 0$: The system exhibits net [[synergy]], indicating emergent information available only when considering multiple variables together
- $\Omega = 0$: Perfect balance between redundancy and synergy

This allows for quantifying whether a system's variables collaborate primarily through shared information (redundancy) or through higher-order interactions (synergy).

## Estimation Methods

The non-parametric estimation of O-information typically employs [[k-nearest neighbor statistics]] using the [[Kraskov estimator]] framework:

1. **Point-wise estimations**: For each data point, identify the k-nearest neighbors in the joint space
2. **Local density approximation**: Use the distances to nearest neighbors to approximate the local probability density
3. **Entropy estimation**: Calculate entropy terms based on these density estimates
4. **O-information calculation**: Combine the entropy estimates according to the O-information formula

The typical algorithm follows:

$$\hat{H}(X) \approx -\psi(k) + \psi(N) + \log(c_d) + \frac{d}{N}\sum_{i=1}^{N}\log(\epsilon_i)$$

where:
- $\psi$ is the [[digamma function]]
- $k$ is the number of nearest neighbors considered
- $N$ is the number of samples
- $c_d$ is the volume of the d-dimensional unit ball
- $\epsilon_i$ is twice the distance to the k-th nearest neighbor of point $i$
- $d$ is the dimensionality of $X$

## Applications

The Kraskov O-information has been applied to:

1. **[[Neural Information Processing]]**: Understanding how neural populations encode and process information
2. **[[Complex Systems Analysis]]**: Characterizing emergent behaviors in complex networks
3. **[[Feature Selection]]**: Identifying variables with unique information contribution
4. **[[Causal Discovery]]**: Supporting inference of causal relationships through information-theoretic properties
5. **[[Computational Neuroscience]]**: Quantifying neural coding strategies and information integration

## Relationship to Other Measures

The O-information relates to several other information-theoretic quantities:

- **[[Partial Information Decomposition (PID)]]**: O-information can be viewed as a specific balance between PID components
- **[[Interaction Information]]**: For three variables, O-information reduces to the negative of interaction information
- **[[Integrated Information Theory]]**: Provides complementary insights to integrated information measures like Φ
- **[[Transfer Entropy]]**: Can be combined with O-information to distinguish directional from symmetric information sharing

## Mathematical Properties

Key properties of the O-information include:

1. **Symmetry**: Invariant to permutation of variables
2. **Scaling**: Scales with the number of variables and their dependencies
3. **Bounds**: For n variables, theoretical bounds exist related to the entropies of individual variables
4. **Special Cases**: For $n=3$, reduces to negative interaction information
5. **Additivity**: For independent subsystems, O-information follows specific additivity rules

## Symbolic Notation

For clarity in complex systems analysis, we can adopt the notation:

$$\Omega_n(\mathbf{X}) = \Omega(X_1, X_2, ..., X_n)$$

For subsets of variables, we can use:

$$\Omega_{\{i,j,k\}}(\mathbf{X}) = \Omega(X_i, X_j, X_k)$$

This allows for tracking O-information across different variable groupings and hierarchical levels.

## Computational Considerations

When implementing Kraskov O-information estimators, several practical considerations apply:

1. **Sample size requirements**: Scales exponentially with dimensionality
2. **Nearest-neighbor parameter selection**: The choice of k affects bias-variance trade-off
3. **Curse of dimensionality**: Mitigation strategies include dimensionality reduction techniques
4. **Boundary bias correction**: Special handling for data points near the boundaries
5. **Computational complexity**: Optimized search structures like KD-trees can improve efficiency

## References

- Kraskov, A., Stögbauer, H., & Grassberger, P. (2004). Estimating mutual information. Physical Review E, 69(6), 066138.
- Rosas, F. E., Mediano, P. A., Gastpar, M., & Jensen, H. J. (2019). Quantifying high-order interdependencies via multivariate extensions of the mutual information. Physical Review E, 100(3), 032305.


---


following: [[transfer entropy]]

See also: [[distributed representations]]


See also: [[dimensionality reduction]]


See also: [[random variables]]