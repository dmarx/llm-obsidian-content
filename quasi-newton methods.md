---
date: 1717808828.175953
tags:
- autogenerated
- source__oai
- conversation_id__77e70e10-f02a-4f98-bd5c-6217b10122cf
- msg_aaa25852-aefd-4ed3-ad80-d25c3f3ffed0
- msg_3f5c090e-e9de-4ab2-8458-cb0a655c4fd4
- colorclass/system-dynamics
title: quasi-newton methods
---

**USER**

Quasi-Newton Methods


**ASSISTANT**

Quasi-Newton methods are a family of optimization algorithms that aim to approximate the Newton-Raphson method for finding local maxima or minima of functions. These methods are used when the Hessian matrix (the matrix of second-order partial derivatives) is either difficult to compute or too expensive to evaluate at every iteration. Instead of computing the exact Hessian, quasi-Newton methods build up an approximation of the Hessian matrix iteratively.

### Motivation and Basic Idea

Newton's method for optimization updates the parameter vector \( \theta \) as follows:
\[ \theta_{k+1} = \theta_k - H_f(\theta_k)^{-1} \nabla f(\theta_k), \]
where \( H_f(\theta_k) \) is the Hessian matrix of \( f \) at \( \theta_k \) and \( \nabla f(\theta_k) \) is the gradient.

However, computing \( H_f(\theta_k) \) and its inverse can be computationally expensive, especially for large-scale problems. Quasi-Newton methods avoid this by approximating the Hessian matrix using only gradient evaluations.

### Key Quasi-Newton Methods

1. **DFP (Davidon-Fletcher-Powell) Method**
2. **BFGS (Broyden-Fletcher-Goldfarb-Shanno) Method**
3. **L-BFGS (Limited-memory BFGS) Method**

### BFGS Method

The BFGS method is one of the most popular quasi-Newton methods due to its robustness and efficiency. It updates an approximation \( B_k \) of the inverse Hessian matrix \( H_f(\theta_k)^{-1} \) using the gradient information.

#### Algorithm

1. **Initialization**: Start with an initial guess \( \theta_0 \) and an initial approximation of the inverse Hessian \( B_0 \) (often chosen as the identity matrix \( I \)). Set the iteration counter \( k = 0 \).

2. **Update Rule**: At each iteration, perform the following steps:
   - Compute the gradient \( g_k = \nabla f(\theta_k) \).
   - Compute the search direction \( p_k = -B_k g_k \).
   - Perform a line search to find a suitable step size \( \alpha_k \) that sufficiently decreases the function value:
     \[ \theta_{k+1} = \theta_k + \alpha_k p_k. \]
   - Compute the new gradient \( g_{k+1} = \nabla f(\theta_{k+1}) \).
   - Update the parameters \( s_k \) and \( y_k \):
     \[ s_k = \theta_{k+1} - \theta_k, \]
     \[ y_k = g_{k+1} - g_k. \]
   - Update the approximation of the inverse Hessian \( B_{k+1} \) using the BFGS formula:
     \[ B_{k+1} = \left( I - \frac{s_k y_k^T}{y_k^T s_k} \right) B_k \left( I - \frac{y_k s_k^T}{y_k^T s_k} \right) + \frac{s_k s_k^T}{y_k^T s_k}. \]

3. **Iteration**: Increment \( k \) and repeat until convergence criteria are met (e.g., \(\|g_k\|\) is below a threshold).

#### Properties

- **Symmetry and Positive Definiteness**: The BFGS update preserves the symmetry and positive definiteness of the Hessian approximation.
- **Superlinear Convergence**: Under certain conditions, BFGS exhibits superlinear convergence, meaning the rate of convergence improves as the solution is approached.

### L-BFGS Method

The Limited-memory BFGS (L-BFGS) method is designed to handle large-scale optimization problems where storing the full Hessian approximation is impractical. Instead of storing \( B_k \) explicitly, L-BFGS maintains a limited number of vectors to implicitly represent the approximation.

#### Algorithm

1. **Initialization**: Start with an initial guess \( \theta_0 \). Set the memory parameter \( m \) (typically \( 5 \leq m \leq 20 \)).

2. **Update Rule**: At each iteration, perform the following steps:
   - Compute the gradient \( g_k = \nabla f(\theta_k) \).
   - Compute the search direction \( p_k \) using a two-loop recursion to apply the L-BFGS update implicitly.
   - Perform a line search to find a suitable step size \( \alpha_k \):
     \[ \theta_{k+1} = \theta_k + \alpha_k p_k. \]
   - Compute the new gradient \( g_{k+1} = \nabla f(\theta_{k+1}) \).
   - Update the parameters \( s_k \) and \( y_k \):
     \[ s_k = \theta_{k+1} - \theta_k, \]
     \[ y_k = g_{k+1} - g_k. \]
   - Update the memory of recent \( s \) and \( y \) vectors, keeping only the most recent \( m \) pairs.

3. **Iteration**: Increment \( k \) and repeat until convergence criteria are met.

#### Properties

- **Memory Efficiency**: By storing only a limited number of vectors, L-BFGS significantly reduces memory requirements, making it suitable for large-scale problems.
- **Scalability**: L-BFGS scales well with the number of parameters, allowing it to handle high-dimensional optimization tasks effectively.

### Advantages and Disadvantages

**Advantages**:
- **Efficiency**: Quasi-Newton methods provide rapid convergence without the need to compute the full Hessian.
- **Robustness**: These methods are robust and often outperform simpler gradient-based methods.

**Disadvantages**:
- **Complexity**: Implementing quasi-Newton methods can be more complex compared to simpler methods like gradient descent.
- **Line Search**: Effective line search algorithms are crucial for the performance of quasi-Newton methods, adding to the complexity.

### Conclusion

Quasi-Newton methods, particularly BFGS and L-BFGS, are powerful tools for optimization, combining efficiency and robustness. They approximate the Hessian matrix iteratively, avoiding the computational expense of exact second-order methods while maintaining rapid convergence. These methods are widely used in machine learning, scientific computing, and engineering optimization problems.

### Further Reading

For more comprehensive studies on quasi-Newton methods, consider the following references:

- "Numerical Optimization" by Jorge Nocedal and Stephen J. Wright: This book provides a detailed analysis of quasi-Newton methods, including BFGS and L-BFGS.
- "Convex Optimization" by Stephen Boyd and Lieven Vandenberghe: This book covers the theoretical foundations of optimization methods, including quasi-Newton methods.
- "Optimization Methods in Finance" by Gerard Cornuejols and Reha Tutuncu: This book includes practical applications of optimization methods in financial contexts, with discussions on quasi-Newton methods.


---

preceding: [[lipschitz continuity of the hessian]]  


following: [[hessian approximation]]

See also: [[partial derivatives]]