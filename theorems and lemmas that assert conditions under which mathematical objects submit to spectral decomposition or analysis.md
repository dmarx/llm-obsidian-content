---
date: 1714447853.370056
tags:
- autogenerated
- source__oai
- conversation_id__c20873a8-e4a3-4c92-a77b-eb08f1f7a9ff
- msg_aaa24d81-c2c3-4c65-b786-5688b669f127
- msg_087c59fa-ff7a-42fb-943b-fc206ac825de
- colorclass/vector-space
title: theorems and lemmas that assert conditions under which mathematical objects
  submit to spectral decomposition or analysis
---

**USER**

theorems and lemmas that assert conditions under which mathematical objects submit to spectral decomposition or analysis


**ASSISTANT**

Spectral decomposition and analysis are crucial concepts in various branches of mathematics, particularly in linear algebra and functional analysis. These techniques are used to simplify complex linear transformations by expressing them in terms of their eigenvalues and eigenvectors. Here, I'll outline key theorems and lemmas that provide conditions under which certain mathematical objects can be subjected to spectral decomposition.

### [[Spectral Theorem|Spectral Theorem]]

The [[Spectral Theorem]] is one of the most fundamental results concerning spectral decomposition. It asserts conditions under which an operator on a vector space can be diagonalized through a basis of eigenvectors.

#### For Symmetric Matrices (Real Case)

For real symmetric matrices, the spectral theorem can be stated as follows:

**Theorem:** Let $A$ be a real symmetric matrix. Then $A$ can be orthogonally diagonalized, which means there exists an orthogonal matrix $Q$ and a diagonal matrix $D$ such that:
$$
A = QDQ^T
$$
Here, the columns of $Q$ are orthonormal eigenvectors of $A$, and $D$ contains the corresponding eigenvalues.

#### For Normal Matrices (Complex Case)

In the complex case, the theorem applies to normal matrices (matrices that commute with their conjugate transpose):
**Theorem:** Let $A$ be a complex normal matrix. Then $A$ can be unitarily diagonalized, that is, there exists a unitary matrix $U$ and a diagonal matrix $D$ such that:
$$
A = UDU^*
$$
where $U^*$ is the conjugate transpose of $U$.

### [[Singular Value Decomposition (SVD)]]

**Singular Value Decomposition (SVD):** Every $m \times n$ matrix $A$ can be decomposed into the product of three matrices:
$$
A = U \Sigma V^*
$$
where $U$ is an $m \times m$ unitary matrix, $V^*$ is the conjugate transpose of an $n \times n$ unitary matrix, and $\Sigma$ is an $m \times n$ diagonal matrix with non-negative real numbers on the diagonal. These numbers are known as the singular values of $A$.

### Perron-Frobenius Theorem

The [[Perron-Frobenius Theorem]] is critical in positive matrices and provides conditions under which a unique largest eigenvalue exists.

**Theorem:** For a positive square matrix $A$, there exists a real positive eigenvalue $\lambda$ (called the Perron-Frobenius eigenvalue) such that:
$$
\lambda \geq |\mu|
$$
for any other eigenvalue $\mu$ of $A$. Furthermore, the corresponding eigenvector can be chosen to have strictly positive components.

### Gershgorin Circle Theorem

The [[Gershgorin Circle Theorem]] gives a method to locate the eigenvalues of a complex square matrix. It states:
**Theorem:** Each eigenvalue of $A$ lies within at least one of the Gershgorin discs, defined as:
$$
D(a_{ii}, R_i) = \{ z \in \mathbb{C} : |z - a_{ii}| \leq R_i \}
$$
where $R_i = \sum_{j \neq i} |a_{ij}|$.

These theorems and lemmas provide a robust framework for analyzing matrices and operators using spectral methods. The spectral decomposition particularly illuminates many properties of linear transformations, including stability, similarity, and more in-depth structural insights.


---


following: [[more generally though, e.g. spaces that submit to eigenmodes]]

See also: [[vector space]]


See also: [[spectral methods]]


See also: [[spectral theorem]]