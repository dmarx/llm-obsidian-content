---
date: 1721688210.030152
tags:
- autogenerated
- source__oai
- conversation_id__a2d4a2dc-4888-4b9b-aa34-ef6394b04fb7
- msg_aaa25fba-f074-48d3-a06b-3d4883d8718e
- msg_b1ea04ed-1050-42a8-ade1-4259ff5c3dc2
- colorclass/information-theory
title: relative entropy
---

**USER**

Relative Entropy


**ASSISTANT**

## Relative Entropy

### Overview

Relative entropy, also known as Kullback-Leibler (KL) divergence, is a measure of how one probability distribution diverges from a second, reference probability distribution. It quantifies the "distance" between two distributions, although it is not a true distance metric since it is not symmetric and does not satisfy the triangle inequality. Relative entropy is widely used in information theory, statistics, machine learning, and thermodynamics.

### Definition

For discrete probability distributions \(P\) and \(Q\) defined on the same probability space \(\mathcal{X}\), the relative entropy (KL divergence) of \(P\) from \(Q\) is defined as:

$$
D(P \| Q) = \sum_{x \in \mathcal{X}} P(x) \log \frac{P(x)}{Q(x)}.
```

For continuous probability distributions with densities \(p(x)\) and \(q(x)\), the relative entropy is defined as:

$$
D(P \| Q) = \int_{\mathcal{X}} p(x) \log \frac{p(x)}{q(x)} \, dx.
```

### Properties

1. **Non-negativity**: \(D(P \| Q) \geq 0\) for all probability distributions \(P\) and \(Q\), with equality if and only if \(P = Q\) almost everywhere.
2. **Asymmetry**: \(D(P \| Q) \neq D(Q \| P)\) in general.
3. **Not a True Distance**: KL divergence does not satisfy the triangle inequality and is therefore not a true metric.

### Interpretation

Relative entropy measures the expected number of extra bits required to code samples from distribution \(P\) using a code optimized for distribution \(Q\) rather than the true distribution \(P\). It can be interpreted as a measure of inefficiency or information loss.

### Applications

#### Information Theory

In information theory, relative entropy is used to quantify the difference between two probability distributions, often in the context of coding and data compression. It measures the inefficiency of assuming the distribution \(Q\) when the true distribution is \(P\).

#### Statistics

In statistics, relative entropy is used in hypothesis testing, Bayesian inference, and model selection. It quantifies the divergence between the empirical distribution and a theoretical model.

#### Machine Learning

In machine learning, relative entropy is used as a loss function for training models, particularly in classification tasks. It measures the difference between the predicted probability distribution and the true distribution of the labels.

#### Thermodynamics

In thermodynamics, relative entropy is used to measure the irreversibility of processes and the distance from equilibrium. It is related to the concept of free energy and entropy in statistical mechanics.

### Examples

#### Example 1: Bernoulli Distributions

Consider two Bernoulli distributions \(P\) and \(Q\) with parameters \(p\) and \(q\), respectively. The relative entropy \(D(P \| Q)\) is:

$$
D(P \| Q) = p \log \frac{p}{q} + (1-p) \log \frac{1-p}{1-q}.
```

#### Example 2: Gaussian Distributions

Consider two normal distributions \(P = N(\mu_1, \sigma_1^2)\) and \(Q = N(\mu_2, \sigma_2^2)\). The relative entropy \(D(P \| Q)\) is:

$$
D(P \| Q) = \log \frac{\sigma_2}{\sigma_1} + \frac{\sigma_1^2 + (\mu_1 - \mu_2)^2}{2\sigma_2^2} - \frac{1}{2}.
```

### Connections to Other Concepts

#### Jensen-Shannon Divergence

The Jensen-Shannon divergence is a symmetric and finite variant of the KL divergence. It is defined as:

$$
JSD(P \| Q) = \frac{1}{2} D(P \| M) + \frac{1}{2} D(Q \| M),
```

where \(M = \frac{1}{2}(P + Q)\) is the average of the two distributions.

#### Maximum Likelihood Estimation

In maximum likelihood estimation, the goal is to find the parameters that minimize the KL divergence between the empirical distribution of the data and the model distribution.

### Further Reading

- [[Large Deviations Theory]]
- [[Sanov's Theorem]]
- [[Gartner-Ellis Theorem]]
- [[Cram√©r's Theorem]]
- [[Legendre-Fenchel Transform]]
- [[Lagrangian Duality]]
- [[Optimization Problems in Spin Glasses]]

Relative entropy is a fundamental concept with wide-ranging applications in various fields. For a comprehensive exploration, refer to specialized texts in information theory, statistics, and machine learning.


---

preceding: [[lagrangian duality]]  


following: [[jensen-shannon divergence]]

See also: [[shannon-mcmillan-breiman theorem]]


See also: [[maximum entropy production]]


See also: [[information theory]]


See also: [[expand on the relationship between mutual information and kl divergence]]


See also: [[geometrical statistical physics]]


See also: [[free energy]]


See also: [[free entropy theory]]