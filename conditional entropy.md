---
date: 1714768747.31579
tags:
- autogenerated
- source__oai
- conversation_id__212e3628-2bdc-43ef-a6d3-6e47e5e4dfec
- msg_aaa27295-fd81-41bd-91eb-5feaef4116cb
- msg_e6b23359-025b-49ab-bd12-1fc380e76207
- colorclass/information-theory
title: conditional entropy
---

**USER**

Conditional Entropy


**ASSISTANT**

Conditional entropy, also known as the entropy of \(X\) given \(Y\), is a fundamental concept in information theory that measures the amount of uncertainty remaining about a random variable \(X\) after the value of another random variable \(Y\) is known. It quantifies the average residual entropy of \(X\) conditioned on different possible values of \(Y\).

### Definition

The conditional entropy of \(X\) given \(Y\) for discrete random variables is defined as:
$$
H(X|Y) = -\sum_{y} p(y) \sum_{x} p(x|y) \log p(x|y)
$$
where \(p(x|y)\) is the conditional probability of \(X\) given \(Y\), and \(p(y)\) is the probability of \(Y\).

For continuous variables, the definition extends to:
$$
H(X|Y) = -\int_y p(y) \left( \int_x p(x|y) \log p(x|y) \, dx \right) \, dy
$$
where \(p(x|y)\) is the conditional probability density function of \(X\) given \(Y\), and the integrals replace the sums.

### Interpretation

1. **Residual Uncertainty**: Conditional entropy quantifies the expected amount of information required to describe \(X\) when \(Y\) is known. It provides a measure of the unpredictability of \(X\) after observing \(Y\).

2. **Reduction from Unconditional Entropy**: The difference between the unconditional entropy \(H(X)\) and the conditional entropy \(H(X|Y)\) gives the [[mutual information]] \(I(X;Y)\), which measures how much knowing \(Y\) reduces the uncertainty in \(X\):
   $$
   I(X;Y) = H(X) - H(X|Y)
   $$
   This represents the amount of information that \(Y\) provides about \(X\).

3. **Properties**:
   - **Non-Negativity**: \(H(X|Y) \geq 0\), with equality if and only if \(X\) is a function of \(Y\).
   - **Chain Rule**: Conditional entropy adheres to the chain rule of entropy, analogous to the chain rule in probability:
     $$
     H(X_1, X_2 | Y) = H(X_1 | Y) + H(X_2 | X_1, Y)
     $$
   - **Conditioning Reduces Entropy**: \(H(X|Y) \leq H(X)\), indicating that additional information (conditioning by \(Y\)) typically reduces uncertainty.

### Applications

Conditional entropy is widely used in various fields, including:

- **Machine Learning and Data Mining**: In decision tree algorithms, such as the ID3 and C4.5 algorithms, conditional entropy is used to select features that best partition sets of data points, effectively reducing the entropy or randomness of the subsets formed.

- **Cryptography**: It is used to measure the amount of uncertainty an adversary would have about a plaintext given some knowledge about the ciphertext.

- **Communication Systems**: In channel coding, conditional entropy helps to determine the efficiency of information transfer over a noisy channel, indicating the amount of information lost due to the noise in the channel.

- **Complex Systems Analysis**: In studying complex networks or systems, conditional entropy can provide insights into conditional dependencies and the flow of information between components of the system.

In summary, conditional entropy is a versatile and powerful concept in information theory that helps in understanding and quantifying how the knowledge of one variable affects the uncertainty of another, enhancing our ability to analyze and design communication, computation, and prediction systems.


---

preceding: [[geometric interpretation of [[renyi entropy]]]]  


following: [[geometric interpretation of conditional entropy]]

See also: [[shannon-mcmillan-breiman theorem]]


See also: [[geometric interpretation of conditional entropy]]


See also: [[tononi-sporns-edelman (tse) complexity]]


See also: [[information scale transform]]


See also: [[shannon's noisy channel coding theorem]]


See also: [[interventional complexity]]


See also: [[information theory]]


See also: [[information theory of deep learning]]


See also: [[plausible deniability architecture]]


See also: [[theories and philosophies of camouflage]]


See also: [[channel capacity]]


See also: [[residual entropy]]


See also: [[architectural anti-patterns]]


See also: [[interpretive distance]]


See also: [[scale-dependent information theory]]


See also: [[tell me about partial conditional mutual information from mixed embeddings. be as technical as possible, i have graduate degrees in math and statistics]]


See also: [[random variables]]


See also: [[geometric interpretation]]


See also: [[mutual information formulation of absorption efficiency]]


See also: [[rainer mühlhoff’s theory of sealed surfaces]]


See also: [[entropy in information theory]]


See also: [[opening the black box of deep neural networks via information theory]]