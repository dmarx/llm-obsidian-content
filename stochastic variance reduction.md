---
date: 1717806168.487404
tags:
- autogenerated
- source__oai
- conversation_id__77e70e10-f02a-4f98-bd5c-6217b10122cf
- msg_aaa2f029-3766-4c8d-9b01-c9fb1c3408ad
- msg_ce536e5d-29b3-42eb-abd3-c2c73c116592
- colorclass/vector-space
title: stochastic variance reduction
---

**USER**

stochastic variance reduction


**ASSISTANT**

Stochastic variance reduction techniques are crucial for improving the convergence rates of stochastic gradient descent (SGD) methods, especially when dealing with large-scale optimization problems commonly encountered in machine learning. These techniques aim to reduce the variance of the gradient estimates, leading to faster and more stable convergence.

### Key Techniques for Stochastic Variance Reduction

1. **Stochastic Variance Reduced Gradient (SVRG)**
2. **SAGA**
3. **SARAH**

### 1. Stochastic Variance Reduced Gradient (SVRG)

SVRG is one of the most well-known variance reduction techniques. The key idea is to periodically compute a full gradient over the entire dataset to serve as a control variate that reduces the variance of the stochastic gradient estimates.

#### Algorithm

Let \( f(\theta) \) be the objective function to minimize, typically of the form:
\[ f(\theta) = \frac{1}{n} \sum_{i=1}^n f_i(\theta), \]
where \( f_i(\theta) \) represents the loss for the \(i\)-th data point.

**SVRG Algorithm**:
1. **Initialization**: Choose an initial parameter vector \(\theta_0\) and a step size \(\eta\).
2. **Outer Loop**: Iterate over \(m\) epochs:
   - Compute the full gradient:
     \[ \tilde{\theta} = \theta_t, \quad \tilde{\nabla} = \frac{1}{n} \sum_{i=1}^n \nabla f_i(\tilde{\theta}). \]
   - **Inner Loop**: Perform \(k\) updates:
     - Randomly pick an index \( i_t \) from \(\{1, \ldots, n\}\).
     - Compute the variance-reduced gradient:
       \[ \nabla_t = \nabla f_{i_t}(\theta_t) - \nabla f_{i_t}(\tilde{\theta}) + \tilde{\nabla}. \]
     - Update the parameter:
       \[ \theta_{t+1} = \theta_t - \eta \nabla_t. \]
3. **Output**: The final parameter vector \(\theta\).

#### Convergence

SVRG achieves a convergence rate of \(O((1 - \eta \mu)^k)\) for strongly convex functions, where \(\mu\) is the strong convexity parameter, significantly improving over traditional SGD.

### 2. SAGA

SAGA is another popular variance reduction technique that maintains and updates an estimate of the gradient for each data point. It combines the benefits of both the SVRG and SAG (Stochastic Average Gradient) algorithms.

#### Algorithm

**SAGA Algorithm**:
1. **Initialization**: Choose an initial parameter vector \(\theta_0\), step size \(\eta\), and initialize gradient estimates \( \nabla f_i(\theta_0) \) for all \( i \).
2. **Loop**: For each iteration \( t \):
   - Randomly pick an index \( i_t \) from \(\{1, \ldots, n\}\).
   - Compute the variance-reduced gradient:
     \[ \nabla_t = \nabla f_{i_t}(\theta_t) - \nabla f_{i_t}(\alpha_{i_t}) + \frac{1}{n} \sum_{j=1}^n \nabla f_j(\alpha_j), \]
     where \(\alpha_i\) is the most recent point at which the gradient \( \nabla f_i \) was computed.
   - Update the parameter:
     \[ \theta_{t+1} = \theta_t - \eta \nabla_t. \]
   - Update the stored gradient estimate:
     \[ \nabla f_{i_t}(\alpha_{i_t}) = \nabla f_{i_t}(\theta_{t+1}). \]
3. **Output**: The final parameter vector \(\theta\).

#### Convergence

SAGA achieves a linear convergence rate for strongly convex functions, similar to SVRG, but does not require full gradient computations periodically, making it more efficient in practice.

### 3. Stochastic Recursive Gradient Algorithm (SARAH)

SARAH is a relatively recent variance reduction technique that maintains a recursive structure to compute the variance-reduced gradient.

#### Algorithm

**SARAH Algorithm**:
1. **Initialization**: Choose an initial parameter vector \(\theta_0\) and step size \(\eta\).
2. **Outer Loop**: Iterate over epochs:
   - Compute the full gradient:
     \[ \tilde{\theta} = \theta_t, \quad \tilde{\nabla} = \frac{1}{n} \sum_{i=1}^n \nabla f_i(\tilde{\theta}). \]
   - **Inner Loop**: Perform updates:
     - Randomly pick an index \( i_t \) from \(\{1, \ldots, n\}\).
     - Compute the gradient difference:
       \[ \nabla_t = \nabla f_{i_t}(\theta_t) - \nabla f_{i_t}(\theta_{t-1}) + \nabla_{t-1}. \]
     - Update the parameter:
       \[ \theta_{t+1} = \theta_t - \eta \nabla_t. \]
3. **Output**: The final parameter vector \(\theta\).

#### Convergence

SARAH has strong convergence guarantees and performs well in practice, offering a good balance between computational efficiency and convergence speed.

### Summary

Stochastic variance reduction techniques significantly improve the efficiency of stochastic optimization methods. SVRG, SAGA, and SARAH are among the most influential methods, each with its strengths:

- **SVRG**: Periodically computes a full gradient, achieving fast convergence.
- **SAGA**: Maintains per-data-point gradient estimates, reducing the need for full gradient computations.
- **SARAH**: Uses a recursive approach to efficiently reduce gradient variance.

### Further Reading

For a comprehensive study of stochastic variance reduction methods, consider the following references:

- "Stochastic Gradient Descent: Going Faster" by Léon Bottou, Frank E. Curtis, and Jorge Nocedal.
- "Optimization Methods for Large-Scale Machine Learning" by Léon Bottou, Éric Bresson, and Arnaud Legrand.
- "Introductory Lectures on Convex Optimization: A Basic Course" by Yurii Nesterov.

These texts and papers provide detailed theoretical analyses, practical implementations, and a wide range of applications of stochastic variance reduction techniques in machine learning and optimization.


---

preceding: [[important results in the theoretical analysis of the em algorithm]]  


following: [[important results in the theoretical analysis of stochastic gradient descent]]