---
date: 1718296963.531116
tags:
- autogenerated
- source__oai
- conversation_id__e4d7038a-7277-4fce-a29a-cfe6b405a119
- msg_aaa2dff6-c2fc-4791-9847-538c75268d81
- msg_b5d0986d-6888-4f2f-b0c0-29447902a399
- msg_aaa2b3da-0c5b-42ad-a3d6-71f6cf56e421
- msg_1bb9a1ce-6ac6-490a-b678-878423bfbbed
- msg_aaa2116a-fab1-444a-b67c-e307604d0132
- msg_251d310a-bbf9-4eed-88b6-03cae5e4712d
- colorclass/decision-making-processes
title: branch and bound
---

**USER**

Branch and Bound


**ASSISTANT**

[[Branch and Bound]] is an algorithmic paradigm used to solve combinatorial optimization problems. It systematically explores the solution space by dividing it into smaller subproblems (branches) and evaluating their bounds to eliminate parts of the search space that cannot contain an optimal solution. This approach helps in efficiently solving problems that would otherwise require exhaustive search.

## Key Concepts

### Branching
Branching involves dividing the problem into smaller subproblems, each representing a portion of the solution space. This is typically done by making a series of choices or decisions, such as including or excluding items in a set.

### Bounding
Bounding involves calculating an upper or lower bound on the optimal solution within a subproblem. If a bound indicates that the subproblem cannot contain a better solution than the current best known solution, the subproblem can be discarded (pruned).

### Pruning
Pruning is the process of discarding subproblems that cannot lead to an optimal solution based on their bounds. This reduces the search space and improves efficiency.

### Backtracking
Backtracking is used to explore different branches by reversing the most recent decision when a subproblem is pruned or completely explored.

## Steps of Branch and Bound

1. **Initialization**: Start with the entire solution space as the initial problem.
2. **Branching**: Divide the problem into smaller subproblems.
3. **Bounding**: Calculate bounds for each subproblem.
4. **Pruning**: Discard subproblems that cannot yield a better solution than the current best known solution.
5. **Selection**: Select the next subproblem to explore based on a strategy (e.g., depth-first, breadth-first, or best-bound).
6. **Iteration**: Repeat the branching, bounding, pruning, and selection steps until all subproblems are either solved or pruned.

## Example Applications

### Traveling Salesman Problem (TSP)
The TSP seeks the shortest possible route that visits each city exactly once and returns to the origin city.

1. **Branching**: Divide the problem by fixing the first few edges of the route.
2. **Bounding**: Calculate a lower bound on the tour length using a method like the minimum spanning tree or reduced cost matrix.
3. **Pruning**: Discard subproblems where the lower bound exceeds the length of the current best solution.
4. **Solution**: Continue exploring and pruning until the shortest route is found.

### Knapsack Problem
The Knapsack Problem involves selecting items with given weights and values to maximize total value without exceeding a weight limit.

1. **Branching**: Create subproblems by including or excluding each item.
2. **Bounding**: Use a greedy approach or linear relaxation to estimate an upper bound on the total value.
3. **Pruning**: Discard subproblems where the upper bound is less than the current best solution.
4. **Solution**: Explore subproblems until the optimal combination of items is found.

## Mathematical Formalization

### Branch and Bound for Integer Programming
Consider an integer programming problem:
$$ \max \{ c^T x \mid Ax \leq b, x \in \mathbb{Z}^n \} $$

1. **Branching**: Split the problem based on variable bounds, such as \( x_i \leq k \) and \( x_i > k \) for some integer \( k \).
2. **Bounding**: Solve the linear relaxation of the subproblem to obtain an upper bound.
3. **Pruning**: If the solution to the linear relaxation is not feasible in the integer domain or its objective value is less than the current best solution, prune the subproblem.
4. **Iteration**: Continue until the best feasible integer solution is found.

### Example: Integer Linear Programming (ILP)

Given:
$$ \max \{ 2x_1 + 3x_2 \mid x_1 + 2x_2 \leq 4, x_1, x_2 \geq 0, x_1, x_2 \in \mathbb{Z} \} $$

1. **Initial Problem**: Solve the relaxed problem \( \max \{ 2x_1 + 3x_2 \mid x_1 + 2x_2 \leq 4, x_1, x_2 \geq 0 \} \). Suppose the solution is \( x_1 = 1.5, x_2 = 1.25 \) with objective value \( 6.75 \).
2. **Branching**: Create two subproblems:
   - \( x_1 \leq 1 \)
   - \( x_1 \geq 2 \)
3. **Bounding**: Solve the relaxed versions of each subproblem.
4. **Pruning**: Discard subproblems where the bound is less than the current best integer solution.
5. **Iteration**: Continue branching and bounding until the optimal integer solution is found.

## Advantages and Disadvantages

### Advantages
- **Efficiency**: Pruning significantly reduces the search space, making the algorithm faster than brute force.
- **Optimality**: Guarantees finding the optimal solution.
- **Flexibility**: Applicable to a wide range of optimization problems.

### Disadvantages
- **Complexity**: Requires good bounding functions to be efficient.
- **Memory Usage**: Can consume a lot of memory for storing subproblems.
- **Time Consumption**: Can be slow for problems with weak bounds or large branching factors.

## Further Reading
- [[Traveling Salesman Problem]]
- [[Knapsack Problem]]
- [[Integer Linear Programming]]
- [[Dynamic Programming]]
- [[Greedy Algorithms]]

Branch and Bound is a powerful paradigm for solving optimization problems by efficiently navigating and pruning the solution space. Its flexibility and applicability to various problems make it a crucial tool in computational optimization.


**USER**

more granular please


**ASSISTANT**

Certainly! Let's delve into the [[Branch and Bound]] paradigm with greater granularity, focusing on its application, implementation details, and illustrative examples.

## Key Concepts in Detail

### Branching
Branching involves dividing the problem into smaller subproblems, typically by making a series of binary decisions. For instance, in the Knapsack Problem, branching can be done by deciding whether to include or exclude each item.

### Bounding
Bounding involves computing an upper or lower bound for the objective function on the solution within a subproblem. If the bound indicates that the subproblem cannot yield a better solution than the current best known solution, it is discarded.

### Pruning
Pruning is the process of discarding subproblems that cannot improve upon the current best solution, effectively reducing the size of the search space.

### Backtracking
Backtracking is the process of reversing the most recent decision when a subproblem is either fully explored or pruned, allowing the algorithm to explore alternative branches.

## Steps of Branch and Bound

1. **Initialization**:
   - Start with the entire solution space.
   - Set the current best solution to an initial feasible solution or to negative/positive infinity (depending on the problem being maximized or minimized).

2. **Branching**:
   - Choose a variable and create subproblems by partitioning the variable's domain.
   - Each subproblem represents a subset of the solution space.

3. **Bounding**:
   - Compute bounds for the objective function for each subproblem.
   - These bounds are typically derived from relaxations of the original problem (e.g., ignoring integer constraints in integer programming).

4. **Pruning**:
   - Discard subproblems where the bound does not exceed (for maximization) or is not below (for minimization) the current best solution.

5. **Selection**:
   - Select the next subproblem to explore based on a specific strategy (e.g., depth-first search, breadth-first search, or best-bound).

6. **Iteration**:
   - Repeat branching, bounding, pruning, and selection until all subproblems are either solved or pruned.

## Example Applications

### Traveling Salesman Problem (TSP)

The TSP seeks the shortest possible route that visits each city exactly once and returns to the origin city.

#### Step-by-Step Process

1. **Initialization**:
   - Start with an initial tour (e.g., found using a heuristic) and set this as the current best solution.

2. **Branching**:
   - Start from a city and branch by selecting the next city in the tour.
   - Each branch represents a partial tour.

3. **Bounding**:
   - Compute a lower bound for the tour length using methods such as the minimum spanning tree of the remaining cities or a reduced cost matrix.

4. **Pruning**:
   - Discard branches where the lower bound exceeds the length of the current best tour.

5. **Selection**:
   - Use depth-first search to explore the branches, prioritizing those with the lowest bound.

6. **Iteration**:
   - Continue until the shortest tour is found.

### Knapsack Problem

The Knapsack Problem involves selecting items with given weights and values to maximize total value without exceeding a weight limit.

#### Step-by-Step Process

1. **Initialization**:
   - Start with the entire set of items and an initial feasible solution (e.g., the greedy solution) as the current best solution.

2. **Branching**:
   - Create two branches: one including the current item and one excluding it.
   - Each branch represents a subset of the items.

3. **Bounding**:
   - Compute an upper bound on the total value for each subproblem. A common method is to use the greedy solution for the remaining items or solve the linear relaxation of the subproblem.

4. **Pruning**:
   - Discard branches where the upper bound is less than the value of the current best solution.

5. **Selection**:
   - Use best-bound search to select the subproblem with the highest upper bound.

6. **Iteration**:
   - Continue until the optimal combination of items is found.

## Mathematical Formalization

### Integer Linear Programming (ILP)

Consider an integer programming problem:
$$ \max \{ c^T x \mid Ax \leq b, x \in \mathbb{Z}^n \} $$

1. **Branching**:
   - Select a fractional variable \( x_i \) and create two branches: \( x_i \leq \lfloor x_i \rfloor \) and \( x_i \geq \lceil x_i \rceil \).

2. **Bounding**:
   - Solve the linear relaxation (i.e., ignore the integer constraints) to obtain an upper bound.
   - The solution to the relaxed problem provides a bound for the subproblem.

3. **Pruning**:
   - If the bound is less than the current best integer solution, prune the branch.
   - If the solution is integral and better than the current best, update the best solution.

4. **Iteration**:
   - Continue branching and bounding until the optimal integer solution is found.

### Example: Solving an ILP

Given:
$$ \max \{ 2x_1 + 3x_2 \mid x_1 + 2x_2 \leq 4, x_1, x_2 \geq 0, x_1, x_2 \in \mathbb{Z} \} $$

1. **Initial Problem**:
   - Solve the relaxed problem: \( \max \{ 2x_1 + 3x_2 \mid x_1 + 2x_2 \leq 4, x_1, x_2 \geq 0 \} \).
   - Suppose the solution is \( x_1 = 1.5, x_2 = 1.25 \) with objective value \( 6.75 \).

2. **Branching**:
   - Create two subproblems: \( x_1 \leq 1 \) and \( x_1 \geq 2 \).

3. **Bounding**:
   - Solve the relaxed versions of each subproblem:
     - For \( x_1 \leq 1 \): \( \max \{ 2x_1 + 3x_2 \mid x_1 + 2x_2 \leq 4, x_1 \leq 1, x_2 \geq 0 \} \). Suppose the solution is \( x_1 = 1, x_2 = 1.5 \) with value \( 6.5 \).
     - For \( x_1 \geq 2 \): Infeasible since \( x_1 + 2x_2 \leq 4 \) and \( x_1 \geq 2 \) cannot be satisfied simultaneously.

4. **Pruning**:
   - Discard the infeasible branch \( x_1 \geq 2 \).
   - Continue with \( x_1 \leq 1 \).

5. **Iteration**:
   - Continue branching on \( x_2 \) for the feasible subproblem, solving and pruning until the optimal integer solution is found.

## Advantages and Disadvantages

### Advantages
- **Efficiency**: Pruning significantly reduces the search space, making the algorithm faster than brute force.
- **Optimality**: Guarantees finding the optimal solution if properly implemented.
- **Flexibility**: Applicable to a wide range of optimization problems.

### Disadvantages
- **Complexity**: Requires good bounding functions to be efficient.
- **Memory Usage**: Can consume a lot of memory for storing subproblems.
- **Time Consumption**: Can be slow for problems with weak bounds or large branching factors.

## Further Reading
- [[Traveling Salesman Problem]]
- [[Knapsack Problem]]
- [[Integer Linear Programming]]
- [[Dynamic Programming]]
- [[Greedy Algorithms]]

Branch and Bound is a robust paradigm for tackling optimization problems by strategically navigating and pruning the solution space. Its flexibility and efficiency make it indispensable in fields requiring optimal solutions, such as operations research and [[computational geometry]].


**USER**

more granular please


**ASSISTANT**

Sure! Here's a more granular look at each of the fundamental algorithmic paradigms, including detailed explanations and specific examples.

## 1. Divide and Conquer

### Principle
Divide the problem into smaller subproblems, solve each subproblem recursively, and combine their solutions to solve the original problem.

### Detailed Steps
1. **Divide**: Split the problem into smaller subproblems.
2. **Conquer**: Solve each subproblem recursively. If the subproblem is small enough, solve it directly.
3. **Combine**: Merge the solutions of the subproblems to form the final solution.

### Example: Merge Sort
- **Divide**: Split the array into two halves.
- **Conquer**: Recursively sort each half.
- **Combine**: Merge the two sorted halves.

#### Pseudocode
```plaintext
function mergeSort(array):
    if length(array) <= 1:
        return array
    middle = length(array) / 2
    left = mergeSort(array[0:middle])
    right = mergeSort(array[middle:])
    return merge(left, right)

function merge(left, right):
    result = []
    while left and right are not empty:
        if left[0] <= right[0]:
            append left[0] to result
            remove left[0]
        else:
            append right[0] to result
            remove right[0]
    append remaining elements of left or right to result
    return result
```

### Example: Quick Sort
- **Divide**: Choose a pivot element and partition the array into two subarrays, one with elements less than the pivot and one with elements greater than the pivot.
- **Conquer**: Recursively sort the subarrays.
- **Combine**: The array is sorted in place, so no additional combining step is needed.

#### Pseudocode
```plaintext
function quickSort(array, low, high):
    if low < high:
        pi = partition(array, low, high)
        quickSort(array, low, pi - 1)
        quickSort(array, pi + 1, high)

function partition(array, low, high):
    pivot = array[high]
    i = low - 1
    for j = low to high - 1:
        if array[j] <= pivot:
            i = i + 1
            swap array[i] and array[j]
    swap array[i + 1] and array[high]
    return i + 1
```

### Example: Binary Search
- **Divide**: Compare the target element with the middle element of the array.
- **Conquer**: Recursively search in the left or right half of the array.
- **Combine**: No explicit combine step is needed.

#### Pseudocode
```plaintext
function binarySearch(array, target, low, high):
    if low > high:
        return -1
    middle = (low + high) / 2
    if array[middle] == target:
        return middle
    if array[middle] > target:
        return binarySearch(array, target, low, middle - 1)
    else:
        return binarySearch(array, target, middle + 1, high)
```

## 2. Dynamic Programming

### Principle
Solve complex problems by breaking them down into simpler overlapping subproblems, storing the results of these subproblems to avoid redundant computations.

### Detailed Steps
1. **Define Subproblems**: Break the original problem into smaller, overlapping subproblems.
2. **Solve Subproblems**: Solve each subproblem, usually using a recursive approach.
3. **Store Results**: Store the results of subproblems to avoid recomputation.
4. **Build Solution**: Construct the solution to the original problem from the stored results.

### Example: Fibonacci Sequence
- **Subproblems**: $F(n) = F(n-1) + F(n-2)$.
- **Base Cases**: $F(0) = 0$, $F(1) = 1$.

#### Pseudocode
```plaintext
function fibonacci(n):
    if n <= 1:
        return n
    fib = [0, 1]
    for i = 2 to n:
        fib[i] = fib[i - 1] + fib[i - 2]
    return fib[n]
```

### Example: Knapsack Problem
- **Subproblems**: Define $K(n, W)$ as the maximum value that can be obtained with $n$ items and a capacity $W$.
- **Recurrence**: $K(n, W) = \max(K(n-1, W), K(n-1, W - w_n) + v_n)$, where $w_n$ and $v_n$ are the weight and value of the $n$-th item.

#### Pseudocode
```plaintext
function knapsack(values, weights, W):
    n = length(values)
    K = array of size (n + 1) by (W + 1)
    for i = 0 to n:
        for w = 0 to W:
            if i == 0 or w == 0:
                K[i][w] = 0
            else if weights[i - 1] <= w:
                K[i][w] = max(values[i - 1] + K[i - 1][w - weights[i - 1]], K[i - 1][w])
            else:
                K[i][w] = K[i - 1][w]
    return K[n][W]
```

### Example: Longest Common Subsequence
- **Subproblems**: Define $LCS(X, Y, m, n)$ as the length of the longest common subsequence of $X[0 \dots m-1]$ and $Y[0 \dots n-1]$.
- **Recurrence**: $LCS(X, Y, m, n) = \begin{cases} 
0 & \text{if } m = 0 \text{ or } n = 0 \\
LCS(X, Y, m-1, n-1) + 1 & \text{if } X[m-1] = Y[n-1] \\
\max(LCS(X, Y, m-1, n), LCS(X, Y, m, n-1)) & \text{otherwise} 
\end{cases}$

#### Pseudocode
```plaintext
function LCS(X, Y):
    m = length(X)
    n = length(Y)
    L = array of size (m + 1) by (n + 1)
    for i = 0 to m:
        for j = 0 to n:
            if i == 0 or j == 0:
                L[i][j] = 0
            else if X[i - 1] == Y[j - 1]:
                L[i][j] = L[i - 1][j - 1] + 1
            else:
                L[i][j] = max(L[i - 1][j], L[i][j - 1])
    return L[m][n]
```

## 3. Greedy Algorithms

### Principle
Make a sequence of choices, each of which looks best at the moment, with the hope of finding a global optimum.

### Detailed Steps
1. **Choice Property**: At each step, make the locally optimal choice.
2. **Feasibility**: Ensure the choice leads to a feasible solution.
3. **Optimal Substructure**: Ensure the problem can be solved using optimal solutions to subproblems.

### Example: Prim’s Algorithm
- **Choice Property**: Choose the smallest edge that connects a vertex in the growing minimum spanning tree (MST) to a vertex outside it.

#### Pseudocode
```plaintext
function prim(graph, start):
    MST = empty set
    visited = set containing start
    edges = priority queue with edges from start
    while edges is not empty:
        (weight, u, v) = edges.extractMin()
        if v not in visited:
            visited.add(v)
            MST.add((u, v, weight))
            for each edge (v, to, weight) in graph.edges(v):
                if to not in visited:
                    edges.insert((weight, v, to))
    return MST
```

### Example: Kruskal’s Algorithm
- **Choice Property**: Add the next smallest edge that does not form a cycle in the MST.

#### Pseudocode
```plaintext
function kruskal(graph):
    MST = empty set
    edges = sorted list of graph edges by weight
    disjointSet = makeSet for each vertex
    for each edge (u, v, weight) in edges:
        if findSet(u) != findSet(v):
            MST.add((u, v, weight))
            union(u, v)
    return MST
```

### Example: Dijkstra’s Algorithm
- **Choice Property**: Choose the closest unvisited node to the source and update distances to its neighbors.

#### Pseudocode
```plaintext
function dijkstra(graph, source):
    dist = dictionary with default value infinity
    dist[source] = 0
    priorityQueue = priority queue with (0, source)
    while priorityQueue is not empty:
        (d, u) = priorityQueue.extractMin()
        for each neighbor v of u:
            alt = d + weight(u, v)
            if alt < dist[v]:
                dist[v] = alt
                priorityQueue.insert((alt, v))
    return dist
```

## 4. Backtracking

### Principle
Generate all possible solutions and discard those that fail to


---

preceding: [[fundamental algorithmic paradigms]]

See also: [[combinatorial optimization]]


See also: [[integer programming (ip)]]


See also: [[mixed-integer programming (mip)]]


See also: [[job shop scheduling]]


See also: [[operations research]]


See also: [[branch and bound methods]]


See also: [[cutting planes]]