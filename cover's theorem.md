---
date: 1715966889.707928
tags:
- autogenerated
- source__oai
- conversation_id__e536633b-8eed-4822-967e-58113d1a33c2
- msg_aaa24b2e-005e-4b9e-bf2f-b93a73805711
- msg_fa95836d-a8f5-41b0-9558-7ffd4fba606b
- colorclass/vector-space
title: cover's theorem
---

**USER**

Cover's Theorem


**ASSISTANT**

## Cover's Theorem

Cover's Theorem is a fundamental result in the field of [[Machine Learning]] and pattern recognition, named after [[Thomas M. Cover]], a prominent figure in information theory. The theorem provides significant insights into the capacity of machine learning models to classify data when mapped into higher-dimensional spaces.

### Statement of the Theorem

Cover's Theorem states:

*Given a set of points that are not linearly separable in an input space, there exists a non-linear mapping to a higher-dimensional feature space in which the points are linearly separable with high probability.*

Formally, let $\mathcal{X}$ be a set of points in the input space $\mathbb{R}^n$ that are not linearly separable. There exists a transformation $\Phi: \mathbb{R}^n \to \mathbb{R}^m$ such that the points $\Phi(\mathcal{X})$ in the higher-dimensional space $\mathbb{R}^m$ are linearly separable, where typically $m \gg n$.

### Mathematical Formalism

Consider a set of points $\{x_i\}_{i=1}^N$ in $\mathbb{R}^n$ with corresponding labels $\{y_i\}_{i=1}^N \subset \{-1, +1\}$. The points are not linearly separable in $\mathbb{R}^n$, meaning there does not exist a hyperplane $\{w \in \mathbb{R}^n, b \in \mathbb{R}\}$ such that:

$$ y_i (w \cdot x_i + b) > 0 \quad \forall i \in \{1, 2, \ldots, N\} $$

Cover's Theorem suggests that there exists a mapping $\Phi: \mathbb{R}^n \to \mathbb{R}^m$ where $m > n$, and in this new feature space, the transformed points $\{\Phi(x_i)\}_{i=1}^N$ are linearly separable. That is, there exists a hyperplane $\{w' \in \mathbb{R}^m, b' \in \mathbb{R}\}$ such that:

$$ y_i (w' \cdot \Phi(x_i) + b') > 0 \quad \forall i \in \{1, 2, \ldots, N\} $$

### Implications and Applications

Cover's Theorem underpins the rationale behind the use of kernel methods in [[Support Vector Machines]] (SVMs) and other machine learning algorithms. By applying a kernel function $k(x, z) = \langle \Phi(x), \Phi(z) \rangle$, we implicitly map the data into a higher-dimensional space without explicitly computing $\Phi(x)$. This allows SVMs to construct hyperplanes in this high-dimensional space, achieving linear separability even if the original data is not linearly separable in the input space.

### Example

An illustrative example is the XOR problem, where data points $(0,0), (1,1)$ are labeled $-1$ and $(0,1), (1,0)$ are labeled $+1$. In $\mathbb{R}^2$, these points are not linearly separable. However, by mapping them into a higher-dimensional space (e.g., $\mathbb{R}^3$) using the transformation $\Phi(x_1, x_2) = (x_1, x_2, x_1 x_2)$, the points become linearly separable.

### References

- Cover, T. M. (1965). *Geometrical and Statistical Properties of Systems of Linear Inequalities with Applications in Pattern Recognition*. IEEE Transactions on Electronic Computers, EC-14(3), 326-334.
- [[Kernel Methods]]
- [[Support Vector Machine|Support Vector Machines]]

Cover's Theorem thus provides a theoretical foundation for many modern machine learning techniques, demonstrating the power of higher-dimensional feature spaces in simplifying complex classification problems.


---


following: [[representer theorems]]

See also: [[information theory]]