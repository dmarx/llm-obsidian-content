---
date: 1715896124.96914
tags:
- autogenerated
- source__oai
- conversation_id__8aad5cc7-f090-4cfa-bb5c-127745791af3
- msg_aaa21173-42ba-41d0-95f4-02d20e9e6d73
- msg_cc8fa924-7dab-43de-a4c1-c5beb25bc902
- colorclass/vector-space
title: orthogonal projection
---

**USER**

Orthogonal Projection


**ASSISTANT**

**Orthogonal projection** is a specific type of projection in linear algebra and functional analysis that maps a vector onto a subspace such that the error vector (the difference between the original vector and the projected vector) is orthogonal to the subspace. This concept is widely used in various fields, including statistics, signal processing, and computer graphics.

### Definition

Given a vector space \(V\) and a subspace \(U \subseteq V\), the orthogonal projection \(P_U\) of a vector \(\mathbf{v} \in V\) onto \(U\) is a vector \(P_U \mathbf{v} \in U\) such that the difference \(\mathbf{v} - P_U \mathbf{v}\) is orthogonal to \(U\). Formally,
$$
\mathbf{v} - P_U \mathbf{v} \in U^\perp,
```
where \(U^\perp\) is the orthogonal complement of \(U\).

### Properties

1. **Idempotent**: \(P_U^2 = P_U\).
2. **Self-Adjoint**: \(P_U = P_U^\dagger\), where \(P_U^\dagger\) is the adjoint (transpose in the real case) of \(P_U\).
3. **Linear**: \(P_U(\alpha \mathbf{v} + \beta \mathbf{w}) = \alpha P_U \mathbf{v} + \beta P_U \mathbf{w}\) for all \(\alpha, \beta \in \mathbb{R}\) (or \(\mathbb{C}\)) and \(\mathbf{v}, \mathbf{w} \in V\).

### Projection onto a Subspace

Let \(U\) be a subspace of \(V\) spanned by the orthonormal basis \(\{\mathbf{u}_1, \mathbf{u}_2, \ldots, \mathbf{u}_k\}\). The orthogonal projection \(P_U\) of a vector \(\mathbf{v} \in V\) onto \(U\) is given by:
$$
P_U \mathbf{v} = \sum_{i=1}^k \langle \mathbf{v}, \mathbf{u}_i \rangle \mathbf{u}_i,
$$
where \(\langle \mathbf{v}, \mathbf{u}_i \rangle\) denotes the inner product of \(\mathbf{v}\) and \(\mathbf{u}_i\).

### Projection Matrix

For a finite-dimensional vector space, if \(U\) is spanned by the columns of a matrix \(A \in \mathbb{R}^{n \times k}\) with orthonormal columns, the projection matrix \(P\) onto \(U\) is:
$$
P = A A^T.
```

If \(A\) does not have orthonormal columns but still spans \(U\), the projection matrix is:
$$
P = A (A^T A)^{-1} A^T.
```
This formula assumes that \(A\) has full column rank, ensuring \(A^T A\) is invertible.

### Example: Projection in \(\mathbb{R}^3\)

Consider projecting a vector \(\mathbf{v} = \begin{pmatrix} v_1 \\ v_2 \\ v_3 \end{pmatrix}\) onto the plane spanned by two orthonormal vectors \(\mathbf{u}_1 = \begin{pmatrix} 1 \\ 0 \\ 0 \end{pmatrix}\) and \(\mathbf{u}_2 = \begin{pmatrix} 0 \\ 1 \\ 0 \end{pmatrix}\).

The orthogonal projection \(P_U \mathbf{v}\) is:
$$
P_U \mathbf{v} = \langle \mathbf{v}, \mathbf{u}_1 \rangle \mathbf{u}_1 + \langle \mathbf{v}, \mathbf{u}_2 \rangle \mathbf{u}_2 = v_1 \begin{pmatrix} 1 \\ 0 \\ 0 \end{pmatrix} + v_2 \begin{pmatrix} 0 \\ 1 \\ 0 \end{pmatrix} = \begin{pmatrix} v_1 \\ v_2 \\ 0 \end{pmatrix}.
```

The corresponding projection matrix is:
$$
P = \mathbf{u}_1 \mathbf{u}_1^T + \mathbf{u}_2 \mathbf{u}_2^T = \begin{pmatrix} 1 & 0 & 0 \\ 0 & 1 & 0 \\ 0 & 0 & 0 \end{pmatrix}.
```

### Applications

1. **Least Squares Approximation**: In statistics and data fitting, the least squares solution to an overdetermined system of linear equations involves projecting the data onto the column space of the design matrix.
2. **Principal Component Analysis (PCA)**: PCA involves projecting data onto the subspace spanned by the principal components, which are the eigenvectors of the covariance matrix.
3. **Signal Processing**: In filtering, signals are projected onto subspaces representing desired signal components while eliminating noise.
4. **Computer Graphics**: Orthogonal projections are used to project 3D objects onto 2D planes for rendering images.

### Orthogonal Projection in Functional Analysis

In the context of Hilbert spaces (infinite-dimensional vector spaces), orthogonal projections are used to project functions onto subspaces. For example, in \(L^2\) space, which consists of square-integrable functions, the projection operator \(P\) onto a closed subspace \(U\) is defined similarly to the finite-dimensional case but involves integrals instead of sums.

### Conclusion

Orthogonal projection is a powerful tool in linear algebra and functional analysis, providing a way to reduce the dimensionality of a problem and simplify the analysis by focusing on the most relevant components. Understanding orthogonal projections and their properties is crucial for applications in statistics, signal processing, computer graphics, and more. For further study, refer to [[Linear Algebra]], [[Functional Analysis]], and [[Principal Component Analysis]].


---

preceding: [[projection operator]]  


following: [[embedding]]

See also: [[signal processing]]


See also: [[orthonormal basis]]


See also: [[vector space]]


See also: [[martingale representation theorem]]


See also: [[geometric interpretation of the trace]]


See also: [[skip to main content]]


See also: [[adjointness]]


See also: [[projection operators]]


See also: [[theoretical framework of clifford group equivariant neural networks]]


See also: [[dually flat structure of binary choice models]]


See also: [[geometric interpretation of the hat matrix]]


See also: [[spectral measures]]