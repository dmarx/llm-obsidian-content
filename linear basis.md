---
date: 1721509498.286341
tags:
- autogenerated
- source__oai
- conversation_id__98a4f107-a2cf-40c9-b0d2-45689415fcb9
- msg_aaa2d185-dcb0-4e28-94a4-b43c5d58fd80
- msg_9248b268-24c6-48bc-9425-b3ffc25eaaa2
- colorclass/vector-space
title: linear basis
---

**USER**

linear basis


**ASSISTANT**

A **linear basis** (or simply **basis**) is a fundamental concept in linear algebra and is used to describe vector spaces. 

### Definition

A **basis** of a vector space $V$ over a field $\mathbb{F}$ is a set of vectors $\{v_1, v_2, \ldots, v_n\}$ such that:

1. **Linearly Independent**: The vectors in the set are linearly independent. This means that if a linear combination of these vectors equals the zero vector, then all coefficients in the combination must be zero. Formally, if 
   $$ a_1 v_1 + a_2 v_2 + \cdots + a_n v_n = 0, $$
   then $a_1 = a_2 = \cdots = a_n = 0$.

2. **Spanning Set**: The set spans the vector space $V$. This means that any vector in $V$ can be expressed as a linear combination of the vectors in the set. Formally, for any vector $v \in V$, there exist scalars $b_1, b_2, \ldots, b_n \in \mathbb{F}$ such that 
   $$ v = b_1 v_1 + b_2 v_2 + \cdots + b_n v_n. $$

### Properties

- **Uniqueness of Representation**: Given a basis $\{v_1, v_2, \ldots, v_n\}$ of a vector space $V$, each vector in $V$ can be uniquely written as a linear combination of the basis vectors. This implies that the coordinates of any vector relative to the basis are unique.
  
- **Cardinality**: The number of vectors in the basis of a vector space $V$ is called the **dimension** of $V$, denoted $\dim(V)$. All bases of a vector space have the same number of elements, which is an important and non-trivial result.

### Examples

1. **Standard Basis in $\mathbb{R}^n$**: In the vector space $\mathbb{R}^n$, the standard basis is $\{e_1, e_2, \ldots, e_n\}$ where $e_i$ is the vector with 1 in the $i$-th position and 0 elsewhere. For instance, in $\mathbb{R}^3$, the standard basis is:
   $$ e_1 = \begin{pmatrix} 1 \\ 0 \\ 0 \end{pmatrix}, \quad e_2 = \begin{pmatrix} 0 \\ 1 \\ 0 \end{pmatrix}, \quad e_3 = \begin{pmatrix} 0 \\ 0 \\ 1 \end{pmatrix}. $$

2. **Polynomial Space**: Consider the vector space $P_n$ of all polynomials of degree at most $n$ over a field $\mathbb{F}$. A standard basis for this space is $\{1, x, x^2, \ldots, x^n\}$. Each polynomial $p(x) \in P_n$ can be uniquely expressed as a linear combination of these basis elements.

### Formalization

Let $V$ be a vector space over a field $\mathbb{F}$, and let $\{v_1, v_2, \ldots, v_n\}$ be a basis of $V$. Then:

- **Linear Independence**:
  $$ \sum_{i=1}^n a_i v_i = 0 \implies a_1 = a_2 = \cdots = a_n = 0. $$

- **Spanning**:
  $$ \forall v \in V, \exists b_1, b_2, \ldots, b_n \in \mathbb{F} \text{ such that } v = \sum_{i=1}^n b_i v_i. $$

These properties ensure that the basis provides a framework for understanding the structure of the vector space. The concept of a basis is also foundational for further exploration in areas such as eigenvalues and eigenvectors, inner product spaces, and more advanced topics like [[Hilbert Space]] and [[Vector Space|linear transformations]].


---


following: [[develop from linear basis towards fourier theory]]

See also: [[develop from linear basis towards fourier theory]]


See also: [[vector space]]