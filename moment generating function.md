---
date: 1716341961.521218
tags:
- autogenerated
- source__oai
- conversation_id__4699c241-4d00-416b-ab11-cc2536c48a20
- msg_aaa2c1cf-22fc-4c05-a731-32c3e268b94a
- msg_70b06023-3b78-4e86-9d0e-d3b80860f593
- colorclass/information-theory
title: moment generating function
---

**USER**

moment generating function


**ASSISTANT**

The moment generating function (MGF) is an important tool in probability theory and statistics, used to uniquely characterize the distribution of a random variable. It provides a convenient way to derive moments (e.g., mean, variance) of the distribution and can simplify the analysis of sums of independent random variables.

## Definition

The moment generating function of a random variable \( X \) is defined as:

$$
M_X(t) = \mathbb{E}[e^{tX}],
$$

where \( \mathbb{E} \) denotes the expectation operator, and \( t \) is a real number in the domain where this expectation exists.

## Properties

### Existence

The MGF \( M_X(t) \) exists for all real numbers \( t \) such that the expectation \( \mathbb{E}[e^{tX}] \) is finite. There are distributions for which the MGF does not exist for all values of \( t \).

### Uniqueness

The MGF uniquely determines the distribution of \( X \). If two random variables have the same MGF for all \( t \) in an interval around 0, they have the same distribution.

### Relation to Moments

The \( n \)-th moment of \( X \) can be obtained by differentiating the MGF \( n \) times with respect to \( t \) and then evaluating at \( t = 0 \):

$$
\mathbb{E}[X^n] = M_X^{(n)}(0),
$$

where \( M_X^{(n)}(t) \) denotes the \( n \)-th derivative of \( M_X(t) \).

### Cumulants

The cumulant generating function (CGF) is the logarithm of the MGF:

$$
K_X(t) = \log M_X(t).
$$

The \( n \)-th cumulant of \( X \) is obtained by differentiating the CGF \( n \) times and evaluating at \( t = 0 \):

$$
\kappa_n = K_X^{(n)}(0).
$$

### Sum of Independent Random Variables

If \( X \) and \( Y \) are independent random variables, the MGF of their sum \( X + Y \) is the product of their MGFs:

$$
M_{X+Y}(t) = M_X(t) M_Y(t).
$$

This property is useful for deriving the distribution of the sum of independent random variables.

## Examples

### Normal Distribution

Let \( X \) be a normally distributed random variable with mean \( \mu \) and variance \( \sigma^2 \). The MGF of \( X \) is:

$$
M_X(t) = \mathbb{E}[e^{tX}] = e^{\mu t + \frac{1}{2} \sigma^2 t^2}.
$$

### Exponential Distribution

Let \( X \) be an exponentially distributed random variable with rate parameter \( \lambda \). The MGF of \( X \) is:

$$
M_X(t) = \mathbb{E}[e^{tX}] = \frac{\lambda}{\lambda - t}, \quad t < \lambda.
$$

### Bernoulli Distribution

Let \( X \) be a Bernoulli random variable with parameter \( p \) (i.e., \( X \) takes the value 1 with probability \( p \) and 0 with probability \( 1-p \)). The MGF of \( X \) is:

$$
M_X(t) = \mathbb{E}[e^{tX}] = (1-p) + p e^t.
$$

### Poisson Distribution

Let \( X \) be a Poisson random variable with parameter \( \lambda \). The MGF of \( X \) is:

$$
M_X(t) = \mathbb{E}[e^{tX}] = e^{\lambda (e^t - 1)}.
$$

## Applications

### Deriving Moments

To find the moments of a random variable \( X \), we use the MGF. For example, if \( M_X(t) \) is the MGF of \( X \), the first moment (mean) is:

$$
\mathbb{E}[X] = M_X'(0),
$$

and the second moment is:

$$
\mathbb{E}[X^2] = M_X''(0).
$$

### Central Limit Theorem

The MGF can be used in the proof of the Central Limit Theorem (CLT). The CLT states that the sum of a large number of independent and identically distributed random variables, appropriately normalized, converges in distribution to a normal distribution.

### Sum of Independent Random Variables

The MGF provides a straightforward way to find the distribution of the sum of independent random variables, as the MGF of the sum is the product of their MGFs.

## References for Further Reading

- Ross, Sheldon M. "A First Course in Probability." Pearson, 2019.
- Grimmett, Geoffrey R., and David R. Stirzaker. "Probability and Random Processes." Oxford University Press, 2001.
- Casella, George, and Roger L. Berger. "Statistical Inference." Duxbury Press, 2002.

These references provide comprehensive introductions and in-depth analyses of moment generating functions and their applications in probability and statistics.


---

preceding: [[generating function]]  


following: [[probability generating function]]

See also: [[cramÃ©r's theorem]]


See also: [[chernoff's inequality]]


See also: [[bernstein's inequality]]


See also: [[probability theory]]


See also: [[independent and identically distributed]]


See also: [[large deviations theory]]


See also: [[large deviations in statistical physics]]


See also: [[hoeffding's inequality]]


See also: [[generating function]]


See also: [[random variables]]


See also: [[statistical moments]]