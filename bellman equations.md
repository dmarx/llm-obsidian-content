---
date: 1717810288.188157
tags:
- autogenerated
- source__oai
- conversation_id__77e70e10-f02a-4f98-bd5c-6217b10122cf
- msg_aaa29948-68a4-4806-8931-70b261405491
- msg_aa09eeb3-2f60-4e7b-a76e-5f98949ae8e6
- colorclass/system-dynamics
title: bellman equations
---

**USER**

bellman equations


**ASSISTANT**

The Bellman equation, named after Richard Bellman, is a fundamental concept in dynamic programming and [[optimal control theory]]. It provides a recursive decomposition of the value function, which represents the optimal cost-to-go from any given state. The Bellman equation allows solving complex optimization problems by breaking them down into simpler subproblems.

### Bellman Equation for Discrete-Time Systems

#### Finite Horizon

For a discrete-time, finite-horizon optimal control problem, the Bellman equation can be written as:

\[ V_k(\mathbf{x}_k) = \min_{\mathbf{u}_k} \left[ L(\mathbf{x}_k, \mathbf{u}_k) + V_{k+1}(\mathbf{f}(\mathbf{x}_k, \mathbf{u}_k)) \right], \]
where:
- \( V_k(\mathbf{x}_k) \) is the value function at time step \( k \) and state \( \mathbf{x}_k \).
- \( \mathbf{u}_k \) is the control action at time step \( k \).
- \( L(\mathbf{x}_k, \mathbf{u}_k) \) is the immediate cost (running cost).
- \( \mathbf{f}(\mathbf{x}_k, \mathbf{u}_k) \) represents the system dynamics, providing the next state \( \mathbf{x}_{k+1} = \mathbf{f}(\mathbf{x}_k, \mathbf{u}_k) \).

The value function at the final time step \( N \) is typically given by the terminal cost:

\[ V_N(\mathbf{x}_N) = \phi(\mathbf{x}_N). \]

#### Infinite Horizon

For an infinite-horizon problem, the Bellman equation simplifies to:

\[ V(\mathbf{x}) = \min_{\mathbf{u}} \left[ L(\mathbf{x}, \mathbf{u}) + \beta V(\mathbf{f}(\mathbf{x}, \mathbf{u})) \right], \]
where:
- \( \beta \) is a discount factor (with \( 0 \leq \beta \leq 1 \)).
- \( V(\mathbf{x}) \) is the value function representing the long-term cost starting from state \( \mathbf{x} \).

### Bellman Equation for Continuous-Time Systems

The continuous-time counterpart of the Bellman equation is the Hamilton-Jacobi-Bellman (HJB) equation. For a continuous-time optimal control problem, the HJB equation is given by:

\[ \frac{\partial V(\mathbf{x}, t)}{\partial t} + \min_{\mathbf{u}} \left[ L(\mathbf{x}, \mathbf{u}) + \nabla V(\mathbf{x}, t) \cdot \mathbf{f}(\mathbf{x}, \mathbf{u}) \right] = 0, \]
where:
- \( V(\mathbf{x}, t) \) is the value function at state \( \mathbf{x} \) and time \( t \).
- \( \nabla V(\mathbf{x}, t) \) is the gradient of the value function with respect to the state.
- \( \mathbf{f}(\mathbf{x}, \mathbf{u}) \) represents the system dynamics.
- \( L(\mathbf{x}, \mathbf{u}) \) is the running cost.

The boundary condition is given by the terminal cost:

\[ V(\mathbf{x}, t_f) = \phi(\mathbf{x}). \]

### Example: Discrete-Time Finite Horizon

Consider a simple discrete-time finite-horizon problem:

1. **System Dynamics**:
   \[ \mathbf{x}_{k+1} = A \mathbf{x}_k + B \mathbf{u}_k, \]
   where \( \mathbf{x}_k \) is the state vector and \( \mathbf{u}_k \) is the control vector.

2. **Cost Function**:
   \[ J = \sum_{k=0}^{N-1} \left( \mathbf{x}_k^T Q \mathbf{x}_k + \mathbf{u}_k^T R \mathbf{u}_k \right) + \mathbf{x}_N^T Q_f \mathbf{x}_N, \]
   where \( Q, R, Q_f \) are weighting matrices.

3. **Bellman Equation**:
   \[ V_k(\mathbf{x}_k) = \min_{\mathbf{u}_k} \left[ \mathbf{x}_k^T Q \mathbf{x}_k + \mathbf{u}_k^T R \mathbf{u}_k + V_{k+1}(A \mathbf{x}_k + B \mathbf{u}_k) \right], \]
   with terminal condition:
   \[ V_N(\mathbf{x}_N) = \mathbf{x}_N^T Q_f \mathbf{x}_N. \]

### Example: Continuous-Time Infinite Horizon (LQR)

Consider a continuous-time Linear Quadratic Regulator (LQR) problem with:

1. **System Dynamics**:
   \[ \dot{\mathbf{x}}(t) = A \mathbf{x}(t) + B \mathbf{u}(t). \]

2. **Cost Function**:
   \[ J = \int_{0}^{\infty} \left( \mathbf{x}(t)^T Q \mathbf{x}(t) + \mathbf{u}(t)^T R \mathbf{u}(t) \right) dt, \]
   where \( Q \) and \( R \) are weighting matrices.

3. **HJB Equation**:
   The value function \( V(\mathbf{x}) = \mathbf{x}^T P \mathbf{x} \) satisfies the HJB equation:
   \[ 0 = \min_{\mathbf{u}} \left[ \mathbf{x}^T Q \mathbf{x} + \mathbf{u}^T R \mathbf{u} + \nabla V(\mathbf{x}) \cdot (A \mathbf{x} + B \mathbf{u}) \right]. \]

   Substituting \( V(\mathbf{x}) = \mathbf{x}^T P \mathbf{x} \) into the HJB equation and solving for \( P \), we get the Algebraic Riccati Equation (ARE):
   \[ A^T P + P A - P B R^{-1} B^T P + Q = 0. \]

The optimal control law is then:
\[ \mathbf{u}^*(t) = -R^{-1} B^T P \mathbf{x}(t). \]

### Summary

The Bellman equation provides a recursive method to solve optimal control problems by breaking them down into simpler subproblems. For discrete-time systems, it forms the basis of dynamic programming, while for continuous-time systems, it leads to the Hamilton-Jacobi-Bellman equation. These equations are essential tools in [[optimal control theory]], enabling the design of optimal policies for a wide range of dynamical systems.


---

preceding: [[algebraic riccati equation]]  


following: [[stochastic [[optimal control theory]]]]

See also: [[hamilton–jacobi–bellman equations]]


See also: [[q-learning in reinforcement learning]]


See also: [[verification theorem]]


See also: [[control theory]]


See also: [[bellman equations in reinforcement learning]]


See also: [[dynamic programming]]


See also: [[temporal difference learning]]


See also: [[system dynamics]]