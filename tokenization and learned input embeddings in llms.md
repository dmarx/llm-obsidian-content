---
date: 1726449854.552891
tags:
- autogenerated
- source__oai
- conversation_id__66e757d1-0534-8008-bb6a-afa8342b5ee2
- msg_df36fa05-4dde-473b-a148-8d5b090882f4
- msg_4bf2dd34-ff41-499b-9510-5c39d3bc2a85
- msg_aaa2b61b-1525-43b7-b962-591e85422a06
- colorclass/vector-space
title: tokenization and learned input embeddings in llms
---

**ASSISTANT**

### **Tokenization and Learned Input Embeddings in LLMs**

In **Large Language Models (LLMs)**, **tokenization** and **learned input embeddings** are fundamental components that bridge raw text input and the internal representation used by the model for processing and generation. Tokenization converts raw text into discrete units (tokens), while input embeddings map these tokens into high-dimensional vectors that capture semantic meaning. This module explores the mechanisms behind tokenization, learned input embeddings, and their importance in LLM architectures like **BERT**, **GPT**, and **Transformer-based models**.

---

### **1. Tokenization: Converting Text into Tokens**

**Tokenization** is the process of splitting text into smaller units called **tokens**. These tokens can represent individual characters, subwords, words, or phrases, depending on the chosen tokenization strategy. Tokenization is necessary because LLMs cannot directly process raw text—they require numerical representations of text, and tokens serve as the bridge between text and embeddings.

#### **Types of Tokenization**:

1. **Word-level Tokenization**:
   - Splits text into words.
   - Example:
     - Input: "Language models are powerful."
     - Tokens: `["Language", "models", "are", "powerful", "."]`
   - **Challenges**: Rare words and vocabulary size. Words not in the vocabulary (out-of-vocabulary words) cannot be represented easily.

2. **Character-level Tokenization**:
   - Splits text into individual characters.
   - Example:
     - Input: "Language"
     - Tokens: `["L", "a", "n", "g", "u", "a", "g", "e"]`
   - **Challenges**: Inefficient for longer texts, as token sequences become very long, making training slower and less efficient.

3. **Subword-level Tokenization (BPE, WordPiece, SentencePiece)**:
   - Splits text into **subwords** that capture frequent word parts. Subword tokenization balances between the inefficiencies of character tokenization and the rarity issues of word tokenization.
   - Example:
     - Input: "unhappiness"
     - Tokens: `["un", "happ", "iness"]`
   - Common algorithms:
     - **Byte Pair Encoding (BPE)**: Iteratively merges frequent byte pairs.
     - **WordPiece**: Used in models like **BERT**, which iteratively builds subwords based on maximizing likelihood.
     - **SentencePiece**: A variation often used in **T5**, which encodes text into subwords.

#### **Why Subword Tokenization is Popular**:
- **Efficient Representation**: Handles rare and frequent words efficiently, with a compact vocabulary size.
- **No Out-of-Vocabulary Words**: Since any word can be decomposed into subwords or characters, subword tokenization ensures that the model can represent every possible input.

#### **Example of Byte Pair Encoding (BPE)**:
- Start with: `["un", "happ", "iness"]`
- The most frequent pair of bytes is `"un" + "happ"`, which gets merged into a single token: `["unhapp", "iness"]`.
- Continue this merging process until a set vocabulary size is reached.

---

### **2. Learned Input Embeddings**

Once the text is tokenized, each token is mapped to a **learned input embedding**—a dense, high-dimensional vector that captures semantic and syntactic information about the token. These embeddings form the foundation of the model's understanding of the input.

#### **What are Input Embeddings?**

Input embeddings are **numerical vector representations** of tokens, learned during the model’s training. They convert discrete tokens into continuous-valued vectors that LLMs can process. These vectors are typically of size **d**, where **d** is the dimensionality of the embedding space (e.g., 768, 1024).

#### **Role of Input Embeddings**:
- **Mapping from Token Space to Vector Space**: Embeddings map tokenized inputs into a dense vector space that captures relationships between tokens (e.g., similar tokens have similar embeddings).
- **Semantic and Syntactic Information**: Input embeddings encapsulate the semantic meaning of words (e.g., "king" and "queen" may have similar embeddings) and syntactic properties (e.g., verbs vs. nouns).
- **Position in Transformer Models**: In **Transformer** models like **GPT** and **BERT**, the input embeddings are the first step in the pipeline, before attention mechanisms are applied.

#### **How Embeddings are Learned**:
During model training, input embeddings are learned as parameters of the model. Each token in the vocabulary is associated with a vector that is updated through **backpropagation** as the model is trained on large amounts of text.

- **Training Objective**: The embeddings are trained alongside the rest of the model (e.g., attention layers) using objectives like **language modeling** (predicting the next token given a sequence) or **masked language modeling** (predicting masked tokens in a sequence).
- **Embedding Matrix**: In models like **GPT-3** or **BERT**, embeddings are stored in an **embedding matrix** $E$, where each row corresponds to a token and its associated vector:
  
  $$
  E \in \mathbb{R}^{V \times d}
  $$
  Where:
  - $V$ is the size of the vocabulary (e.g., 50,000 tokens).
  - $d$ is the embedding dimension (e.g., 768 in BERT).

  Each token **T** is mapped to a vector by looking up its row in the embedding matrix.

---

### **3. Positional Encodings in Transformer Models**

While learned input embeddings capture token meanings, Transformer models like **GPT** and **BERT** are **order-agnostic**, meaning they don’t inherently know the position of tokens in a sequence. To account for word order, **positional encodings** are added to input embeddings, ensuring that the model can differentiate between tokens based on their position in the sequence.

#### **Why Positional Encodings are Needed**:
- **Transformers Lack Sequential Bias**: Unlike RNNs, which inherently process sequences in order, Transformers process all tokens in parallel. Without positional encodings, Transformers would lose information about the position of tokens in a sentence.
  
#### **Types of Positional Encodings**:
1. **Learned Positional Encodings**: The model learns a separate vector for each position in the input sequence during training.
2. **Sinusoidal Positional Encodings**: These are deterministic encodings based on sine and cosine functions of different frequencies. The idea is to have a smooth encoding that captures relative positions.

**Example** (Sinusoidal encoding):
  
For a position $pos$ and a dimension $i$, the positional encoding is given by:

$$
PE_{(pos, 2i)} = \sin\left(\frac{pos}{10000^{2i/d}}\right)
$$
$$
PE_{(pos, 2i+1)} = \cos\left(\frac{pos}{10000^{2i/d}}\right)
$$

Where $d$ is the embedding dimension.

- **Addition to Input Embeddings**: The positional encoding is added directly to the learned input embeddings, so the input to the Transformer is the sum of the token embedding and its positional encoding:
  
  $$
  \text{Input to Transformer} = \text{Token Embedding} + \text{Positional Encoding}
  $$

---

### **4. Tokenization and Input Embeddings in Common LLMs**

Different LLM architectures handle tokenization and embeddings in unique ways based on their tasks and design:

#### **BERT (Bidirectional Encoder Representations from Transformers)**:
- **Tokenization**: BERT uses **WordPiece tokenization**, which splits words into subwords to ensure a manageable vocabulary size.
  - Example: "unhappiness" becomes `["un", "##happiness"]`.
- **Input Embeddings**:
  - BERT learns **token embeddings** and **positional embeddings**, which are summed to form the input to the Transformer encoder.
  - BERT also adds **segment embeddings** for tasks requiring pairs of sentences (e.g., question answering), where tokens from different segments are distinguished by different segment embeddings.

#### **GPT (Generative Pre-trained Transformer)**:
- **Tokenization**: GPT uses **Byte Pair Encoding (BPE)**, a subword tokenization technique.
  - Example: "language" might be tokenized as `["lan", "gu", "age"]`.
- **Input Embeddings**:
  - GPT learns token embeddings and positional embeddings for each token in a sequence.
  - The sum of token and positional embeddings is passed to the Transformer decoder layers for generating text.

#### **T5 (Text-to-Text Transfer Transformer)**:
- **Tokenization**: T5 uses **SentencePiece tokenization**, another subword-based approach that works on both word and character levels.
  - Example: "tokenization" might be tokenized as `["tok", "en", "ization"]`.
- **Input Embeddings**:
  - T5 uses learned token embeddings but does not rely on traditional positional encodings. Instead, T5's architecture handles position awareness differently within the Transformer layers.

---

### **5. Challenges and Considerations**

#### **A. Vocabulary Size Trade-offs**:
The choice of vocabulary size affects performance and efficiency:
- **Larger Vocabularies**: Reduce the length of tokenized sequences but increase the size of the embedding matrix, leading to larger models and slower training.
- **Smaller Vocabularies**: Result in longer sequences (since more tokens are needed to represent the same text),


**ASSISTANT**

which increases the complexity of sequence processing in Transformers.

#### **B. Out-of-Vocabulary (OOV) Issues**:
- **Subword tokenization** mitigates OOV issues by breaking down unknown words into smaller, known units. However, rare or domain-specific subwords might still be difficult for the model to represent accurately.

#### **C. Context Length Limitations**:
- The length of tokenized sequences is constrained by the model's architecture (e.g., GPT-3 can handle sequences up to 2048 tokens). When a sequence exceeds this limit, some models truncate the input, potentially losing important context.

---

### **Conclusion**

**Tokenization** and **learned input embeddings** are essential components in LLMs, enabling the transformation of raw text into representations that models can process. Tokenization strategies, particularly subword methods like **BPE**, ensure that models handle a wide variety of text inputs efficiently, while learned embeddings capture the rich semantic and syntactic meanings of tokens. These embeddings, along with positional encodings, provide the foundation for LLMs like **BERT**, **GPT**, and **T5** to excel in various NLP tasks, from language generation to understanding. 

By carefully selecting tokenization strategies and learning robust embeddings, LLMs are able to handle diverse, complex inputs with a high degree of flexibility and accuracy.


**USER**

task-specific tokens, prefix tuning


---

preceding: [[semantic similarity, semantic search, and the role of vector databases and knowledge stores in [[rag]] systems]]  


following: [[task-specific tokens and prefix tuning]]

See also: [[vector space]]


See also: [[embedding]]


See also: [[positional embeddings]]