---
date: '2025-05-11T06:36:17.264049Z'
tags:
- autogenerated
- source__claude
- conversation_id__cb963dc1-461f-4347-8c25-05347bee69a6
- msg_e4e2172b-772b-48ce-9766-ba5be7de1189
- msg_2d542556-ebe1-4254-91ff-c8501fb16421
- colorclass/information-theory
title: hamiltonian neural networks
---

**USER**

Hamiltonian Neural Networks


**ASSISTANT**

# Hamiltonian Neural Networks

Hamiltonian Neural Networks (HNNs) represent a specialized class of neural architectures that incorporate the principles of [[Hamiltonian mechanics]] into deep learning frameworks. They explicitly model the dynamics of physical systems while respecting fundamental conservation laws, particularly energy conservation.

## Theoretical Foundation

### Hamiltonian Mechanics

Hamiltonian mechanics provides a reformulation of classical mechanics through the lens of energy rather than forces. The state of a system is represented by generalized coordinates $q \in \mathbb{R}^d$ and their conjugate momenta $p \in \mathbb{R}^d$, collectively forming the phase space $z = (q, p) \in \mathbb{R}^{2d}$.

The system dynamics are governed by Hamilton's equations:

$$\begin{align}
\frac{dq}{dt} &= \frac{\partial H(q, p)}{\partial p} \\
\frac{dp}{dt} &= -\frac{\partial H(q, p)}{\partial q}
\end{align}$$

where $H(q, p)$ is the Hamiltonian function, typically representing the total energy of the system.

These equations can be expressed more compactly using the [[symplectic gradient]] $\nabla_s$ and the [[symplectic matrix]] $J$:

$$\frac{dz}{dt} = J \nabla H(z)$$

where $J = \begin{bmatrix} 0 & I_d \\ -I_d & 0 \end{bmatrix}$ and $I_d$ is the $d \times d$ identity matrix.

### Conservation Properties

A key property of Hamiltonian systems is that they conserve the Hamiltonian $H$ along trajectories:

$$\frac{dH}{dt} = \nabla H(z)^T \frac{dz}{dt} = \nabla H(z)^T J \nabla H(z) = 0$$

This equality holds because $J$ is antisymmetric $(J^T = -J)$, ensuring energy conservation in physical systems.

Additionally, Hamiltonian flows preserve the [[symplectic form]], leading to [[Liouville's theorem]] which states that phase space volume is conserved under Hamiltonian dynamics.

## Mathematical Formulation of HNNs

Hamiltonian Neural Networks learn to approximate the Hamiltonian function $H(q, p)$ using a neural network $H_\theta(q, p)$ parameterized by $\theta$. The dynamics are then modeled using Hamilton's equations:

$$\begin{align}
\frac{dq}{dt} &= \frac{\partial H_\theta(q, p)}{\partial p} \\
\frac{dp}{dt} &= -\frac{\partial H_\theta(q, p)}{\partial q}
\end{align}$$

The gradients $\frac{\partial H_\theta}{\partial q}$ and $\frac{\partial H_\theta}{\partial p}$ are computed using [[automatic differentiation]].

### Loss Function

The HNN is trained to predict state derivatives that match observed or simulated system dynamics. Given a dataset of phase space points and their derivatives $\{(z_i, \dot{z}_i)\}_{i=1}^N$, the loss function is typically defined as:

$$\mathcal{L}(\theta) = \frac{1}{N} \sum_{i=1}^N \|J\nabla H_\theta(z_i) - \dot{z}_i\|_2^2$$

This loss encourages the HNN to learn a Hamiltonian whose symplectic gradient accurately predicts the observed dynamics.

### Energy Supervision

An alternative training approach involves directly supervising the Hamiltonian using energy labels $\{(z_i, H_i)\}_{i=1}^N$:

$$\mathcal{L}_{\text{energy}}(\theta) = \frac{1}{N} \sum_{i=1}^N \|H_\theta(z_i) - H_i\|_2^2$$

This is particularly useful when energy measurements are available or when the system's analytical Hamiltonian is known.

## Architectural Considerations

### Neural Network Architecture

The choice of neural architecture for $H_\theta$ should respect the properties of Hamiltonian systems:

1. **Separable Hamiltonians**: For systems with separable Hamiltonians $H(q, p) = T(p) + V(q)$ (kinetic plus potential energy), the architecture can reflect this structure:
   
   $$H_\theta(q, p) = T_{\theta_1}(p) + V_{\theta_2}(q)$$

2. **Symmetry-Preserving Networks**: For systems with symmetries (e.g., rotational or translational invariance), architectures like [[Equivariant Neural Networks]] can be employed.

3. **Antisymmetric Regularization**: To enhance stability, the learned dynamics can be regularized to be antisymmetric:
   
   $$\mathcal{L}_{\text{antisym}}(\theta) = \|\nabla_z \dot{z}_\theta(z) + \nabla_z \dot{z}_\theta(z)^T\|_F^2$$

### Integration Schemes

To generate trajectories, the Hamiltonian dynamics must be integrated numerically. Standard numerical integrators often violate energy conservation. Instead, [[symplectic integrators]] are preferred:

1. **Symplectic Euler**:
   $$\begin{align}
   p_{n+1} &= p_n - \Delta t \cdot \frac{\partial H_\theta(q_n, p_n)}{\partial q} \\
   q_{n+1} &= q_n + \Delta t \cdot \frac{\partial H_\theta(q_n, p_{n+1})}{\partial p}
   \end{align}$$

2. **St√∂rmer-Verlet**:
   $$\begin{align}
   p_{n+1/2} &= p_n - \frac{\Delta t}{2} \cdot \frac{\partial H_\theta(q_n, p_{n+1/2})}{\partial q} \\
   q_{n+1} &= q_n + \Delta t \cdot \frac{\partial H_\theta(q_n, p_{n+1/2})}{\partial p} \\
   p_{n+1} &= p_{n+1/2} - \frac{\Delta t}{2} \cdot \frac{\partial H_\theta(q_{n+1}, p_{n+1/2})}{\partial q}
   \end{align}$$

These integrators preserve the symplectic structure and exhibit excellent energy conservation properties over long time periods.

## Extensions and Variants

### Symplectic Neural Networks

[[Symplectic Neural Networks]] directly parameterize the update function to be symplectic, rather than deriving it from a Hamiltonian:

$$z_{n+1} = \Phi_\theta(z_n)$$

where $\Phi_\theta$ is designed to be a symplectic transformation, often structured as a sequence of symplectic layers.

### Lagrangian Neural Networks

[[Lagrangian Neural Networks]] formulate the dynamics using the Lagrangian formalism instead of the Hamiltonian:

$$\frac{d}{dt}\left(\frac{\partial L_\theta(q, \dot{q})}{\partial \dot{q}}\right) - \frac{\partial L_\theta(q, \dot{q})}{\partial q} = 0$$

where $L_\theta(q, \dot{q})$ is the learned Lagrangian function.

### Hamiltonian Generative Networks

[[Hamiltonian Generative Networks]] combine HNNs with generative models to learn the Hamiltonian from visual observations, enabling the generation of physically plausible trajectories for complex systems.

### Port-Hamiltonian Neural Networks

[[Port-Hamiltonian Neural Networks]] extend HNNs to model systems with external inputs and dissipation:

$$\begin{align}
\frac{dz}{dt} &= [J(z) - R(z)]\nabla H_\theta(z) + g(z)u \\
y &= g(z)^T \nabla H_\theta(z)
\end{align}$$

where $R(z)$ is a positive semi-definite dissipation matrix, $g(z)$ is the input matrix, $u$ is the control input, and $y$ is the output.

## Applications

HNNs have been successfully applied to various domains:

1. **Molecular Dynamics**: Modeling energy-conserving trajectories of molecular systems.

2. **Celestial Mechanics**: Predicting long-term orbital dynamics of planetary systems.

3. **Quantum Mechanics**: Approximating quantum Hamiltonians for many-body systems.

4. **Robotics**: Modeling and controlling mechanical systems with energy-based methods.

5. **Fluid Dynamics**: Simulating incompressible flows using Hamiltonian formulations.

## Symbolic Notation for HNNs

Let's establish a consistent notation for HNNs:

- $\mathcal{Z} = \mathbb{R}^{2d}$: The phase space
- $z = (q, p) \in \mathcal{Z}$: A point in phase space
- $H_\theta: \mathcal{Z} \rightarrow \mathbb{R}$: The learned Hamiltonian function
- $\nabla_s H_\theta: \mathcal{Z} \rightarrow \mathcal{Z}$: The symplectic gradient $J\nabla H_\theta$
- $\Phi_{t_0 \rightarrow t_1}^{H_\theta}: \mathcal{Z} \rightarrow \mathcal{Z}$: The Hamiltonian flow map

The dynamics of an HNN can be expressed concisely as:

$$\frac{dz}{dt} = \nabla_s H_\theta(z)$$

And the evolution from $t_0$ to $t_1$ is given by:

$$z(t_1) = \Phi_{t_0 \rightarrow t_1}^{H_\theta}(z(t_0))$$

This notation emphasizes the geometric nature of Hamiltonian dynamics and provides a framework for analyzing HNNs as approximators of physical systems.


---

preceding: [[neural ordinary differential equations]]

See also: [[potential energy]]


See also: [[generative models]]


See also: [[system dynamics]]