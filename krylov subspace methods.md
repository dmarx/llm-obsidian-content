---
date: 1717826628.563785
tags:
- autogenerated
- source__oai
- conversation_id__ac8c3253-a669-4989-bf7c-c535d52c8140
- msg_aaa2ffec-9aaa-4629-8636-839f22fd99f6
- msg_ba5f5298-12d4-4c18-bb2a-482daa10d147
- colorclass/vector-space
title: krylov subspace methods
---

**USER**

Krylov Subspace Methods


**ASSISTANT**

[[Krylov Subspace Methods]] are a family of iterative techniques for solving linear systems and eigenvalue problems. These methods are based on constructing a sequence of subspaces, called Krylov subspaces, that approximate the solution iteratively. They are particularly effective for large, sparse matrices where direct methods are computationally infeasible.

## Overview

Krylov subspace methods operate by exploiting the structure of the matrix and the initial vector to build a series of increasingly accurate approximations to the solution. These methods are widely used in numerical linear algebra for solving systems of the form \(Ax = b\) and for finding eigenvalues and eigenvectors of matrices.

### Definition of Krylov Subspace

Given a matrix \(A \in \mathbb{R}^{n \times n}\) and a vector \(r_0 \in \mathbb{R}^n\), the Krylov subspace of dimension \(m\) is defined as:

$$ \mathcal{K}_m(A, r_0) = \text{span} \{ r_0, Ar_0, A^2r_0, \ldots, A^{m-1}r_0 \} $$

### Key Krylov Subspace Methods

1. **[[Conjugate Gradient Method]] (CG)**:
   - Designed for solving symmetric positive-definite linear systems \(Ax = b\).
   - Uses the Krylov subspace generated by the residual \(r_0 = b - Ax_0\).
   - Iteratively minimizes the quadratic form \( f(x) = \frac{1}{2} x^T A x - x^T b \).

2. **[[Lanczos Algorithm]]**:
   - Used for finding the eigenvalues and eigenvectors of symmetric matrices.
   - Constructs an orthonormal basis for the Krylov subspace and reduces the original matrix to a tridiagonal form.

3. **[[Generalized Minimal Residual Method|Generalized Minimal Residual (GMRES)]]**:
   - Applicable to non-symmetric linear systems.
   - Minimizes the residual over the Krylov subspace at each iteration.
   - Uses Arnoldi iteration to construct an orthonormal basis for the Krylov subspace.

4. **[[Bi-Conjugate Gradient Method|Bi-Conjugate Gradient (BiCG)]]**:
   - Extends the conjugate gradient method to non-symmetric systems.
   - Uses two sequences of vectors to ensure convergence.

5. **[[Arnoldi Iteration]]**:
   - Generalization of the Lanczos algorithm for non-symmetric matrices.
   - Constructs an orthonormal basis for the Krylov subspace and reduces the original matrix to an upper Hessenberg form.

### Mathematical Formulation

To illustrate the basic idea, consider the generalized minimal residual (GMRES) method for solving \(Ax = b\):

1. **Initialization**:
   - Choose an initial guess \(x_0\).
   - Compute the initial residual \(r_0 = b - Ax_0\).
   - Set \(v_1 = \frac{r_0}{\|r_0\|}\).

2. **Iteration** (for \(j = 1, 2, \ldots, m\)):
   - Compute \(w = Av_j\).
   - Orthogonalize \(w\) against \(\{v_1, v_2, \ldots, v_j\}\):
     $$ h_{ij} = v_i^T w \quad \text{for } i = 1, 2, \ldots, j $$
     $$ w = w - \sum_{i=1}^j h_{ij} v_i $$
   - Normalize:
     $$ h_{j+1,j} = \|w\| $$
     $$ v_{j+1} = \frac{w}{h_{j+1,j}} $$
   - Form the Hessenberg matrix \(H_m\) with \(h_{ij}\) elements.

3. **Solve** the least squares problem to minimize the residual:
   $$ \min \| \beta e_1 - H_m y \| $$
   where \(\beta = \|r_0\|\) and \(e_1\) is the first unit vector.

4. **Update** the solution:
   $$ x_m = x_0 + V_m y $$

### Advantages and Applications

- **Scalability**: Particularly well-suited for large-scale problems due to their iterative nature.
- **Memory Efficiency**: Require storage of only a few vectors, making them feasible for high-dimensional systems.
- **Versatility**: Can handle both symmetric and non-symmetric systems, as well as eigenvalue problems.
- **Applications**: Used in computational fluid dynamics, structural analysis, machine learning (e.g., in [[Kernel Methods]]), and many other areas of scientific computing.

### Example

Consider solving the system \(Ax = b\) using the conjugate gradient method, where:

$$ A = \begin{pmatrix} 4 & 1 \\ 1 & 3 \end{pmatrix}, \quad b = \begin{pmatrix} 1 \\ 2 \end{pmatrix}, \quad x_0 = \begin{pmatrix} 2 \\ 1 \end{pmatrix} $$

1. **Initialization**:
   - Compute \(r_0 = b - Ax_0 = \begin{pmatrix} -6 \\ -3 \end{pmatrix}\).
   - Set \(p_0 = r_0\).

2. **First Iteration**:
   - Compute \(\alpha_0 = \frac{r_0^T r_0}{p_0^T A p_0} = \frac{45}{153} = \frac{5}{17}\).
   - Update \(x_1 = x_0 + \alpha_0 p_0 = \begin{pmatrix} \frac{14}{17} \\ \frac{12}{17} \end{pmatrix}\).
   - Compute \(r_1 = r_0 - \alpha_0 A p_0 = \begin{pmatrix} 0 \\ 0 \end{pmatrix}\).

Since \(r_1 = 0\), the method converges, and the solution is \( x = \begin{pmatrix} \frac{14}{17} \\ \frac{12}{17} \end{pmatrix} \).

## Related Topics

- [[Preconditioning]]: Techniques to improve the convergence of Krylov subspace methods.
- [[Spectral Methods]]: Approaches involving the spectrum (eigenvalues) of the matrix.
- [[Multigrid Methods]]: Hierarchical techniques for solving linear systems, often combined with Krylov subspace methods for efficiency.

Krylov subspace methods are powerful and versatile tools in numerical linear algebra, enabling efficient solutions to large-scale linear systems and eigenvalue problems.


---

preceding: [[lanczos algorithm]]  


following: [[upper hessenberg form]]

See also: [[orthonormal basis]]