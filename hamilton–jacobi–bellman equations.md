---
date: 1712011057.250631
tags:
- autogenerated
- source__oai
- conversation_id__0beda6c8-c75a-44e1-b2c0-0904cd4a353a
- msg_aaa2d6dc-61c9-4514-9fd8-a12ec9d0f420
- msg_390845ed-1fef-47ae-82c2-3d928f0ab033
- colorclass/system-dynamics
title: hamilton–jacobi–bellman equations
---

**USER**

Hamilton–Jacobi–Bellman equations


**ASSISTANT**

The [[Hamilton–Jacobi–Bellman Equation|Hamilton–Jacobi–Bellman (HJB) equation]] stands as a cornerstone in the realm of optimal control theory, blending concepts from calculus of variations, partial differential equations (PDEs), and dynamic programming. Its utility spans across various domains such as finance, robotics, and any field where the optimization of dynamic systems is critical. The equation provides a necessary condition for optimality associated with differential equations constraining the dynamics of the system under consideration.

### Formal Definition

Given a control system described by the state dynamics

$$\dot{x}(t) = f(x(t), u(t), t),$$

where $x(t) \in \mathbb{R}^n$ represents the state of the system at time $t$, $u(t) \in \mathbb{R}^m$ denotes the control input, and $f: \mathbb{R}^n \times \mathbb{R}^m \times \mathbb{R} \rightarrow \mathbb{R}^n$ is a function defining the evolution of the system's state over time. The objective is to minimize (or maximize) a cost function (or performance criterion) of the form

$$J(x(t_0), u(\cdot)) = \int_{t_0}^{t_f} L(x(t), u(t), t) \, dt + g(x(t_f)),$$

where $L: \mathbb{R}^n \times \mathbb{R}^m \times \mathbb{R} \rightarrow \mathbb{R}$ is the running cost (or Lagrangian), $g: \mathbb{R}^n \rightarrow \mathbb{R}$ is the terminal cost, $t_0$ and $t_f$ are the initial and final times, respectively.

The HJB equation is derived under the principle of optimality, which asserts that any subpath of an optimal path must itself be optimal. This principle leads to a dynamic programming formulation that gives rise to the HJB equation:

$$V_t + \min_u \left\{L(x,u,t) + \nabla V \cdot f(x,u,t)\right\} = 0,$$

where $V(x,t)$ is the value function representing the minimum cost to go from state $x$ at time $t$ to the target state by time $t_f$. Here, $\nabla V$ denotes the gradient of $V$ with respect to $x$, and $V_t$ denotes the partial derivative of $V$ with respect to time $t$.

### Solving the HJB Equation

Solving the HJB equation involves finding the value function $V(x,t)$ that satisfies the equation for all $(x,t)$. In general, solving this PDE directly is challenging due to its nonlinearity and the necessity to solve it backward in time, from $t_f$ to $t_0$. However, when a solution is obtainable, it not only provides the optimal cost but also facilitates the determination of the optimal control policy through

$$u^*(x,t) = \arg\min_u \left\{L(x,u,t) + \nabla V \cdot f(x,u,t)\right\}.$$

### Applications

- **Finance**: In portfolio optimization, stochastic control problems can often be formulated in terms of HJB equations to determine optimal trading strategies under uncertainty.
- **Robotics**: Path planning and the control of robotic systems to achieve desired goals while minimizing energy consumption or time can be framed as HJB equation problems.
- **Aerospace**: Optimal control problems, including trajectory optimization for spacecraft and aircraft, are often solved using methods based on the HJB equation.

### Challenges

Despite its powerful framework for dealing with optimal control problems, the HJB equation poses significant challenges, especially for high-dimensional systems due to the "curse of dimensionality". Numerical methods and approximations, such as the use of neural networks or other machine learning techniques, have been developed to tackle these challenges, making the HJB equation more tractable for complex, real-world systems.


---


following: [[verification theorem]]

See also: [[optimal control theory]]


See also: [[bellman equations]]


See also: [[control theory]]


See also: [[dynamic programming]]


See also: [[partial differential equations]]