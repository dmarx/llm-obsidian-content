---
date: 1715982606.212689
tags:
- autogenerated
- source__oai
- conversation_id__e536633b-8eed-4822-967e-58113d1a33c2
- msg_aaa2dbf9-2604-428f-89c0-24c1530b849f
- msg_52eb106d-48de-46aa-9a74-1906497dea08
- colorclass/information-theory
title: marcenko-pasteur distribution
---

**USER**

Marcenko-Pasteur distribution


**ASSISTANT**

## Marcenko-Pasteur Distribution

The Marcenko-Pasteur distribution is a fundamental result in random matrix theory, describing the asymptotic distribution of eigenvalues of large random matrices. It is particularly relevant in the study of high-dimensional statistics, wireless communication, finance, and machine learning, where large covariance matrices are common.

### Definition

Consider a large random matrix \( \mathbf{X} \) of size \( n \times p \) with entries that are independent and identically distributed (i.i.d.) with mean zero and variance \( \sigma^2 \). The sample covariance matrix is given by:

$$
\mathbf{S} = \frac{1}{n} \mathbf{X}^\top \mathbf{X}.
$$

The Marcenko-Pasteur distribution describes the asymptotic behavior of the eigenvalues of \( \mathbf{S} \) as both \( n \) and \( p \) grow to infinity at a fixed ratio \( c = \frac{p}{n} \).

### Theoretical Formulation

The Marcenko-Pasteur law states that the empirical distribution of the eigenvalues of \( \mathbf{S} \), when properly normalized, converges to a deterministic distribution given by the density function:

$$
\rho(\lambda) = \begin{cases} 
\frac{1}{2\pi \lambda c \sigma^2} \sqrt{(\lambda_+ - \lambda)(\lambda - \lambda_-)}, & \lambda \in [\lambda_-, \lambda_+] \\
0, & \text{otherwise}
\end{cases}
$$

where the bounds \( \lambda_- \) and \( \lambda_+ \) are defined as:

$$
\lambda_{\pm} = \sigma^2 (1 \pm \sqrt{c})^2.
$$

### Key Properties

1. **Support**: The distribution is supported on the interval \([ \lambda_-, \lambda_+ ]\).
2. **Shape**: The density function has a square root singularity at both \( \lambda_- \) and \( \lambda_+ \), meaning it diverges at these endpoints.
3. **Normalization**: The total integral of the density function over the support is 1, ensuring it is a valid probability distribution.

### Applications

#### 1. High-Dimensional Statistics

In high-dimensional settings where the number of variables (features) \( p \) is comparable to or exceeds the number of observations \( n \), the Marcenko-Pasteur distribution helps in understanding the behavior of sample covariance matrices. It provides a theoretical foundation for methods that rely on eigenvalue distributions, such as Principal Component Analysis (PCA).

#### 2. Wireless Communications

In wireless communication theory, the distribution is used to analyze the capacity of multiple-input multiple-output (MIMO) systems. The eigenvalues of the channel covariance matrix follow the Marcenko-Pasteur distribution, influencing the system's capacity and performance.

#### 3. Financial Engineering

In finance, the distribution aids in the analysis of large correlation matrices of asset returns. It helps in identifying the eigenvalue spectrum's deviations, which can be attributed to market factors and systemic risks.

#### 4. Machine Learning

In machine learning, particularly in the study of deep neural networks, the Marcenko-Pasteur distribution is used to understand the spectra of weight matrices and their implications on training dynamics and generalization.

### Example

Consider a random matrix \( \mathbf{X} \) of size \( 100 \times 200 \) where entries are drawn from a standard normal distribution. The ratio \( c = \frac{p}{n} = 2 \). The eigenvalues of the sample covariance matrix \( \mathbf{S} = \frac{1}{100} \mathbf{X}^\top \mathbf{X} \) will asymptotically follow the Marcenko-Pasteur distribution with \( \sigma^2 = 1 \):

$$
\lambda_{\pm} = (1 \pm \sqrt{2})^2 = (1 \pm \sqrt{2})^2.
$$

Thus, the support of the distribution is approximately \([0.17, 5.83]\).

### Visualization

A histogram of the eigenvalues of \( \mathbf{S} \) can be compared to the theoretical Marcenko-Pasteur density function:

```python
import numpy as np
import matplotlib.pyplot as plt
from scipy.stats import norm

# Generate random matrix X
n, p = 100, 200
X = np.random.randn(n, p)

# Compute sample covariance matrix S
S = np.dot(X.T, X) / n

# Compute eigenvalues
eigenvalues = np.linalg.eigvalsh(S)

# Marcenko-Pasteur distribution parameters
c = p / n
sigma = 1
lambda_min = sigma**2 * (1 - np.sqrt(c))**2
lambda_max = sigma**2 (1 + np.sqrt(c))**2

# Define Marcenko-Pasteur density function
def marcenko_pasteur_density(lambda_, c, sigma=1):
    lambda_min = sigma**2 * (1 - np.sqrt(c))**2
    lambda_max = sigma**2 (1 + np.sqrt(c))**2
    density = np.zeros_like(lambda_)
    idx = (lambda_ >= lambda_min) & (lambda_ <= lambda_max)
    density[idx] = (1 / (2 * np.pi * c * sigma**2)) * np.sqrt((lambda_max - lambda_[idx]) * (lambda_[idx] - lambda_min))
    return density

# Plot histogram of eigenvalues and Marcenko-Pasteur density
plt.hist(eigenvalues, bins=30, density=True, alpha=0.6, color='g', label='Eigenvalues')
lambda_vals = np.linspace(lambda_min, lambda_max, 1000)
plt.plot(lambda_vals, marcenko_pasteur_density(lambda_vals, c), 'r-', lw=2, label='Marcenko-Pasteur')
plt.xlabel('Eigenvalue')
plt.ylabel('Density')
plt.legend()
plt.title('Eigenvalue Distribution vs Marcenko-Pasteur Law')
plt.show()
```

### References

1. **Marčenko, V. A., & Pastur, L. A. (1967). *Distribution of eigenvalues for some sets of random matrices*. Mathematics of the USSR-Sbornik, 1(4), 457-483.**
2. **Bai, Z. D., & Silverstein, J. W. (2010). *Spectral Analysis of Large Dimensional Random Matrices*. Springer.**
3. **Tulino, A. M., & Verdu, S. (2004). *Random Matrix Theory and Wireless Communications*. Foundations and Trends in Communications and Information Theory, 1(1), 1-182.**

The Marcenko-Pasteur distribution provides critical insights into the spectral properties of large random matrices, serving as a powerful tool in both theoretical studies and practical applications across various fields.


---

preceding: [[recent developments in theory of deep learning representations and dynamics]]  


following: [[all of generative ai implicitly invokes the bayesian paradigm.]]

See also: [[communication theory]]


See also: [[independent and identically distributed]]


See also: [[random matrix theory]]