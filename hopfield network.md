---
date: 1716601762.238882
tags:
- autogenerated
- source__oai
- conversation_id__1da89a1a-bc3f-4b71-a716-88fe28b143ec
- msg_aaa29203-e0bc-4a8d-ae9a-08f384ed519e
- msg_2f2fc005-5dc4-42a1-9249-d3fa5604551c
- colorclass/systems-theory
title: hopfield network
---

**USER**

Hopfield Network


**ASSISTANT**

## Hopfield Network

### Introduction

A Hopfield network is a form of recurrent artificial neural network that serves as an associative memory system with binary threshold nodes. It was invented by John Hopfield in 1982. The network is inspired by the behavior of neurons in the brain and is primarily used for pattern recognition and optimization problems. It is particularly known for its ability to converge to stable states, which correspond to stored patterns.

### Structure

A Hopfield network consists of:
- **Neurons**: Each neuron \(i\) has a binary state \(s_i \in \{-1, 1\}\).
- **Weights**: The connection weight \(w_{ij}\) between neuron \(i\) and neuron \(j\) is symmetric, meaning \(w_{ij} = w_{ji}\).
- **Thresholds**: Each neuron has an activation threshold, which can be considered part of the weight matrix or as a separate parameter.

### Dynamics

The state of the neurons in a Hopfield network is updated asynchronously or synchronously based on the following rule:

$$
s_i(t+1) = \text{sign}\left( \sum_{j} w_{ij} s_j(t) - \theta_i \right)
$$

where:
- \(s_i(t)\) is the state of neuron \(i\) at time \(t\),
- \(w_{ij}\) is the weight between neurons \(i\) and \(j\),
- \(\theta_i\) is the threshold for neuron \(i\),
- \(\text{sign}(x)\) is the sign function that returns \(+1\) if \(x \ge 0\) and \(-1\) otherwise.

### Energy Function

The dynamics of the Hopfield network can be understood in terms of an energy function \(E\):

$$
E = -\frac{1}{2} \sum_{i \neq j} w_{ij} s_i s_j + \sum_{i} \theta_i s_i
$$

The network evolves to minimize this energy function, converging to a local minimum, which corresponds to a stable state or a stored pattern.

### Learning Rule

The Hebbian learning rule is commonly used to set the weights for storing patterns in a Hopfield network. If we want to store a set of patterns \(\{\xi^\mu\}\), the weights are updated as:

$$
w_{ij} = \frac{1}{N} \sum_{\mu} \xi_i^\mu \xi_j^\mu \quad \text{for} \quad i \neq j
$$

where \(N\) is the number of neurons, and \(\xi^\mu\) is the \(\mu\)-th pattern.

### Storage Capacity

The storage capacity of a Hopfield network is the maximum number of patterns it can store and retrieve accurately. For a network with \(N\) neurons, the storage capacity \(P\) is approximately:

$$
P \approx 0.15N
$$

Beyond this limit, the network becomes unreliable, with patterns interfering with each other (overlap).

### Example: Implementation in Python

Below is a Python example demonstrating a Hopfield network that stores and retrieves patterns using the Hebbian learning rule:

```python
import numpy as np
import matplotlib.pyplot as plt

# Define the sign function
def sign(x):
    return np.where(x >= 0, 1, -1)

# Define the Hopfield network class
class HopfieldNetwork:
    def __init__(self, n_neurons):
        self.n_neurons = n_neurons
        self.weights = np.zeros((n_neurons, n_neurons))

    def train(self, patterns):
        for pattern in patterns:
            self.weights += np.outer(pattern, pattern)
        np.fill_diagonal(self.weights, 0)
        self.weights /= len(patterns)

    def run(self, initial_state, steps=10):
        state = initial_state.copy()
        for _ in range(steps):
            for i in range(self.n_neurons):
                input_i = np.dot(self.weights[i], state)
                state[i] = sign(input_i)
        return state

# Define the patterns to be stored
patterns = np.array([
    [1, -1, 1, -1, 1, -1, 1, -1],
    [-1, 1, -1, 1, -1, 1, -1, 1]
])

# Initialize the Hopfield network
hopfield_net = HopfieldNetwork(n_neurons=patterns.shape[1])

# Train the network
hopfield_net.train(patterns)

# Test the network with a noisy version of a stored pattern
initial_state = np.array([1, -1, 1, -1, 1, -1, 1, 1])
retrieved_state = hopfield_net.run(initial_state)

print("Initial State:  ", initial_state)
print("Retrieved State:", retrieved_state)

# Plot the initial and retrieved states
plt.figure(figsize=(10, 2))
plt.subplot(1, 2, 1)
plt.imshow(initial_state.reshape(1, -1), cmap='binary', aspect='auto')
plt.title('Initial State')
plt.subplot(1, 2, 2)
plt.imshow(retrieved_state.reshape(1, -1), cmap='binary', aspect='auto')
plt.title('Retrieved State')
plt.show()
```

### Applications

1. **Pattern Recognition**: Storing and recalling visual patterns, characters, and symbols.
2. **Optimization Problems**: Solving problems like the traveling salesman problem.
3. **Associative Memory**: Recalling stored information based on partial inputs.
4. **Neural Modeling**: Modeling the behavior of biological neural networks.

### Conclusion

The Hopfield network is a powerful tool for associative memory and pattern recognition, demonstrating how simple neural networks can perform complex computational tasks. Its dynamics, governed by energy minimization, allow for the stable storage and retrieval of patterns, making it a foundational model in neural computation and cognitive science. For further exploration, refer to the pages on [[Neural Networks|neural networks]], [[Associative Memory|associative memory]], and [[Optimization Algorithms|optimization algorithms]].


---

preceding: [[ising model universality class non-ferromagnetic examples]]  


following: [[associative memory]]