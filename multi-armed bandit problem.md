---
date: 1717295471.609743
tags:
- autogenerated
- source__oai
- conversation_id__ca65c423-5293-48ca-a72e-29630dbc5375
- msg_aaa2e5d7-6003-4ce4-b820-70fb901d7d32
- msg_5c627524-6cb9-4b20-9533-4854ad2b4dc0
- colorclass/decision-making-processes
title: multi-armed bandit problem
---

**USER**

multi-armed bandit


**ASSISTANT**

## Multi-Armed Bandit Problem

### Overview

The multi-armed bandit problem is a classic problem in [[reinforcement learning]] and decision theory. It involves making a sequence of decisions to maximize some notion of cumulative reward in an environment where the outcomes are partly random and partly under the control of the decision maker. The name comes from imagining a gambler at a row of slot machines (one-armed bandits), each with a different probability of paying out.

### Key Concepts

1. **Arms**: Each slot machine or action that the agent can choose from.
2. **Rewards**: The return or payoff received after selecting an arm.
3. **Exploration vs. Exploitation**: The trade-off between trying out different arms to gather more information (exploration) and selecting the best-known arm to maximize reward (exploitation).

### Problem Definition

Formally, a multi-armed bandit problem consists of:
- **K arms**: \(\{1, 2, \ldots, K\}\).
- **Reward distribution**: Each arm \(i\) provides a reward from a probability distribution \(R_i\) with an unknown mean \(\mu_i\).
- **Objective**: Maximize the cumulative reward over a sequence of trials.

### Objective Function

The goal is to maximize the expected cumulative reward over \(T\) time steps:

\[
\max_{\pi} \mathbb{E} \left[ \sum_{t=1}^T r_t \right]
\]

where \(\pi\) is the policy that dictates the arm to choose at each time step, and \(r_t\) is the reward received at time \(t\).

### Exploration Strategies

Several strategies have been developed to address the exploration vs. exploitation trade-off in the multi-armed bandit problem.

1. **\(\epsilon\)-Greedy Strategy**
2. **Upper Confidence Bound (UCB)**
3. **Thompson Sampling**

### 1. \(\epsilon\)-Greedy Strategy

The \(\epsilon\)-greedy strategy is one of the simplest methods to balance exploration and exploitation.

#### Mechanism

- With probability \(\epsilon\), choose a random arm (exploration).
- With probability \(1 - \epsilon\), choose the arm with the highest estimated mean reward (exploitation).

#### Example

```python
import numpy as np

class EpsilonGreedyBandit:
    def __init__(self, k, epsilon):
        self.k = k
        self.epsilon = epsilon
        self.counts = np.zeros(k)
        self.values = np.zeros(k)
    
    def select_arm(self):
        if np.random.rand() < self.epsilon:
            return np.random.randint(self.k)
        else:
            return np.argmax(self.values)
    
    def update(self, arm, reward):
        self.counts[arm] += 1
        n = self.counts[arm]
        value = self.values[arm]
        self.values[arm] = value + (reward - value) / n

# Example usage
k = 10
epsilon = 0.1
bandit = EpsilonGreedyBandit(k, epsilon)

for t in range(1000):
    arm = bandit.select_arm()
    reward = np.random.randn() + (arm == 5)  # Reward with some noise, optimal arm is 5
    bandit.update(arm, reward)

print("Estimated values:", bandit.values)
```

### 2. Upper Confidence Bound (UCB)

The UCB strategy aims to select arms based on their potential to be optimal, taking into account both the average reward and the uncertainty (variance) of the estimate.

#### Mechanism

At each time step \(t\), select the arm \(i\) that maximizes:

\[
\text{UCB}_i = \hat{\mu}_i + c \sqrt{\frac{\ln t}{N_i}}
\]

where \(\hat{\mu}_i\) is the estimated mean reward, \(N_i\) is the number of times arm \(i\) has been selected, and \(c\) is a confidence parameter.

#### Example

```python
import numpy as np

class UCBBandit:
    def __init__(self, k, c):
        self.k = k
        self.c = c
        self.counts = np.zeros(k)
        self.values = np.zeros(k)
    
    def select_arm(self):
        t = np.sum(self.counts) + 1
        ucb_values = self.values + self.c * np.sqrt(np.log(t) / (self.counts + 1e-5))
        return np.argmax(ucb_values)
    
    def update(self, arm, reward):
        self.counts[arm] += 1
        n = self.counts[arm]
        value = self.values[arm]
        self.values[arm] = value + (reward - value) / n

# Example usage
k = 10
c = 2
bandit = UCBBandit(k, c)

for t in range(1000):
    arm = bandit.select_arm()
    reward = np.random.randn() + (arm == 5)  # Reward with some noise, optimal arm is 5
    bandit.update(arm, reward)

print("Estimated values:", bandit.values)
```

### 3. Thompson Sampling

Thompson Sampling is a Bayesian approach that selects arms based on sampling from the posterior distributions of the arm's reward probabilities.

#### Mechanism

For each arm \(i\), sample from the posterior distribution of the mean reward and select the arm with the highest sampled value.

#### Example

```python
import numpy as np

class ThompsonSamplingBandit:
    def __init__(self, k):
        self.k = k
        self.successes = np.zeros(k)
        self.failures = np.zeros(k)
    
    def select_arm(self):
        samples = np.random.beta(self.successes + 1, self.failures + 1)
        return np.argmax(samples)
    
    def update(self, arm, reward):
        if reward > 0:
            self.successes[arm] += 1
        else:
            self.failures[arm] += 1

# Example usage
k = 10
bandit = ThompsonSamplingBandit(k)

for t in range(1000):
    arm = bandit.select_arm()
    reward = np.random.binomial(1, 0.5 + 0.1 * (arm == 5))  # Reward with some noise, optimal arm is 5
    bandit.update(arm, reward)

print("Successes:", bandit.successes)
print("Failures:", bandit.failures)
```

### Applications of Multi-Armed Bandit Algorithms

1. **Online Advertising**: Selecting ads to display to maximize click-through rates.
2. **Recommendation Systems**: Recommending items to users to maximize engagement.
3. **Clinical Trials**: Allocating treatments to patients to maximize health outcomes.
4. **A/B Testing**: Deciding which version of a webpage to show to users to maximize conversions.
5. **Financial Trading**: Selecting financial instruments to trade to maximize returns.

### Conclusion

The multi-armed bandit problem is a fundamental problem in [[reinforcement learning]] and decision theory. It involves balancing exploration and exploitation to maximize cumulative rewards. Various strategies, including \(\epsilon\)-greedy, UCB, and Thompson Sampling, provide effective solutions to this problem. Understanding and implementing these strategies is essential for solving a wide range of real-world problems where decision-making under uncertainty is required.


---

preceding: [[experience replay in [[reinforcement learning]]]]  


following: [[thompson sampling in multi-armed bandit problems]]

See also: [[thompson sampling in multi-armed bandit problems]]


See also: [[balancing exploration and exploitation in reinforcement learning]]


See also: [[decision-making under uncertainty]]


See also: [[decision theory]]


See also: [[upper confidence bound (ucb) strategy in multi-armed bandit problems]]


See also: [[(epsilon)-greedy strategy in reinforcement learning and multi-armed bandit problems]]