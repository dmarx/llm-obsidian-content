---
date: 1717826455.645271
tags:
- autogenerated
- source__oai
- conversation_id__ac8c3253-a669-4989-bf7c-c535d52c8140
- msg_aaa2943c-5e0d-424f-be1e-398f8b6cb147
- msg_f9b74ff7-6186-4dbb-b65f-6b9609e66bde
- colorclass/vector-space
title: lanczos algorithm
---

**USER**

Lanczos Algorithm


**ASSISTANT**

The [[Lanczos Algorithm]] is an iterative method used to find the eigenvalues and eigenvectors of a large, sparse symmetric matrix. It is particularly efficient because it reduces the matrix to a tridiagonal form, which is easier to handle computationally. The algorithm is a key tool in numerical linear algebra and is closely related to the conjugate gradient method.

## Overview

The primary goal of the Lanczos algorithm is to approximate the eigenvalues and eigenvectors of a symmetric matrix \(A\) by constructing a sequence of orthogonal vectors, known as the Lanczos vectors, that span a Krylov subspace. The algorithm projects the original matrix \(A\) onto this subspace, resulting in a smaller tridiagonal matrix \(T\), whose eigenvalues approximate those of \(A\).

### Mathematical Formulation

Given a symmetric matrix \(A \in \mathbb{R}^{n \times n}\) and an initial vector \(v_1\) (with \(\|v_1\| = 1\)), the Lanczos algorithm constructs an orthonormal basis \(\{v_1, v_2, \ldots, v_m\}\) for the Krylov subspace \(\mathcal{K}_m(A, v_1) = \text{span}\{v_1, Av_1, A^2v_1, \ldots, A^{m-1}v_1\}\). The algorithm iterates as follows:

1. **Initialization**:
   - Set \(v_0 = 0\) and \(\beta_0 = 0\).

2. **Iteration** (for \(j = 1, 2, \ldots, m\)):
   - Compute \(w_j = Av_j - \beta_{j-1} v_{j-1}\).
   - Orthogonalize \(w_j\) against \(v_j\): 
     $$ \alpha_j = v_j^T w_j $$
     $$ w_j = w_j - \alpha_j v_j $$
   - Re-orthogonalize \(w_j\) against previous vectors if necessary to maintain numerical stability.
   - Compute \(\beta_j = \|w_j\|\).
   - Normalize \(w_j\): 
     $$ v_{j+1} = \frac{w_j}{\beta_j} $$
   - Form the tridiagonal matrix \(T_m\) with \(\alpha_j\) on the diagonal and \(\beta_j\) on the off-diagonals.

### Tridiagonal Matrix

The resulting tridiagonal matrix \(T_m\) is:

$$
T_m = \begin{pmatrix}
\alpha_1 & \beta_1 & 0 & \cdots & 0 \\
\beta_1 & \alpha_2 & \beta_2 & \cdots & 0 \\
0 & \beta_2 & \alpha_3 & \cdots & 0 \\
\vdots & \vdots & \vdots & \ddots & \beta_{m-1} \\
0 & 0 & 0 & \beta_{m-1} & \alpha_m
\end{pmatrix}
$$

### Eigenvalue Approximation

The eigenvalues of \(T_m\) are approximations of the eigenvalues of \(A\). As \(m\) increases, these approximations generally improve. If \(A\) is large and sparse, solving the eigenproblem for \(T_m\) is much more efficient than for \(A\).

### Advantages and Applications

- **Efficiency**: Particularly suitable for large, sparse matrices due to its iterative nature and reduced computational complexity.
- **Accuracy**: Can provide good approximations for a subset of eigenvalues and eigenvectors.
- **Applications**: Widely used in areas such as [[Quantum Mechanics]], structural engineering, and machine learning (e.g., for spectral clustering and [[Principal Component Analysis|PCA]]).

### Example

Consider a symmetric matrix \(A\):

$$
A = \begin{pmatrix}
4 & 1 & 0 & 0 \\
1 & 4 & 1 & 0 \\
0 & 1 & 4 & 1 \\
0 & 0 & 1 & 4
\end{pmatrix}
$$

1. **Initialization**:
   - Let \(v_1 = (1, 0, 0, 0)^T\).

2. **First Iteration**:
   - Compute \(w_1 = Av_1 = (4, 1, 0, 0)^T\).
   - Orthogonalize: \(\alpha_1 = v_1^T w_1 = 4\).
   - Compute \(w_1 = (4, 1, 0, 0)^T - 4(1, 0, 0, 0)^T = (0, 1, 0, 0)^T\).
   - \(\beta_1 = \|w_1\| = 1\).
   - Normalize: \(v_2 = (0, 1, 0, 0)^T\).

3. **Second Iteration**:
   - Compute \(w_2 = Av_2 - \beta_1 v_1 = (1, 4, 1, 0)^T - 1(1, 0, 0, 0)^T = (0, 4, 1, 0)^T\).
   - Orthogonalize: \(\alpha_2 = v_2^T w_2 = 4\).
   - Compute \(w_2 = (0, 4, 1, 0)^T - 4(0, 1, 0, 0)^T = (0, 0, 1, 0)^T\).
   - \(\beta_2 = \|w_2\| = 1\).
   - Normalize: \(v_3 = (0, 0, 1, 0)^T\).

4. **Continue** until the desired subspace dimension \(m\) is reached.

The tridiagonal matrix \(T_m\) will have its eigenvalues approximate those of \(A\).

## Related Topics

- [[Arnoldi Iteration]]: Generalization of the Lanczos algorithm for non-symmetric matrices.
- [[Conjugate Gradient Method]]: Another iterative method for solving linear systems, related to the Lanczos algorithm in the symmetric case.
- [[Rayleigh Quotient Iteration]]: An iterative method for finding eigenvalues and eigenvectors, often used in conjunction with Lanczos.

The Lanczos algorithm remains a fundamental technique in numerical linear algebra, offering a powerful means to approximate eigenvalues and eigenvectors of large, sparse matrices efficiently.


---

preceding: [[conjugate gradient methods]]  


following: [[krylov subspace methods]]

See also: [[orthonormal basis]]


See also: [[structural engineering]]