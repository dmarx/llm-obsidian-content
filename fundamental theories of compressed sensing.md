---
date: 1716695505.154813
tags:
- autogenerated
- source__oai
- conversation_id__6f59b0e3-e6e6-4f63-9641-71199b8865a0
- msg_aaa22000-56cf-4070-9058-bd818fb8f5ce
- msg_2327257d-1bd7-486a-9066-f27b25951309
- colorclass/information-theory
title: fundamental theories of compressed sensing
---

**USER**

fundamental theories of compressed sensing


**ASSISTANT**

Compressed sensing (CS), also known as compressive sampling, is a [[signal processing]] technique for efficiently acquiring and reconstructing signals that are sparse or compressible in some domain. The fundamental theories of compressed sensing challenge the traditional Nyquist-Shannon sampling theorem by enabling accurate signal reconstruction from fewer samples than required by conventional methods. Here, we will explore the key concepts and mathematical formalisms underlying compressed sensing.

## Key Concepts in Compressed Sensing

### Sparsity

A signal \( \mathbf{x} \in \mathbb{R}^N \) is considered sparse if most of its components are zero or close to zero. Formally, a signal is \( k \)-sparse if it has at most \( k \) non-zero elements. Sparsity is the cornerstone of compressed sensing.

### Incoherence

Incoherence measures the degree to which the sampling basis and the sparsity basis are correlated. Compressed sensing requires that the measurement matrix \( \mathbf{\Phi} \) (sampling basis) and the sparsity basis \( \mathbf{\Psi} \) be incoherent. This ensures that the information captured by the samples is spread out over the entire signal space, allowing for accurate reconstruction from fewer samples.

### Restricted Isometry Property (RIP)

The Restricted Isometry Property is a condition on the measurement matrix \( \mathbf{\Phi} \) that ensures it preserves the Euclidean length of sparse signals. Specifically, a matrix \( \mathbf{\Phi} \) satisfies the \( k \)-RIP if, for all \( k \)-sparse signals \( \mathbf{x} \):

$$
(1 - \delta_k) \| \mathbf{x} \|_2^2 \leq \| \mathbf{\Phi x} \|_2^2 \leq (1 + \delta_k) \| \mathbf{x} \|_2^2
$$

where \( \delta_k \) is the restricted isometry constant.

## Mathematical Formulation

### Sensing Model

In compressed sensing, we acquire a signal \( \mathbf{x} \in \mathbb{R}^N \) by taking \( M \) linear measurements \( \mathbf{y} \in \mathbb{R}^M \) using a measurement matrix \( \mathbf{\Phi} \in \mathbb{R}^{M \times N} \):

$$
\mathbf{y} = \mathbf{\Phi x}
$$

where \( M \ll N \). The goal is to reconstruct \( \mathbf{x} \) from \( \mathbf{y} \).

### Reconstruction via \( \ell_1 \)-Minimization

Given the measurements \( \mathbf{y} \) and the matrix \( \mathbf{\Phi} \), the signal \( \mathbf{x} \) can be reconstructed by solving an optimization problem that promotes sparsity. The most common approach is \( \ell_1 \)-minimization:

$$
\min \| \mathbf{x} \|_1 \quad \text{subject to} \quad \mathbf{y} = \mathbf{\Phi x}
$$

This problem can be solved using various convex optimization algorithms.

### Basis Pursuit

Basis pursuit is a specific instance of \( \ell_1 \)-minimization used for sparse signal reconstruction. The objective is to find the sparsest solution that satisfies the linear measurements:

$$
\min \| \mathbf{x} \|_1 \quad \text{subject to} \quad \mathbf{y} = \mathbf{\Phi x}
$$

### LASSO (Least Absolute Shrinkage and Selection Operator)

LASSO is another approach to sparse signal reconstruction, combining \( \ell_1 \)-regularization with a least-squares error term:

$$
\min \| \mathbf{y} - \mathbf{\Phi x} \|_2^2 + \lambda \| \mathbf{x} \|_1
$$

where \( \lambda \) is a regularization parameter controlling the trade-off between sparsity and measurement error.

## Theoretical Guarantees

### Recovery Conditions

Several theoretical results provide guarantees for the recovery of sparse signals using compressed sensing. These include:

1. **Exact Recovery**: For a \( k \)-sparse signal \( \mathbf{x} \), if the measurement matrix \( \mathbf{\Phi} \) satisfies the \( 2k \)-RIP with \( \delta_{2k} < \sqrt{2} - 1 \), then \( \mathbf{x} \) can be exactly recovered by solving the \( \ell_1 \)-minimization problem.

2. **Stable Recovery**: If \( \mathbf{x} \) is not exactly sparse but compressible (i.e., its entries decay rapidly), then the recovery error using \( \ell_1 \)-minimization is proportional to the noise level and the tail energy of \( \mathbf{x} \).

### Measurement Bounds

The number of measurements \( M \) required for accurate recovery of a \( k \)-sparse signal is typically on the order of:

$$
M \geq Ck \log \left(\frac{N}{k}\right)
$$

where \( C \) is a constant. This indicates that the number of measurements needed grows logarithmically with the signal dimension \( N \) and linearly with the sparsity level \( k \).

## Practical Algorithms

### Iterative Shrinkage-Thresholding Algorithm (ISTA)

ISTA is an iterative algorithm for solving \( \ell_1 \)-minimization problems. Each iteration involves a gradient descent step followed by a soft-thresholding operation:

$$
\mathbf{x}^{(k+1)} = \mathcal{S}_{\lambda/L} \left( \mathbf{x}^{(k)} - \frac{1}{L} \mathbf{\Phi}^T (\mathbf{\Phi x}^{(k)} - \mathbf{y}) \right)
$$

where \( \mathcal{S}_{\lambda} \) is the soft-thresholding operator and \( L \) is the Lipschitz constant of \( \mathbf{\Phi}^T \mathbf{\Phi} \).

### Compressive Sensing Matching Pursuit (CoSaMP)

CoSaMP is a greedy algorithm that iteratively selects the columns of \( \mathbf{\Phi} \) that are most correlated with the current residual:

1. **Initialization**: Set \( \mathbf{r} = \mathbf{y} \), \( \mathbf{x} = 0 \).
2. **Support Identification**: Identify the largest components of \( \mathbf{\Phi}^T \mathbf{r} \).
3. **Signal Estimation**: Solve a least-squares problem over the identified support.
4. **Support Pruning**: Keep the largest components of the signal estimate.
5. **Residual Update**: Update the residual \( \mathbf{r} \).

### Orthogonal Matching Pursuit (OMP)

OMP is another greedy algorithm that iteratively builds the sparse signal by selecting the columns of \( \mathbf{\Phi} \) that are most aligned with the current residual:

1. **Initialization**: Set \( \mathbf{r} = \mathbf{y} \), \( \mathbf{x} = 0 \), and the support \( \Lambda = \emptyset \).
2. **Support Update**: Add the index of the column of \( \mathbf{\Phi} \) that is most correlated with \( \mathbf{r} \) to \( \Lambda \).
3. **Signal Estimation**: Solve a least-squares problem over the support \( \Lambda \).
4. **Residual Update**: Update the residual \( \mathbf{r} = \mathbf{y} - \mathbf{\Phi}_{\Lambda} \mathbf{x}_{\Lambda} \).

These algorithms and theories provide a powerful framework for compressed sensing, enabling efficient signal acquisition and reconstruction in a wide range of applications, from medical imaging to communications and beyond.


---

preceding: [[fundamental theories of [[signal processing]]]]  


following: [[information theory in compressive sensing]]

See also: [[information theory]]