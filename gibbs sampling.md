---
date: 1721161582.819075
tags:
- autogenerated
- source__oai
- conversation_id__4f78839e-971a-4476-b886-e0eef3bd4caf
- msg_83bb173f-a534-4b51-bc0a-28aeac126da3
- msg_73964ee2-1dfa-4fe5-9feb-f7bacc5babe6
- colorclass/game-theory
title: gibbs sampling
---

**USER**

Gibbs sampling


**ASSISTANT**

Gibbs sampling is a Markov Chain Monte Carlo (MCMC) algorithm used for sampling from high-dimensional probability distributions when direct sampling is difficult. It is particularly useful for Bayesian inference in hierarchical models and other complex settings. Gibbs sampling iteratively samples each variable from its conditional distribution, given the current values of all other variables. This method is efficient and relatively easy to implement when these conditional distributions are known and can be sampled from directly.

## Algorithm

Consider a multivariate distribution $\pi(\theta_1, \theta_2, \ldots, \theta_k \mid x)$ where $\theta = (\theta_1, \theta_2, \ldots, \theta_k)$ are the parameters of interest and $x$ is the observed data. The Gibbs sampling algorithm proceeds as follows:

1. **Initialization**: Choose initial values for all parameters $\theta^{(0)} = (\theta_1^{(0)}, \theta_2^{(0)}, \ldots, \theta_k^{(0)})$.

2. **Iterative Sampling**: For each iteration $t = 1, 2, \ldots, T$:
   - Sample $\theta_1^{(t)}$ from the conditional distribution $\pi(\theta_1 \mid \theta_2^{(t-1)}, \theta_3^{(t-1)}, \ldots, \theta_k^{(t-1)}, x)$.
   - Sample $\theta_2^{(t)}$ from the conditional distribution $\pi(\theta_2 \mid \theta_1^{(t)}, \theta_3^{(t-1)}, \ldots, \theta_k^{(t-1)}, x)$.
   - Continue this process for all parameters, sampling $\theta_i^{(t)}$ from $\pi(\theta_i \mid \theta_1^{(t)}, \ldots, \theta_{i-1}^{(t)}, \theta_{i+1}^{(t-1)}, \ldots, \theta_k^{(t-1)}, x)$.

3. **Convergence**: After a sufficient number of iterations (burn-in period), the samples $\{\theta^{(t)}\}_{t=B+1}^T$ are considered to be drawn from the joint posterior distribution $\pi(\theta \mid x)$.

## Example: Bivariate Normal Distribution

Consider a bivariate normal distribution with parameters $\theta = (\mu_1, \mu_2)$ and known covariance matrix $\Sigma$. Suppose we have the following model:

$$
\begin{pmatrix}
\mu_1 \\
\mu_2
\end{pmatrix}
\sim N\left(\begin{pmatrix}
\mu_{1,0} \\
\mu_{2,0}
\end{pmatrix}, \Sigma \right)
$$

where $\Sigma = \begin{pmatrix}
\sigma_1^2 & \rho\sigma_1\sigma_2 \\
\rho\sigma_1\sigma_2 & \sigma_2^2
\end{pmatrix}$.

The conditional distributions for $\mu_1$ and $\mu_2$ are:

$$
\mu_1 \mid \mu_2, x \sim N\left(\mu_{1,0} + \rho \frac{\sigma_1}{\sigma_2} (\mu_2 - \mu_{2,0}), \sigma_1^2(1 - \rho^2)\right)
$$

$$
\mu_2 \mid \mu_1, x \sim N\left(\mu_{2,0} + \rho \frac{\sigma_2}{\sigma_1} (\mu_1 - \mu_{1,0}), \sigma_2^2(1 - \rho^2)\right)
$$

### Gibbs Sampling Steps:

1. **Initialization**: Start with initial values $\mu_1^{(0)}$ and $\mu_2^{(0)}$.

2. **Iterative Sampling**:
   - Sample $\mu_1^{(t)}$ from $N\left(\mu_{1,0} + \rho \frac{\sigma_1}{\sigma_2} (\mu_2^{(t-1)} - \mu_{2,0}), \sigma_1^2(1 - \rho^2)\right)$.
   - Sample $\mu_2^{(t)}$ from $N\left(\mu_{2,0} + \rho \frac{\sigma_2}{\sigma_1} (\mu_1^{(t)} - \mu_{1,0}), \sigma_2^2(1 - \rho^2)\right)$.

3. **Repeat**: Continue the iterations until convergence.

## Advantages and Limitations

### Advantages:
- **Ease of Implementation**: Gibbs sampling is relatively easy to implement when the conditional distributions are known and can be sampled from directly.
- **Efficiency**: It can be efficient in high-dimensional problems where direct sampling from the joint distribution is difficult.
- **Versatility**: Gibbs sampling can be applied to a wide range of models, including hierarchical models, graphical models, and more.

### Limitations:
- **Convergence**: Ensuring convergence can be difficult. Diagnostics such as trace plots, the Gelman-Rubin statistic, and effective sample size calculations are often used to assess convergence.
- **Dependence**: Gibbs sampling can suffer from high autocorrelation between successive samples, leading to slow mixing and requiring more iterations to obtain independent samples.
- **Conditional Distribution Requirement**: The method requires that the full conditional distributions be known and easily sampled from. In some cases, these distributions may not be available in a closed form, complicating the implementation.

## Applications

### Hierarchical Models
In hierarchical models, Gibbs sampling is often used to sample from the posterior distribution of parameters at different levels of the hierarchy. For example, in a hierarchical linear model:

$$
y_{ij} = \beta_0 + \beta_1 x_{ij} + \epsilon_{ij} \\
\beta_0, \beta_1 \sim N(\mu, \Sigma)
$$

### Bayesian Networks
In Bayesian networks, Gibbs sampling can be used to sample from the joint posterior distribution of the network's parameters, given the observed data.

### Latent Variable Models
In models with latent variables, such as mixture models or hidden Markov models, Gibbs sampling can be used to sample from the posterior distribution of both the latent variables and the model parameters.

## Conclusion

Gibbs sampling is a powerful MCMC algorithm for sampling from high-dimensional posterior distributions. Its iterative approach of sampling from conditional distributions makes it particularly suitable for hierarchical models and other complex Bayesian settings. While it has its limitations, such as ensuring convergence and handling high autocorrelation, Gibbs sampling remains a fundamental tool in modern Bayesian statistics and computational inference.


---

preceding: [[modern bayesian approaches]]  


following: [[relate gibbs sampling to coordinate descent]]

See also: [[relate gibbs sampling to coordinate descent]]


See also: [[coordinate descent]]