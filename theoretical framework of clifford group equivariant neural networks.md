---
date: 1713153194.471989
tags:
- autogenerated
- source__oai
- conversation_id__5fa9dfe5-a59e-4ed4-a2e9-ff745c9a6df9
- msg_aaa20c4f-d5e1-4a77-a401-346f9a0f5f2f
- msg_01407140-baae-4dbe-8060-643520eef299
- colorclass/vector-space
title: theoretical framework of clifford group equivariant neural networks
---

**USER**

3 Theoretical Results
In order to construct equivariant multivector-valued neural networks, we outline our theoretical
results. We first introduce the following theorem regarding the multivector grading of the Clifford
algebra, which is well-known in case the algebra’s metric is non-degenerate. Although the general
case, including a potentially degenerate metric, appears to be accepted, we were unable to find a
self-contained proof during our studies. Hence, we include it here for completeness.
Theorem 3.1 (The multivector grading of the Clifford algebra). Let e1, . . . , en and b1, . . . , bn be
two orthogonal bases of (V, q). Then the following sub-vector spaces Cl(m)
(V, q) of Cl(V, q),
2
If ki = 0 the product of vectors vi,1 · · · vi,ki
is empty, and, in this case, we mean the unit 1 in Cl(V, q).

m = 0, . . . , n, are independent of the choice of the orthogonal basis, i.e.,
Cl(m)
(V, q) := span {eA | A ⊆ [n], |A| = m}
!
= span {bA | A ⊆ [n], |A| = m} . (1)
The proof can be found in Theorem D.27. Intuitively, this means that the claims made in the following
(and the supplementary material) are not dependent on the chosen frame of reference, even in the
degenerate setting. We now declare that the Clifford algebra Cl(V, q) decomposes into an orthogonal
direct sum of the vector subspaces Cl(m)
(V, q). To this end, we need to extend the bilinear form b
from V to Cl(V, q). For elements x1, x2, x ∈ Cl(V, q), the extended bilinear form ¯b and the extended
quadratic form ¯q are given via the projection onto the zero-component (explained below), where
β : Cl(V, q) → Cl(V, q) denotes the main anti-involution of Cl(V, q)
3
:
¯b : Cl(V, q) × Cl(V, q) → F,
¯b(x1, x2) := (β(x1)x2)
(0)
, ¯q(x) := ¯b(x, x). (2)
Note that by construction, both ¯b and ¯q reduce to their original versions when restricted to V . Using
the extended quadratic form, the tuple (Cl(V, q), ¯q) turns into a quadratic vector space in itself. As a
corollary (see Corollary D.30), the Clifford algebra has an orthogonal-sum decomposition w.r.t. the
extended bilinear form ¯b:
Cl(V, q) = Mn
m=0
Cl(m)
(V, q), dim Cl(m)
(V, q) = 
n
m

. (3)
This result implies that we can always write x ∈ Cl(V, q) as x = x
(0) + x
(1) + · · · + x
(n)
, where
x
(m) ∈ Cl(m)
(V, q) denotes the grade-m part of x. Selecting a grade defines an orthogonal projection:
(_)
(m)
: Cl(V, q) → Cl(m)
(V, q), x 7→ x
(m)
, m = 0, . . . , n. (4)
Let us further introduce the notation Cl[0](V, q) := Ln
m even Cl(m)
(V, q), whose elements are of even
parity, and Cl[1](V, q) := Ln
m odd Cl(m)
(V, q) for those of odd parity. We use x = x
[0] + x
[1] to
denote the parity decomposition of a multivector.
3.1 The Clifford Group and its Clifford Algebra Representations
Let Cl×(V, q) denote the group of invertible elements of the Clifford algebra, i.e., the set of those elements w ∈ Cl(V, q) that have an inverse w
−1 ∈ Cl(V, q): w
−1w = ww−1 = 1. For w ∈ Cl×(V, q),
we then define the (adjusted) twisted conjugation as follows:
ρ(w) : Cl(V, q) → Cl(V, q), ρ(w)(x) := wx[0]w
−1 + α(w)x
[1]w
−1
, (5)
where α is the main involution of Cl(V, q), which is given by α(w) := w
[0] − w
[1]. This map
ρ(w) : Cl(V, q) → Cl(V, q), notably not just from V → V , will be essential for constructing
equivariant neural networks operating on the Clifford algebra. In general, ρ and similar maps defined
in the literature do not always posses the required properties (see Motivation E.1). However, when
our ρ is restricted to a carefully chosen subgroup of Cl×(V, q), many desirable characteristics emerge.
This subgroup will be called the Clifford group4 of Cl(V, q) and we define it as:
Γ(V, q) := n
w ∈ Cl×(V, q) ∩

Cl[0](V, q) ∪ Cl[1](V, q)
 


∀v ∈ V, ρ(w)(v) ∈ V
o
. (6)
In words, Γ(V, q) contains all invertible (parity) homogeneous elements that preserve vectors (m = 1
elements) via ρ. The special Clifford group is defined as Γ
[0](V, q) := Γ(V, q) ∩ Cl[0](V, q).
Regarding the twisted conjugation, ρ(w) was ensured to reduce to a reflection when restricted
to V – a property that we will conveniently use in the upcoming section. Specifically, when
w, x ∈ Cl(1)(V, q) = V , w ∈ Cl×(V, q), then ρ(w) reflects x in the hyperplane normal to w:
ρ(w)(x) = −wxw−1 != x − 2
b(w, x)
b(w, w)
w. (7)
Next, we collect several advantageous identities of ρ in the following theorem, which we elaborate
on afterwards. For proofs, consider Lemma E.8, Theorem E.10, and Theorem E.29.
3Recall that any x ∈ Cl(V, q) can be written as a linear combination of vector products. β is the map that
reverses the order: β
P
i∈I
ci · vi,1 · · · vi,ki

:= P
i∈I
ci · vi,ki
· · · vi,1. For details, see Definition D.18.
4We elaborate shortly on the term Clifford group in contrast to similar definitions in Remark E.14.

Cl(V, q) × · · · × Cl(V, q)
ℓ times
z }| {
Cl(V, q) × · · · × Cl(V, q)
Cl(V, q)
Cl(V, q)
ρ(w) ρ(w) ρ(w)
F
ρ(w)
F
Cl(V, q)
Cl(V, q)
Cl(m)
(V, q)
Cl(m)
(V, q)
ρ(w) ρ(w)
(_)
(m)
(_)
(m)
Figure 2: Commutative diagrams expressing Clifford group equivariance with respect to the main
operations: polynomials F (left) and grade projections (_)
(m)
(right).
Theorem 3.2. Let w1, w2, w ∈ Γ(V, q), x1, x2, x ∈ Cl(V, q), c ∈ F, m ∈ {0, . . . , n}. ρ then
satisfies:
1. Additivity: ρ(w)(x1 + x2) = ρ(w)(x1) + ρ(w)(x2),
2. Multiplicativity: ρ(w)(x1x2) = ρ(w)(x1)ρ(w)(x2), and: ρ(w)(c) = c,
3. Invertibility: ρ(w
−1
)(x) = ρ(w)
−1
(x),
4. Composition: ρ(w2) (ρ(w1)(x)) = ρ(w2w1)(x), and: ρ(c)(x) = x for c ̸= 0,
5. Orthogonality: ¯b (ρ(w)(x1), ρ(w)(x2)) = ¯b (x1, x2).
The first two properties state that ρ(w) is not only additive, but even multiplicative regarding the
geometric product. The third states that ρ(w) is invertible, making it an algebra automorphism of
Cl(V, q).
The fourth property then states that ρ : Γ(V, q) → AutAlg(Cl(V, q)) is also a group homomorphism
to the group of all algebra automorphisms. In other words, Cl(V, q) is a linear representation of
Γ(V, q) and, moreover, it is also an algebra representation of Γ(V, q). Finally, the last point shows
that each ρ(w) generates an orthogonal map with respect to the extended bilinear form ¯b. These
properties yield the following results (see also Theorem E.16, Corollary E.18, Figure 2).
Corollary 3.3 (All grade projections are Clifford group equivariant). For w ∈ Γ(V, q), x ∈ Cl(V, q)
and m = 0, . . . , n we have the following equivariance property:
ρ(w)(x
(m)
) = (ρ(w)(x))(m)
. (8)
In particular, for x ∈ Cl(m)
(V, q) we also have ρ(w)(x) ∈ Cl(m)
(V, q).
This implies that the grade projections: Cl(V, q) → Cl(m)
(V, q) are Γ(V, q)-equivariant maps, and,
that each Cl(m)
(V, q) constitutes an orthogonal representation of Γ(V, q). The latter means that ρ
induces a group homomorphisms from the Clifford group to the group of all orthogonal invertible
linear transformations of Cl(m)
(V, q):
ρ
(m)
: Γ(V, q) → O

Cl(m)
(V, q), ¯q

, ρ(m)
(w) := ρ(w)|Cl(m)
(V,q)
. (9)
Corollary 3.4 (All polynomials are Clifford group equivariant). Let F ∈ F[T1, . . . , Tℓ] be a
polynomial in ℓ variables with coefficients in F, w ∈ Γ(V, q). Further, consider ℓ elements
x1, . . . , xℓ ∈ Cl(V, q). Then we have the following equivariance property:
ρ(w) (F(x1, . . . , xℓ)) = F(ρ(w)(x1), . . . , ρ(w)(xℓ)). (10)
To prove that ρ(w) distributes over any polynomial, we use both its additivity and multiplicativity
regarding the geometric product.
By noting that one can learn the coefficients of such polynomials, we can build flexible parameterizations of Clifford group equivariant layers for quadratic vector spaces of any dimension or metric. We
involve grade projections to achieve denser parameterizations. More details regarding neural network
constructions follow in Section 4.
5
3.2 Orthogonal Representations
To relate to the orthogonal group O(V, q), the set of invertible linear maps Φ : V → V preserving q,
first note that Equation (5) shows that for every w ∈ F
× we always have: ρ(w) = idCl(V,q)
. So the
action ρ on Cl(V, q) can already be defined on the quotient group Γ(V, q)/F
×. Moreover, we have:
Theorem 3.5 (See also Remark E.31 and Corollary E.32 ). If (V, q) is non-degenerate5
then ρ induces
a well-defined group isomorphism:
ρ¯
(1) : Γ(V, q)/F
× ∼−→ O(V, q), [w] 7→ ρ(w)|V . (11)
The above implies that O(V, q) acts on whole Cl(V, q) in a well-defined way. Concretely, if
x ∈ Cl(V, q) is of the form x =
P
i∈I
ci
· vi,1 · · · vi,ki with vi,j ∈ V , ci ∈ F and Φ ∈ O(V, q)
is given by ρ¯
(1)([w]) = Φ with w ∈ Γ(V, q), then we have:
ρ(w)(x) = X
i∈I
ci
· ρ(w)(vi,1)· · · ρ(w)(vi,ki
) = X
i∈I
ci
· Φ(vi,1)· · · Φ(vi,ki
). (12)
This means that Φ acts on a multivector x by applying an orthogonal transformation to all
its vector components vi,j , as illustrated in Figure 1. Theorem 3.5 implies that that a map
f : Cl(V, q)
ℓin → Cl(V, q)
ℓout is equivariant to the Clifford group Γ(V, q) if and only if it is
equivariant to the orthogonal group O(V, q) when both use the actions on Cl(V, q) described
above (for each component). To leverage these results for constructing O(V, q)-equivariant neural
networks acting on vector features, i.e., x ∈ V
ℓin , we refer to Section 4.
To prove Theorem 3.5, we invoke Cartan-Dieudonné (Theorem C.13), stating that every orthogonal
transformation decomposes into compositions of reflections, and recall that ρ reduces to a reflection
when restricted to V , see Theorem E.25. Theorem 3.5 also allows one to provide explicit descriptions
of the element of the Clifford group, see Corollary E.27.
Finally, it is worth noting that our method is also Pin and Spin group-equivariant6
. These groups,
sharing properties with the Clifford group, are also often studied in relation to the orthogonal group.


**ASSISTANT**

### Theoretical Framework of Clifford Group Equivariant Neural Networks

The section discusses important theoretical results underpinning Clifford Group Equivariant Neural Networks (CGENNs), which leverage the structural properties of Clifford algebras. These networks are designed to maintain equivariance under the action of the Clifford group, thus ensuring that neural network operations respect geometric transformations such as rotations and reflections.

#### Theorem on Multivector Grading

The Clifford algebra, denoted as $\text{Cl}(V, q)$, where $V$ is a vector space with a quadratic form $q$, supports a grading by multivectors. This theorem (Theorem 3.1) asserts that the subspaces $\text{Cl}_m(V, q)$, which are spanned by multivectors of a specific grade $m$, are independent of the choice of orthogonal basis. This property is fundamental because it implies that the structure of the Clifford algebra—and by extension, the operations of CGENNs—is stable across different orthogonal transformations of the space.

##### Key Concepts:
- **Multivector Grading**: The grading of the Clifford algebra into subspaces based on the degree of multivectors.
- **Orthogonal Basis Independence**: Ensures that the algebraic structure is consistent regardless of the basis used, which is critical for maintaining equivariance in practical applications.

#### Extended Bilinear and Quadratic Forms

The extended bilinear form $\overline{b}$ and quadratic form $\overline{q}$ are defined for the entire Clifford algebra, not just the original vector space $V$. These forms play a crucial role in defining the algebraic operations and ensuring that they behave well under transformations prescribed by the Clifford group.

##### Definitions:
- **Extended Bilinear Form**: $\overline{b}(x_1, x_2) = (\beta(x_1)x_2)^{(0)}$, where $\beta$ is the main anti-involution of $\text{Cl}(V, q)$.
- **Extended Quadratic Form**: $\overline{q}(x) = \overline{b}(x, x)$.

#### Clifford Group and Equivariance

The Clifford group $\Gamma(V, q)$ consists of elements in $\text{Cl}(V, q)$ that preserve the vector space structure under conjugation and maintain the algebra's multiplicative (geometric product) structure. The action defined by this group, especially the adjusted twisted conjugation $\rho(w)$, is central to constructing CGENNs.

##### Twisted Conjugation:
- **Definition**: $\rho(w)(x) = wx[0]w^{-1} + \alpha(w)x[1]w^{-1}$, where $\alpha$ is the main involution of the Clifford algebra.
- **Properties**: This map is additive, multiplicative, and invertible, making it an algebra automorphism and ensuring that the network's behavior is consistent under transformations.

#### Theoretical Implications and Results

1. **Equivariance of Projections**: All grade projections $\rho(w)(x^{(m)}) = (\rho(w)(x))^{(m)}$ are equivariant, implying that the transformation properties are preserved across different grades of multivectors.
2. **Polynomial Equivariance**: The equivariance extends to polynomials in multivectors, which means complex network operations involving multivectors adhere to the equivariant structure.
3. **Orthogonal Transformations**: The group action induces a group isomorphism between $\Gamma(V, q)/F^\times$ and the orthogonal group $O(V, q)$, establishing a deep connection between abstract algebraic structures and geometric transformations.

### Conclusion

The theoretical grounding of CGENNs in the properties of the Clifford algebra and its group of transformations provides a robust framework for developing neural networks that are inherently sensitive to the geometric and algebraic properties of the data they process. This makes them particularly suited for scientific and engineering applications where data often comes with intrinsic geometric structures that need to be preserved.


---

preceding: [[clifford group equivariant neural networks]]  


following: [[why is this useful]]

See also: [[vector space]]


See also: [[clifford group equivariant neural networks]]


See also: [[abstract algebra]]


See also: [[orthogonal projection]]