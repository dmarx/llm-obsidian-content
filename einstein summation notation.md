---
date: 1721500407.017252
tags:
- autogenerated
- source__oai
- conversation_id__032fc94b-1121-4861-a1f8-b3d77bfcd6f3
- msg_aaa21b54-a12e-405f-8c57-b119d4d6696b
- msg_5adeb676-17b7-4a6c-9c20-50f8e420c0ec
- colorclass/vector-space
title: einstein summation notation
---

**USER**

Einstein summation notation


**ASSISTANT**

### Einstein Summation Notation

**Einstein summation notation** is a concise and efficient way to represent summation over indices in tensor algebra. It was introduced by Albert Einstein to simplify the expressions in his theory of relativity. This notation is particularly useful for dealing with high-dimensional arrays (tensors) by reducing the complexity of the equations.

#### Basic Principle

The fundamental rule of Einstein summation notation is that whenever an index variable appears twice in a single term, it implies summation over that index. This removes the need to write explicit summation symbols.

For example, consider the summation of products of elements in vectors \(a\) and \(b\):

$$
c = \sum_{i} a_i b_i
$$

In Einstein summation notation, this is written as:

$$
c = a_i b_i
$$

Here, the repeated index \(i\) implies summation over all values of \(i\).

### Generalization to Tensors

For tensors of higher rank, the same principle applies. Consider two tensors \(A\) and \(B\):

$$
A_{ij} B_{jk}
$$

This notation implies summation over the index \(j\), which is common to both tensors:

$$
C_{ik} = \sum_{j} A_{ij} B_{jk}
$$

Here, \(C\) is the resulting tensor after contraction over \(j\).

#### Example with Matrices

Matrix multiplication is a common example of Einstein summation notation. For matrices \(A \in \mathbb{R}^{m \times n}\) and \(B \in \mathbb{R}^{n \times p}\):

$$
C_{ik} = A_{ij} B_{jk}
$$

This is equivalent to:

$$
C_{ik} = \sum_{j=1}^{n} A_{ij} B_{jk}
$$

#### Higher-Dimensional Tensor Contraction

For higher-dimensional tensors, the notation helps simplify expressions considerably. Consider the contraction of two 3-dimensional tensors:

$$
A_{ijk} B_{klm}
$$

This implies summation over the index \(k\):

$$
C_{ijlm} = \sum_{k} A_{ijk} B_{klm}
$$

### Advantages

1. **Conciseness**: Reduces lengthy summation expressions to simple products.
2. **Clarity**: Makes the operations and transformations on tensors clearer and more intuitive.
3. **Reduction of Errors**: Minimizes the chance of error by reducing the complexity of the notation.

### Computational Implementation

Einstein summation notation can be implemented in computational tools such as NumPy, which provides the `einsum` function for this purpose.

Here is an example using Python and NumPy:

```python
import numpy as np

# Define tensors
A = np.random.rand(3, 4, 5)
B = np.random.rand(5, 6)

# Contract over the third index of A and the first index of B
C = np.einsum('ijk,km->ijm', A, B)

print(C.shape)  # Output will be (3, 4, 6)
```

In the `np.einsum` function, the string `'ijk,km->ijm'` specifies the indices to sum over and the desired output format.

### Formal Mathematical Context

#### Kronecker Delta

The Kronecker delta, \( \delta_{ij} \), plays a crucial role in Einstein summation notation, where:

$$
\delta_{ij} = \begin{cases}
1 & \text{if } i = j \\
0 & \text{if } i \neq j
\end{cases}
$$

It serves as an identity element in tensor operations. For example:

$$
A_{ik} \delta_{kj} = A_{ij}
$$

This effectively substitutes \(k\) with \(j\).

### Conclusion

Einstein summation notation is a powerful tool for simplifying tensor algebra, making complex operations more manageable and less error-prone. It is widely used in physics, engineering, and applied mathematics to streamline calculations and enhance the clarity of tensor manipulations. By understanding and utilizing this notation, one can more effectively work with high-dimensional data and complex mathematical models.


---

preceding: [[tensor contraction]]  


following: [[einops notation]]