---
date: 1717809735.476833
tags:
- autogenerated
- source__oai
- conversation_id__77e70e10-f02a-4f98-bd5c-6217b10122cf
- msg_7b784fc7-1663-4119-8235-e9e0df664a6c
- msg_aaa2be11-40dd-4a7f-9a13-367e90cc8962
- msg_c71c3e17-db7d-4b09-b434-de6da0f61d43
- colorclass/system-dynamics
title: hamilton-jacobi-bellman equation
---

**ASSISTANT**

The principle of optimality, introduced by Richard Bellman, is a fundamental concept in dynamic programming and [[optimal control theory]]. It states that an optimal policy has the property that, regardless of the initial state and initial decision, the remaining decisions must constitute an optimal policy with regard to the state resulting from the first decision. This principle provides the basis for solving complex optimization problems by breaking them down into simpler subproblems.

### Formal Statement

The principle of optimality can be formally stated as follows:

**Principle of Optimality**:
Given an optimal policy for a control process, the remaining part of the policy, from any intermediate state to the terminal state, must be optimal regardless of the initial state.

### Dynamic Programming

Dynamic programming is a method for solving optimization problems by recursively breaking them down into simpler subproblems. The principle of optimality underpins this approach. 

### Value Function and Bellman Equation

In dynamic programming, the value function \( V(x) \) represents the optimal cost-to-go from state \( x \). The Bellman equation is a recursive equation that defines the value function in terms of the value functions of subsequent states.

For a discrete-time system, the Bellman equation is:
\[ V(x_t) = \min_{u_t} \left[ L(x_t, u_t) + V(x_{t+1}) \right], \]
where:
- \( x_t \) is the state at time \( t \),
- \( u_t \) is the control action at time \( t \),
- \( L(x_t, u_t) \) is the cost incurred at time \( t \),
- \( x_{t+1} \) is the state at time \( t+1 \) resulting from \( x_t \) and \( u_t \).

### Continuous-Time Systems

For continuous-time systems, the Hamilton-Jacobi-Bellman (HJB) equation expresses the principle of optimality. The HJB equation for the value function \( V(x, t) \) is:
\[ \frac{\partial V}{\partial t} + \min_{u} \left[ L(x, u) + \nabla V \cdot f(x, u) \right] = 0, \]
where:
- \( \frac{\partial V}{\partial t} \) is the partial derivative of the value function with respect to time,
- \( \nabla V \) is the gradient of the value function with respect to the state,
- \( f(x, u) \) represents the system dynamics,
- \( L(x, u) \) is the running cost.

### Example: Shortest Path Problem

Consider a shortest path problem where the objective is to find the shortest path from an initial node to a destination node in a weighted graph.

- Let \( d(i) \) be the shortest distance from node \( i \) to the destination node.
- The principle of optimality states that if node \( k \) is on the shortest path from node \( i \) to the destination, then the path from node \( k \) to the destination must also be the shortest path from \( k \) to the destination.

Using the principle of optimality, the value function \( d(i) \) satisfies the recursive relation:
\[ d(i) = \min_{j \in \mathcal{N}(i)} \left[ c(i, j) + d(j) \right], \]
where \( \mathcal{N}(i) \) is the set of neighbors of node \( i \), and \( c(i, j) \) is the cost of the edge from node \( i \) to node \( j \).

### Applications

The principle of optimality and dynamic programming are applied in various fields, including:

- **Operations Research**: For solving resource allocation, scheduling, and inventory management problems.
- **Economics**: In modeling optimal investment and consumption strategies.
- **Robotics and Control Systems**: For path planning, trajectory optimization, and control of robotic systems.
- **Computer Science**: In algorithms for shortest paths, network flows, and other combinatorial optimization problems.

### Summary

The principle of optimality provides a powerful framework for solving complex optimization problems by breaking them down into simpler subproblems. It forms the basis of dynamic programming and is fundamental in deriving recursive equations such as the Bellman equation and the Hamilton-Jacobi-Bellman equation. This approach is widely used across various disciplines to find optimal policies and control strategies.


**USER**

Hamilton-Jacobi-Bellman equation


**ASSISTANT**

The Hamilton-Jacobi-Bellman (HJB) equation is a fundamental result in [[optimal control theory]]. It provides a necessary condition for optimality in continuous-time dynamic optimization problems. The HJB equation characterizes the value function, which represents the minimum cost-to-go from a given state to the final state.

### Problem Setup

Consider a dynamical system described by the differential equation:
\[ \dot{\mathbf{x}}(t) = \mathbf{f}(\mathbf{x}(t), \mathbf{u}(t)), \quad \mathbf{x}(t_0) = \mathbf{x}_0, \]
where:
- \(\mathbf{x}(t) \in \mathbb{R}^n\) is the state vector at time \(t\),
- \(\mathbf{u}(t) \in \mathbb{R}^m\) is the control vector at time \(t\),
- \(\mathbf{f}\) is a given function that describes the system dynamics.

The goal is to find the control \(\mathbf{u}(t)\) that minimizes the cost function:
\[ J(\mathbf{u}) = \phi(\mathbf{x}(t_f)) + \int_{t_0}^{t_f} L(\mathbf{x}(t), \mathbf{u}(t)) \, dt, \]
where:
- \(L(\mathbf{x}, \mathbf{u})\) is the running cost,
- \(\phi(\mathbf{x}(t_f))\) is the terminal cost.

### Value Function

The value function \(V(\mathbf{x}, t)\) represents the minimum cost-to-go from state \(\mathbf{x}\) at time \(t\) to the final time \(t_f\):
\[ V(\mathbf{x}, t) = \min_{\mathbf{u}(\cdot)} \left[ \phi(\mathbf{x}(t_f)) + \int_{t}^{t_f} L(\mathbf{x}(\tau), \mathbf{u}(\tau)) \, d\tau \right], \]
subject to the dynamics \(\dot{\mathbf{x}}(\tau) = \mathbf{f}(\mathbf{x}(\tau), \mathbf{u}(\tau))\).

### Hamilton-Jacobi-Bellman Equation

The HJB equation is a partial differential equation that the value function \(V(\mathbf{x}, t)\) satisfies. It is given by:
\[ \frac{\partial V}{\partial t} + \min_{\mathbf{u}} \left[ L(\mathbf{x}, \mathbf{u}) + \nabla V \cdot \mathbf{f}(\mathbf{x}, \mathbf{u}) \right] = 0, \]
with the boundary condition:
\[ V(\mathbf{x}, t_f) = \phi(\mathbf{x}(t_f)). \]

### Interpretation

- \(\frac{\partial V}{\partial t}\): Represents the rate of change of the value function with respect to time.
- \(\nabla V\): The gradient of the value function with respect to the state \(\mathbf{x}\).
- \(\nabla V \cdot \mathbf{f}(\mathbf{x}, \mathbf{u})\): Represents the directional derivative of \(V\) along the dynamics of the system.

The HJB equation states that the value function \(V(\mathbf{x}, t)\) evolves backward in time from the terminal cost \(\phi(\mathbf{x}(t_f))\).

### Solving the HJB Equation

Solving the HJB equation directly can be challenging due to its nonlinearity and high dimensionality. However, certain special cases and numerical methods can be used to find solutions.

#### Linear Quadratic Regulator (LQR)

For a linear-quadratic problem, the dynamics and cost are quadratic and linear, respectively. The system dynamics are:
\[ \dot{\mathbf{x}}(t) = A\mathbf{x}(t) + B\mathbf{u}(t), \]
and the cost function is:
\[ J = \mathbf{x}(t_f)^T F \mathbf{x}(t_f) + \int_{t_0}^{t_f} \left( \mathbf{x}(t)^T Q \mathbf{x}(t) + \mathbf{u}(t)^T R \mathbf{u}(t) \right) dt, \]
where \(A, B, Q, R,\) and \(F\) are matrices of appropriate dimensions.

In this case, the value function is quadratic:
\[ V(\mathbf{x}, t) = \mathbf{x}^T P(t) \mathbf{x}, \]
and the HJB equation reduces to the Riccati differential equation for \(P(t)\):
\[ -\dot{P}(t) = Q + A^T P(t) + P(t) A - P(t) B R^{-1} B^T P(t), \]
with terminal condition \(P(t_f) = F\).

The optimal control is:
\[ \mathbf{u}^*(t) = -R^{-1} B^T P(t) \mathbf{x}(t). \]

#### Numerical Methods

For more general problems, numerical methods such as finite difference methods, dynamic programming, and discretization techniques can be used to approximate the solution of the HJB equation.

### Example

Consider a simple example where the state dynamics are given by:
\[ \dot{x}(t) = u(t), \]
and the cost function is:
\[ J = x(t_f)^2 + \int_{0}^{t_f} (x(t)^2 + u(t)^2) \, dt. \]

The Hamiltonian is:
\[ H = x^2 + u^2 + \lambda u, \]
where \(\lambda\) is the costate variable. The optimal control \(u^*\) minimizes the Hamiltonian:
\[ u^* = -\frac{\lambda}{2}. \]

The costate dynamics are:
\[ \dot{\lambda} = -\frac{\partial H}{\partial x} = -2x. \]

By solving these equations along with the boundary conditions, we can determine the optimal control and the evolution of the state.

### Conclusion

The Hamilton-Jacobi-Bellman equation is a powerful tool in [[optimal control theory]], providing a necessary condition for optimality in continuous-time problems. While solving the HJB equation can be complex, it forms the basis for many control strategies and numerical methods used in practice. Understanding and applying the HJB equation allows for the design of optimal control laws that can be used in a wide range of applications, from engineering to economics.


---

preceding: [[principal of optimality]]  


following: [[linear quadratic regulator]]---
date: 1717269375.212564
tags:
- autogenerated
- source__oai
- conversation_id__44c599d0-b5a1-4f63-beba-a5fc36d87619
- msg_aaa270ba-4287-4de5-a9d4-89f9cc591319
- msg_c41161e4-3f36-4005-a5ee-e5e23409d509
title: hamilton-jacobi-bellman equation
---


**USER**

Hamilton-Jacobi-Bellman Equation


**ASSISTANT**

### Hamilton-Jacobi-Bellman Equation

The Hamilton-Jacobi-Bellman (HJB) equation is a partial differential equation used in optimal control theory to determine the optimal policy for a dynamic system. It is a central result in the theory of dynamic programming and is widely used in [[finance]], economics, and engineering to solve optimization problems where decisions are made over time under uncertainty.

#### Derivation and Formulation

The HJB equation provides a way to compute the value function \( V(W, t) \), which represents the maximum expected utility given the state variables (such as wealth \( W \)) and time \( t \). The value function is defined as:

$$ V(W, t) = \max_{\pi(\cdot), c(\cdot)} E \left[ \int_t^T e^{-\rho (s-t)} U(c(s)) \, ds + e^{-\rho (T-t)} U(W(T)) \mid W(t) = W \right] $$

where:
- \( \pi(\cdot) \) is the control variable representing the proportion of wealth invested in the risky asset.
- \( c(\cdot) \) is the control variable representing the consumption rate.
- \( \rho \) is the subjective discount rate.
- \( U(\cdot) \) is the utility function.

#### Dynamic Programming Principle

The principle of dynamic programming states that the value function at any time \( t \) can be decomposed into immediate rewards plus the discounted value of future rewards. This leads to the HJB equation, which, for a continuous-time optimization problem, can be expressed as:

$$ \rho V(W, t) = \max_{\pi, c} \left[ U(c) + V_W(W, t) \cdot \left( W(r + \pi (\mu - r)) - c \right) + \frac{1}{2} V_{WW}(W, t) \cdot (\pi W \sigma)^2 \right] $$

where:
- \( V_W \) is the partial derivative of \( V \) with respect to wealth \( W \).
- \( V_{WW} \) is the second partial derivative of \( V \) with respect to wealth \( W \).
- \( r \) is the risk-free rate.
- \( \mu \) is the expected return on the risky asset.
- \( \sigma \) is the volatility of the risky asset.

#### Solving the HJB Equation

To solve the HJB equation, we perform the following steps:

1. **Set Up the HJB Equation**: Write down the HJB equation based on the given optimization problem and the dynamics of the state variables.
2. **First-Order Conditions**: Differentiate the HJB equation with respect to the control variables \( \pi \) and \( c \), and set the derivatives to zero to find the optimal policies.
3. **Substitute Back**: Substitute the optimal policies back into the HJB equation to simplify it.
4. **Solve the PDE**: Solve the resulting partial differential equation for the value function \( V(W, t) \).

#### Example: Merton's Portfolio Problem

Consider Merton's portfolio problem with a CRRA utility function \( U(c) = \frac{c^{1-\gamma}}{1-\gamma} \). The HJB equation for this problem is:

$$ \rho V(W, t) = \max_{\pi, c} \left[ \frac{c^{1-\gamma}}{1-\gamma} + V_W(W, t) (W(r + \pi (\mu - r)) - c) + \frac{1}{2} V_{WW}(W, t) (\pi W \sigma)^2 \right] $$

**Step-by-Step Solution**:

1. **First-Order Conditions**:
   - For consumption \( c \):
     $$ \frac{\partial}{\partial c} \left( \frac{c^{1-\gamma}}{1-\gamma} + V_W(W, t) (-c) \right) = c^{-\gamma} - V_W(W, t) = 0 $$
     $$ c^* = \left( V_W(W, t) \right)^{-\frac{1}{\gamma}} $$

   - For portfolio choice \( \pi \):
     $$ \frac{\partial}{\partial \pi} \left( V_W(W, t) W \pi (\mu - r) + \frac{1}{2} V_{WW}(W, t) (\pi W \sigma)^2 \right) = V_W(W, t) W (\mu - r) + V_{WW}(W, t) (\pi W^2 \sigma^2) = 0 $$
     $$ \pi^* = \frac{\mu - r}{\gamma \sigma^2} $$

2. **Substitute Back**:
   Substitute \( c^* \) and \( \pi^* \) back into the HJB equation:

   $$ \rho V(W, t) = \frac{\left( (V_W(W, t))^{-\frac{1}{\gamma}} \right)^{1-\gamma}}{1-\gamma} + V_W(W, t) \left( W \left( r + \frac{\mu - r}{\gamma \sigma^2} (\mu - r) \right) - (V_W(W, t))^{-\frac{1}{\gamma}} \right) + \frac{1}{2} V_{WW}(W, t) \left( \frac{\mu - r}{\gamma \sigma^2} \right)^2 W^2 \sigma^2 $$

3. **Simplify and Solve**:
   Solve the resulting PDE for \( V(W, t) \). For CRRA utility and infinite horizon, the value function typically takes the form:

   $$ V(W) = \frac{W^{1-\gamma}}{1-\gamma} A $$

   where \( A \) is a constant derived from the parameters of the model.

### Summary

The Hamilton-Jacobi-Bellman (HJB) equation is a fundamental tool in optimal control theory, providing a method to determine the optimal policy for dynamic systems. In the context of [[finance]], it is used to solve problems like Merton's portfolio problem by deriving the value function and the optimal consumption and investment strategies. The HJB equation accounts for the trade-offs between immediate rewards and future benefits, leading to decisions that maximize expected utility over time. By solving the HJB equation, we obtain insights into how to manage wealth and consumption dynamically under uncertainty.


---

preceding: [[merton's portfolio problem]]  


following: [[wealth dynamics]]

See also: [[combinatorial optimization]]


See also: [[control theory]]


See also: [[dynamic programming]]


See also: [[inventory management]]


See also: [[resource allocation]]


See also: [[system dynamics]]