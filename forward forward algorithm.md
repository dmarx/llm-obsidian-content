---
date: 1716453802.389375
tags:
- autogenerated
- source__oai
- conversation_id__a2f90ccb-7613-4621-b4fa-d5fa52aa5ed3
- msg_aaa25e60-960b-413c-b122-4d2ef4b64a03
- msg_49dd1aef-09a8-4f75-b0a3-662c2b70eae1
title: forward forward algorithm
---


**USER**

forward forward algorithm


**ASSISTANT**

The **Forward-Forward Algorithm** is a fundamental method used in Hidden Markov Models (HMMs) to compute the probability of an observed sequence given the model parameters. It is especially useful for tasks in speech recognition, bioinformatics, and various sequence analysis applications. This algorithm is also referred to as the **Forward Algorithm**.

### Overview of Hidden Markov Models

A Hidden Markov Model is defined by:
1. **States**: \( S = \{ S_1, S_2, \ldots, S_N \} \)
2. **Observations**: \( O = \{ O_1, O_2, \ldots, O_T \} \)
3. **Transition Probabilities**: \( A = \{ a_{ij} \} \) where \( a_{ij} = P(S_j \mid S_i) \)
4. **Emission Probabilities**: \( B = \{ b_j(o_t) \} \) where \( b_j(o_t) = P(O_t = o_t \mid S_t = S_j) \)
5. **Initial State Probabilities**: \( \pi = \{ \pi_i \} \) where \( \pi_i = P(S_1 = S_i) \)

### Forward Algorithm

The goal is to compute the probability of an observation sequence \( O = (O_1, O_2, \ldots, O_T) \) given the HMM parameters. This is denoted as \( P(O \mid \lambda) \), where \( \lambda = (A, B, \pi) \) represents the model parameters.

#### Step-by-Step Process

1. **Initialization**:
   Define the forward probabilities \( \alpha_t(i) \) as the probability of observing the partial sequence \( O_1, O_2, \ldots, O_t \) and ending in state \( S_i \) at time \( t \). For the initial step \( t = 1 \):
   $$ \alpha_1(i) = \pi_i b_i(O_1) \quad \text{for } 1 \leq i \leq N $$

2. **Induction**:
   For \( t = 2 \) to \( T \), the forward probability is computed recursively as:
   $$ \alpha_t(j) = \left[ \sum_{i=1}^N \alpha_{t-1}(i) a_{ij} \right] b_j(O_t) \quad \text{for } 1 \leq j \leq N $$

3. **Termination**:
   The total probability of the observation sequence \( O \) is obtained by summing the forward probabilities at the final time step \( T \):
   $$ P(O \mid \lambda) = \sum_{i=1}^N \alpha_T(i) $$

### Detailed Mathematical Formalism

Let \( \alpha_t(i) \) be the forward variable representing the probability of the partial observation sequence \( (O_1, O_2, \ldots, O_t) \) and state \( S_i \) at time \( t \):
$$ \alpha_t(i) = P(O_1, O_2, \ldots, O_t, S_t = S_i \mid \lambda) $$

Using the above definitions, the algorithm's steps can be written more formally as:

1. **Initialization**:
   $$ \alpha_1(i) = \pi_i b_i(O_1) $$

2. **Induction**:
   $$ \alpha_t(j) = b_j(O_t) \sum_{i=1}^N \alpha_{t-1}(i) a_{ij} $$

3. **Termination**:
   $$ P(O \mid \lambda) = \sum_{i=1}^N \alpha_T(i) $$

### Example Application

Suppose we have an HMM with two states \( S = \{ S_1, S_2 \} \), two possible observations \( O = \{ O_1, O_2 \} \), and the following parameters:

- Transition matrix \( A \):
  $$ A = \begin{pmatrix}
  a_{11} & a_{12} \\
  a_{21} & a_{22}
  \end{pmatrix} $$
- Emission matrix \( B \):
  $$ B = \begin{pmatrix}
  b_1(O_1) & b_1(O_2) \\
  b_2(O_1) & b_2(O_2)
  \end{pmatrix} $$
- Initial probabilities \( \pi = (\pi_1, \pi_2) \)

Given an observation sequence \( O = (O_1, O_2) \), the steps would be:

1. **Initialization**:
   $$ \alpha_1(1) = \pi_1 b_1(O_1) $$
   $$ \alpha_1(2) = \pi_2 b_2(O_1) $$

2. **Induction**:
   $$ \alpha_2(1) = b_1(O_2) \left[ \alpha_1(1) a_{11} + \alpha_1(2) a_{21} \right] $$
   $$ \alpha_2(2) = b_2(O_2) \left[ \alpha_1(1) a_{12} + \alpha_1(2) a_{22} \right] $$

3. **Termination**:
   $$ P(O \mid \lambda) = \alpha_2(1) + \alpha_2(2) $$

This detailed approach ensures that we can efficiently compute the likelihood of any observation sequence given the parameters of the Hidden Markov Model. For further reading, see the entries on [[Hidden Markov Models]], [[Baum-Welch Algorithm]], and [[Viterbi Algorithm]].