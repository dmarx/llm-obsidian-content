---
date: 1746061534.777
tags:
- autogenerated
- source__oai
- conversation_id__6812c8de-e9cc-8008-aa1f-9f5153fd9d27
- msg_f7aa07bb-e842-4acf-8951-e48f403eb61e
- msg_04b93f3a-9806-4f0f-b92f-820c9056d3cc
- colorclass/decision-making-processes
title: on-policy vs off-policy rl
---

**USER**

on-policy vs off-policy rl


**ASSISTANT**

In [[Reinforcement Learning|reinforcement learning]] (RL), **on-policy** and **off-policy** learning refer to different approaches to how experience (state, action, reward, next state tuples) is generated and used for learning the value function or policy.

---

### **On-Policy RL**

An **on-policy** algorithm learns the value of the policy being carried out (i.e., *followed*) by the agent. More formally:

- The **behavior policy** $\mu$ (used to generate data) is the **same** as the **target policy** $\pi$ (the one being evaluated and improved).
- This generally implies that updates are based on the same policy that generates the trajectories.
- Exploration must be built into the policy itself (e.g., via $\epsilon$-greedy or softmax over action-values).
  
#### Examples:
- [[SARSA]]: State-Action-Reward-State-Action.
  
  Update rule:
  $$ Q(s_t, a_t) \leftarrow Q(s_t, a_t) + \alpha \left[ r_t + \gamma Q(s_{t+1}, a_{t+1}) - Q(s_t, a_t) \right] $$

  Here, the update depends on the action actually taken at $t+1$, i.e., under the same policy.

---

### **Off-Policy RL**

An **off-policy** algorithm evaluates and improves a policy $\pi$ that is **different** from the policy $\mu$ used to generate data.

- The **behavior policy** $\mu$ generates exploratory trajectories.
- The **target policy** $\pi$ (which might be deterministic or greedy) is the one being improved.
- This allows for the use of **experience replay** and **reusing old trajectories**, which can be more sample-efficient.

#### Examples:
- [[Q-learning]]:
  
  Update rule:
  $$ Q(s_t, a_t) \leftarrow Q(s_t, a_t) + \alpha \left[ r_t + \gamma \max_{a'} Q(s_{t+1}, a') - Q(s_t, a_t) \right] $$

  Note: $\max_{a'} Q(s_{t+1}, a')$ corresponds to acting *greedily* according to the target policy, even though the data was collected under $\mu$.

- [[Deep Q-Network|DQN]], [[Deep Deterministic Policy Gradient|DDPG]], [[Soft Actor-Critic|SAC]] are all off-policy methods.

---

### Key Differences Summary

| Aspect | On-Policy | Off-Policy |
|--------|-----------|------------|
| Data Generating Policy | Same as target policy ($\mu = \pi$) | Different from target policy ($\mu \ne \pi$) |
| Sample Efficiency | Lower | Higher (reuses past experience) |
| Exploration | Built into policy | Can be handled separately |
| Stability | Typically more stable | Often needs tricks (e.g., target networks, experience replay) |

---

Would you like to explore a visual comparison or dive into how these concepts manifest in actor-critic architectures?

See also: [[reinforcement learning]]