---
date: 1713148370.458911
tags:
- autogenerated
- source__oai
- conversation_id__a36ccd8c-cbfe-443d-b4bb-7c38f8db8e81
- msg_aaa26d55-7005-4b03-a559-942e0a2b46bc
- msg_fa006b38-e320-4b83-b491-3bbddeb42050
- colorclass/information-theory
title: classifier-free guidance
---

**USER**

classifier-free guidance


**ASSISTANT**

### Classifier-Free Guidance

Classifier-free guidance (CFG) is a technique used in generative models, particularly in the context of [[Generative Adversarial Networks (GANs)|Generative Adversarial Networks]] and [[Diffusion Models|diffusion models]], to enhance the control over the generated outputs without relying on a separate classifier during the generation process. This approach is particularly useful for conditional generation tasks, such as text-to-image generation or controlled text generation.

#### Conceptual Overview

In typical generative modeling, especially in models like GANs or diffusion models that are conditioned on specific attributes (e.g., a label denoting the class of an image), it is common to employ a classifier either during or post-training to ensure that the generated outputs match the desired conditions. However, integrating a classifier can introduce complexity and dependencies that might be undesirable in some use cases.

Classifier-free guidance proposes a way to bypass the need for a separate classifier by modifying the training process of the model itself. The idea is to train the model in such a way that it learns to both generate samples and conditionally generate samples without additional input beyond the standard noise vector and optional minimal conditioning.

#### Mathematical Formalism

In the training of a diffusion model, for example, the process involves learning a series of reverse diffusion steps that convert a sample from a noise distribution back to a data distribution. Let $x_0$ be a sample from the data distribution, and $x_T$ be a corresponding sample from the noise distribution after $T$ forward diffusion steps. The reverse diffusion process is modeled as:

$$
p_{\theta}(x_{t-1} | x_t) = \mathcal{N}(x_{t-1}; \mu_{\theta}(x_t, t), \Sigma_{\theta}(x_t, t))
$$

Where $\mu_{\theta}$ and $\Sigma_{\theta}$ are learned functions parameterized by $\theta$, and $t$ denotes the timestep.

Classifier-free guidance modifies this setup by occasionally replacing the conditioning information (e.g., class labels) with a null value during training. This forces the model to learn an internal representation that can interpolate or extrapolate the conditioning based on the learned distribution of the data and its characteristics. Essentially, the model is trained on a mixture of conditioned and unconditioned data, learning to generate both specific and non-specific outputs.

During generation, the strength of the conditioning can be controlled by a hyperparameter $\gamma$, which scales the influence of the conditional vector:

$$
x_{t-1} \sim p_{\theta}(x_{t-1} | \gamma \cdot c + x_t)
$$

Here, $c$ is the conditional vector (e.g., embedding of class labels), and $\gamma$ controls the extent to which the condition influences the reverse diffusion process. Higher values of $\gamma$ enforce stronger adherence to the specified condition, enhancing the model’s ability to generate data that closely aligns with the desired attributes.

#### Applications and Implications

Classifier-free guidance has broad implications for enhancing the flexibility and controllability of generative models:

1. **Increased Model Efficiency**: Eliminates the need for external classifiers or additional conditional input during generation, simplifying the model architecture and potentially reducing computational overhead.
2. **Enhanced Flexibility**: Offers a mechanism to adjust the specificity of generation through the $\gamma$ parameter, allowing for a spectrum of outputs from highly specific to more diverse and creative.
3. **Better Generalization**: By training on both conditioned and unconditioned data, models may better generalize to unseen data or conditions.

This approach is especially relevant in fields where generative models are crucial, such as in digital content creation, drug discovery, and other areas where tailored generation of complex data is beneficial. Classifier-free guidance thus represents a significant advancement in the pursuit of more controllable and efficient generative models.


---


following: [[classifier guidance in generative models]]---
date: '2025-06-22T07:36:11.907829Z'
tags:
- autogenerated
- source__claude
- conversation_id__8fcecd0f-193e-4b76-b760-649e0798cf8e
- msg_c455e3bd-5025-4dc6-9201-f340fd6860ce
- msg_21a8779a-1bca-4867-914c-f67b44371df0
title: classifier-free guidance
---


**USER**

classifier-free guidance


**ASSISTANT**

The user wants me to create a wiki entry about "classifier-free guidance" for their knowledge base. This is a technical concept in diffusion models and generative AI, particularly relevant to their background as an AI researcher and ML scientist.

I need to:
1. Write this as a standalone wiki page, not conversational
2. Use technical depth appropriate for someone with graduate degrees in applied math, statistics, and philosophy of science
3. Include mathematical formalisms
4. Use [[double brackets]] for novel terms that would be wiki links
5. Propose symbolic notation for modeling
6. Explain any physics notation in accessible mathematical terms
7. Keep it technical and detailed

Classifier-free guidance (CFG) is a technique used in diffusion models to control generation without requiring a separate classifier network. It was introduced by Ho & Salimans in 2021. The key idea is to train a single model that can perform both conditional and unconditional generation, then use the difference between these predictions to guide the generation process.

Let me structure this as a proper wiki entry with mathematical formalism.# Classifier-Free Guidance

**Classifier-Free Guidance** (CFG) is a [[conditioning mechanism]] for [[diffusion models]] that enables controlled generation without requiring a separate classifier network. Introduced by Ho & Salimans (2021), CFG trains a single [[denoising network]] to perform both conditional and unconditional generation, then leverages the difference between these predictions to guide the sampling process.

## Mathematical Formulation

Let $\epsilon_\theta(x_t, t, c)$ denote a [[denoising network]] parameterized by $\theta$, where:
- $x_t$ is the noisy input at timestep $t$
- $c$ represents the conditioning information
- $\emptyset$ denotes the null condition (unconditional case)

The **classifier-free guidance prediction** is defined as:

$$\tilde{\epsilon}_\theta(x_t, t, c) = \epsilon_\theta(x_t, t, \emptyset) + w \cdot [\epsilon_\theta(x_t, t, c) - \epsilon_\theta(x_t, t, \emptyset)]$$

where $w \geq 0$ is the [[guidance scale]] parameter.

This can be rewritten in the more compact form:
$$\tilde{\epsilon}_\theta(x_t, t, c) = (1 + w) \epsilon_\theta(x_t, t, c) - w \epsilon_\theta(x_t, t, \emptyset)$$

## Training Procedure

During training, the model learns to handle both conditional and unconditional scenarios through [[condition dropout]]. For each training sample with condition $c$, the model randomly:
- Uses the actual condition $c$ with probability $p_{\text{cond}}$
- Replaces $c$ with the null condition $\emptyset$ with probability $(1 - p_{\text{cond}})$

The training objective becomes:
$$\mathcal{L}_{\text{CFG}} = \mathbb{E}_{t, x_0, \epsilon, c} \left[ \| \epsilon - \epsilon_\theta(x_t, t, c') \|^2 \right]$$

where $c'$ is the randomly dropped condition and $x_t = \sqrt{\bar{\alpha}_t} x_0 + \sqrt{1 - \bar{\alpha}_t} \epsilon$.

## Geometric Interpretation

CFG can be understood as performing [[gradient ascent]] in the direction that increases [[conditional likelihood]] relative to [[unconditional likelihood]]. The guidance vector:

$$\nabla_x \log p(c|x) \propto \epsilon_\theta(x_t, t, \emptyset) - \epsilon_\theta(x_t, t, c)$$

represents the direction in which the conditional probability increases most rapidly.

## Guidance Scale Effects

The guidance scale $w$ controls the strength of conditioning:

- $w = 0$: Pure unconditional generation
- $w > 0$: Increasingly conditional generation
- $w \gg 1$: High fidelity to condition but potential [[mode collapse]]

Let $\mathcal{S}(w)$ denote the [[sample quality metric]] and $\mathcal{D}(w)$ the [[sample diversity metric]]. CFG exhibits the trade-off:

$$\frac{\partial \mathcal{S}}{\partial w} > 0, \quad \frac{\partial \mathcal{D}}{\partial w} < 0$$

for $w$ in a practical range.

## Implementation in Sampling

During [[DDPM sampling]] or [[DDIM sampling]], the guided noise prediction replaces the standard prediction:

```
For each timestep t in reverse:
    ε_uncond = model(x_t, t, null_condition)
    ε_cond = model(x_t, t, condition)
    ε_guided = ε_uncond + w * (ε_cond - ε_uncond)
    x_{t-1} = sampling_step(x_t, ε_guided, t)
```

## Theoretical Connections

CFG relates to several theoretical frameworks:

### Energy-Based Perspective
The guided prediction approximates sampling from an [[energy-based model]] with energy:
$$E(x, c) = E_{\text{uncond}}(x) - w \cdot \log p(c|x)$$

### Information Theory
The guidance strength $w$ can be interpreted as a [[Lagrange multiplier]] in a constrained optimization problem maximizing [[mutual information]] $I(X; C)$ subject to generation quality constraints.

### Variational Inference
CFG approximates the [[variational posterior]]:
$$q_w(x|c) \propto p(x)^{1-w} p(x|c)^w$$

## Extensions and Variants

### Multi-Conditional CFG
For multiple conditions $\{c_1, c_2, \ldots, c_k\}$:
$$\tilde{\epsilon} = \epsilon_\theta(x_t, t, \emptyset) + \sum_{i=1}^k w_i [\epsilon_\theta(x_t, t, c_i) - \epsilon_\theta(x_t, t, \emptyset)]$$

### Adaptive Guidance
The guidance scale can be made timestep-dependent: $w(t)$, often decreasing with $t$ to balance [[semantic control]] early and [[detail refinement]] late in generation.

### Compositional CFG
Multiple conditions can be combined compositionally:
$$\tilde{\epsilon} = \epsilon_\theta(x_t, t, \emptyset) + w_1 \Delta_1 + w_2 \Delta_2 + w_{12} (\Delta_{12} - \Delta_1 - \Delta_2)$$

where $\Delta_i = \epsilon_\theta(x_t, t, c_i) - \epsilon_\theta(x_t, t, \emptyset)$ and $\Delta_{12}$ represents the joint condition.

## Computational Considerations

CFG requires **two forward passes** per denoising step (conditional and unconditional), effectively doubling inference cost. Optimization strategies include:

- [[Batch parallelization]] of conditional/unconditional predictions
- [[Cached unconditional predictions]] for multiple conditional generations
- [[Approximate CFG]] using fewer guidance steps

## Related Concepts

- [[Classifier Guidance]]: Uses separate classifier gradients
- [[CLIP Guidance]]: Employs [[CLIP]] embeddings for text conditioning  
- [[ControlNet]]: Architectural approach to conditional generation
- [[Prompt Engineering]]: Optimization of conditioning inputs
- [[Negative Prompting]]: Using undesired conditions with negative weights


---


following: [[negative prompting]]

See also: [[generative models]]