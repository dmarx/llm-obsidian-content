---
date: 1721144167.38179
tags:
- autogenerated
- source__oai
- conversation_id__4f78839e-971a-4476-b886-e0eef3bd4caf
- msg_aaa25e80-0fc3-420f-b822-59f6c72d79b4
- msg_515f7cfe-84cf-4c66-8247-33c619910f6f
- colorclass/information-theory
title: uninformative priors
---

**USER**

uninformative priors


**ASSISTANT**

Uninformative priors, also known as non-informative or diffuse priors, are used in Bayesian statistics to reflect a state of minimal prior knowledge about the parameters. These priors aim to have as little influence as possible on the posterior distribution, allowing the data to play the primary role in shaping the inference.

## Characteristics and Types of Uninformative Priors

### Characteristics
- **Objective**: Uninformative priors are often designed to be objective, meaning they are not influenced by subjective beliefs or prior knowledge.
- **Invariance**: Many uninformative priors are invariant under reparameterization, ensuring that the choice of [[parameterization]] does not affect the prior.
- **Impropriety**: Some uninformative priors are improper, meaning they do not integrate to one. Despite this, they can still yield proper posterior distributions when combined with the likelihood.

### Common Types of Uninformative Priors

1. **Uniform Prior**: The simplest uninformative prior, where every possible value of the parameter is equally likely.
    - **Discrete Case**: For a parameter $\theta$ with possible values $\{\theta_1, \theta_2, \ldots, \theta_k\}$, the uniform prior is:
      $$
      \pi(\theta_i) = \frac{1}{k}, \quad i = 1, 2, \ldots, k
      $$
    - **Continuous Case**: For a continuous parameter $\theta \in [a, b]$, the uniform prior is:
      $$
      \pi(\theta) = \frac{1}{b-a}, \quad a \leq \theta \leq b
      $$
    - **Improper Uniform Prior**: For an unbounded parameter $\theta \in (-\infty, \infty)$, the prior is constant:
      $$
      \pi(\theta) \propto 1
      $$
      This is improper but can be useful in certain contexts.

2. **Jeffreys Prior**: Based on the Fisher information and is invariant under reparameterization. For a parameter $\theta$, it is defined as:
    $$
    \pi(\theta) \propto \sqrt{I(\theta)}
    $$
    where $I(\theta)$ is the Fisher information.

3. **Reference Priors**: Introduced by Jos√© M. Bernardo, these priors maximize the expected Kullback-Leibler divergence between the posterior and the prior, leading to priors that are often objective and minimally informative.

### Examples of Uninformative Priors in Specific Models

#### Binomial Distribution
- **Jeffreys Prior**: For the binomial proportion $\theta$, the Jeffreys prior is:
  $$
  \pi(\theta) \propto \frac{1}{\sqrt{\theta(1 - \theta)}}
  $$
  Leading to the posterior distribution:
  $$
  \pi(\theta \mid x, n) = \text{Beta}(\theta; x + \frac{1}{2}, n - x + \frac{1}{2})
  $$

- **Haldane Prior**: An alternative prior for the binomial proportion:
  $$
  \pi(\theta) \propto \frac{1}{\theta(1 - \theta)}
  $$
  This is also improper and often used in cases with extreme ignorance about $\theta$.

#### Normal Distribution
- **Mean with Known Variance**: For a normal distribution with known variance $\sigma^2$ and unknown mean $\mu$, the uninformative prior for $\mu$ is often taken as:
  $$
  \pi(\mu) \propto 1
  $$

- **Variance with Known Mean**: For a normal distribution with known mean $\mu$ and unknown variance $\sigma^2$, the uninformative prior for $\sigma^2$ is:
  $$
  \pi(\sigma^2) \propto \frac{1}{\sigma^2}
  $$
  This corresponds to a Jeffreys prior.

- **Both Mean and Variance Unknown**: The joint prior for $\mu$ and $\sigma^2$ is:
  $$
  \pi(\mu, \sigma^2) \propto \frac{1}{\sigma^2}
  $$

### Mathematical Formalization

The role of uninformative priors can be illustrated through Bayesian updating. Consider the likelihood function $L(\theta \mid x)$ and an uninformative prior $\pi(\theta)$. The posterior distribution is:

$$
\pi(\theta \mid x) \propto L(\theta \mid x) \pi(\theta)
$$

For example, in a normal distribution with known variance, the likelihood function is:

$$
L(\mu \mid x) = \exp\left( -\frac{1}{2\sigma^2} \sum_{i=1}^n (x_i - \mu)^2 \right)
$$

With a uniform prior $\pi(\mu) \propto 1$, the posterior is proportional to the likelihood:

$$
\pi(\mu \mid x) \propto \exp\left( -\frac{1}{2\sigma^2} \sum_{i=1}^n (x_i - \mu)^2 \right)
$$

This leads to a normal posterior distribution for $\mu$ with mean $\bar{x}$ and variance $\frac{\sigma^2}{n}$.

## Conclusion

Uninformative priors play a crucial role in Bayesian statistics, particularly when prior knowledge is minimal or when an objective analysis is desired. By minimizing the influence of prior assumptions, they allow the data to drive the inference process, making the resulting posterior distributions more reflective of the observed data.


---

preceding: [[jeffreys prior]]  


following: [[empirical bayes]]