---
date: 1717293946.705163
tags:
- autogenerated
- source__oai
- conversation_id__ca65c423-5293-48ca-a72e-29630dbc5375
- msg_aaa29707-d4f9-43a2-8cd7-a8b76219a398
- msg_649d3350-42d6-4d16-a969-5863f6177f2f
- colorclass/decision-making-processes
title: policy gradient theorem
---

**USER**

Policy Gradient Theorem


**ASSISTANT**

## Policy Gradient Theorem

### Overview

The Policy Gradient Theorem provides a theoretical foundation for policy gradient methods in [[reinforcement learning]] (RL). It expresses the gradient of the expected cumulative reward with respect to the policy parameters in a way that can be estimated and used to optimize the policy. This theorem is crucial for deriving algorithms that directly optimize the policy to maximize the expected return.

### Key Concepts

1. **Policy (\(\pi\))**: A mapping from states to actions, parameterized by \(\theta\), denoted as \(\pi_\theta(a|s)\).
2. **Expected Return (\(J(\theta)\))**: The objective function representing the expected cumulative reward.
3. **Gradient Ascent**: An optimization technique used to adjust policy parameters to maximize the expected return.

### Objective Function

The objective in policy gradient methods is to maximize the expected return, \(J(\theta)\), which can be formalized as:

\[
J(\theta) = \mathbb{E}_\pi [G_t] = \mathbb{E}_\pi \left[ \sum_{t=0}^\infty \gamma^t R(s_t, a_t) \right]
\]

where:
- \(G_t\) is the return (cumulative reward) starting from time step \(t\).
- \(\gamma\) is the discount factor.
- \(R(s_t, a_t)\) is the reward received after taking action \(a_t\) in state \(s_t\).

### Policy Gradient Theorem

The Policy Gradient Theorem provides an expression for the gradient of \(J(\theta)\) with respect to the policy parameters \(\theta\):

\[
\nabla_\theta J(\theta) = \mathbb{E}_\pi \left[ \nabla_\theta \log \pi_\theta(a|s) Q^\pi(s, a) \right]
\]

where:
- \(\nabla_\theta\) denotes the gradient with respect to \(\theta\).
- \(\pi_\theta(a|s)\) is the probability of taking action \(a\) in state \(s\) under policy \(\pi_\theta\).
- \(Q^\pi(s, a)\) is the action-value function, representing the expected return after taking action \(a\) in state \(s\) under policy \(\pi_\theta\).

### Derivation of the Policy Gradient Theorem

1. **Expected Return**:
   
   \[
   J(\theta) = \sum_s d^\pi(s) \sum_a \pi_\theta(a|s) Q^\pi(s, a)
   \]

   where \(d^\pi(s)\) is the stationary distribution of states under policy \(\pi_\theta\).

2. **Gradient of the Expected Return**:

   \[
   \nabla_\theta J(\theta) = \nabla_\theta \left( \sum_s d^\pi(s) \sum_a \pi_\theta(a|s) Q^\pi(s, a) \right)
   \]

3. **Applying the Log-Derivative Trick**:

   \[
   \nabla_\theta \pi_\theta(a|s) = \pi_\theta(a|s) \nabla_\theta \log \pi_\theta(a|s)
   \]

   Using this, we get:

   \[
   \nabla_\theta J(\theta) = \sum_s d^\pi(s) \sum_a \nabla_\theta \pi_\theta(a|s) Q^\pi(s, a)
   = \sum_s d^\pi(s) \sum_a \pi_\theta(a|s) \nabla_\theta \log \pi_\theta(a|s) Q^\pi(s, a)
   = \mathbb{E}_\pi \left[ \nabla_\theta \log \pi_\theta(a|s) Q^\pi(s, a) \right]
   \]

### Simplification Using Advantage Function

In practice, using the advantage function \(A^\pi(s, a) = Q^\pi(s, a) - V^\pi(s)\) instead of the Q-function often results in lower variance gradient estimates:

\[
\nabla_\theta J(\theta) = \mathbb{E}_\pi \left[ \nabla_\theta \log \pi_\theta(a|s) A^\pi(s, a) \right]
\]

### Practical Implementation

#### REINFORCE Algorithm

The REINFORCE algorithm is a simple Monte Carlo policy gradient method that follows from the Policy Gradient Theorem:

1. **Sample Trajectories**: Generate trajectories using the current policy.
2. **Compute Returns**: Calculate the return \(G_t\) for each time step \(t\).
3. **Update Policy**: Update the policy parameters \(\theta\) using the gradient estimate.

The policy update rule is:

\[
\theta \leftarrow \theta + \alpha \sum_{t=0}^\infty \nabla_\theta \log \pi_\theta(a_t | s_t) G_t
\]

where \(\alpha\) is the learning rate.

#### Example: REINFORCE Algorithm in Python

```python
import numpy as np
import gym

env = gym.make('CartPole-v1')
n_actions = env.action_space.n
state_dim = env.observation_space.shape[0]

class PolicyNetwork:
    def __init__(self, state_dim, n_actions):
        self.state_dim = state_dim
        self.n_actions = n_actions
        self.theta = np.random.rand(state_dim, n_actions)

    def policy(self, state):
        z = state.dot(self.theta)
        exp = np.exp(z)
        return exp / np.sum(exp)

    def get_action(self, state):
        probs = self.policy(state)
        return np.random.choice(self.n_actions, p=probs)

    def update(self, grad_log_probs, returns, alpha):
        for grad, G in zip(grad_log_probs, returns):
            self.theta += alpha * grad * G

policy_net = PolicyNetwork(state_dim, n_actions)
alpha = 0.01
gamma = 0.99

def compute_returns(rewards, gamma):
    returns = []
    G = 0
    for r in reversed(rewards):
        G = r + gamma * G
        returns.insert(0, G)
    return returns

for episode in range(1000):
    state = env.reset()
    states = []
    actions = []
    rewards = []
    grad_log_probs = []
    
    done = False
    while not done:
        action = policy_net.get_action(state)
        next_state, reward, done, _ = env.step(action)
        
        states.append(state)
        actions.append(action)
        rewards.append(reward)
        
        probs = policy_net.policy(state)
        grad_log_prob = -probs
        grad_log_prob[action] += 1
        grad_log_probs.append(grad_log_prob)
        
        state = next_state
    
    returns = compute_returns(rewards, gamma)
    policy_net.update(grad_log_probs, returns, alpha)

    if episode % 100 == 0:
        print(f'Episode {episode}: Total Reward: {sum(rewards)}')
```

### Applications of Policy Gradient Methods

1. **Robotics**: Training robots for manipulation, navigation, and control tasks.
2. **Game AI**: Developing strategies for complex games like Go, chess, and video games.
3. **Finance**: Optimizing trading strategies and portfolio management.
4. **Healthcare**: Personalizing treatment plans and optimizing drug discovery.
5. **Recommendation Systems**: Improving user recommendations and content personalization.

### Conclusion

The Policy Gradient Theorem provides a fundamental basis for policy gradient methods in [[reinforcement learning]]. By deriving the gradient of the expected return with respect to policy parameters, it enables the development of algorithms that directly optimize policies. Implementing these methods involves leveraging the theorem to derive practical algorithms like REINFORCE and actor-critic methods, which can handle complex, high-dimensional action spaces and learn sophisticated policies for a wide range of applications.


---

preceding: [[policy gradient methods in [[reinforcement learning]]]]  


following: [[actor-critic methods in [[reinforcement learning]]]]

See also: [[gradient]]