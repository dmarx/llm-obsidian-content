---
date: 1716334762.228001
tags:
- autogenerated
- source__oai
- conversation_id__888d597c-ab09-4e8c-ae14-867b391cfac3
- msg_aaa2aeb5-a576-4c49-9db4-02c57a837ea1
- msg_9fbe7a81-26c9-45ae-8d00-f7967de47dbf
- colorclass/information-theory
title: hartley function
---

**USER**

hartley function


**ASSISTANT**

The Hartley function, named after Ralph Hartley, is a measure of information that quantifies the uncertainty or information content of a uniformly distributed random variable. It is one of the earliest measures of information and is particularly useful when dealing with uniform distributions. The Hartley function is also known as the Hartley entropy or max-entropy.

### Definition

For a set \(X\) with \(N\) equally likely outcomes, the Hartley function \(H_0(X)\) is defined as:

$$
H_0(X) = \log N,
$$

where the base of the logarithm can be chosen according to the context:
- Base 2: \(H_0(X) = \log_2 N\), giving the information content in bits.
- Base \(e\): \(H_0(X) = \ln N\), giving the information content in nats.
- Base 10: \(H_0(X) = \log_{10} N\), giving the information content in hartleys or dits.

### Properties

1. **Non-negativity**: \(H_0(X) \geq 0\), with \(H_0(X) = 0\) if and only if \(N = 1\).
2. **Additivity**: For independent sets \(X\) and \(Y\), the Hartley function is additive:
   $$
   H_0(X \times Y) = H_0(X) + H_0(Y).
   $$
3. **Uniform Distribution**: The Hartley function assumes that all outcomes are equally likely, making it a measure of the maximum possible uncertainty.

### Relationship to Other Entropy Measures

- **Shannon Entropy**: The Hartley function can be seen as a special case of the Shannon entropy when all probabilities are equal:
  $$
  H(X) = -\sum_{i=1}^N p_i \log p_i,
  $$
  where \(p_i = \frac{1}{N}\). In this case, the Shannon entropy simplifies to:
  $$
  H(X) = -\sum_{i=1}^N \frac{1}{N} \log \frac{1}{N} = \log N = H_0(X).
  $$

- **Rényi Entropy**: The Hartley function is the Rényi entropy of order 0:
  $$
  H_0(P) = \frac{1}{1-0} \log \left( \sum_{i=1}^N p_i^0 \right) = \log N,
  $$
  where \(N\) is the number of non-zero probabilities \(p_i\).

### Applications

#### Information Theory

The Hartley function provides a simple measure of information content for systems with equally likely outcomes. It is useful in understanding the fundamental limits of data compression and transmission.

#### Combinatorics

In combinatorics, the Hartley function helps quantify the number of possible outcomes in a set. For example, it can be used to calculate the number of possible configurations or permutations of a system.

#### Cryptography

In cryptography, the Hartley function is used to measure the size of the key space. The security of a cryptographic system can be assessed by the information content of its keys, with larger values indicating more secure systems.

### Example

Consider a set \(X\) with \(N = 8\) equally likely outcomes. The Hartley function in different bases is:

- **Base 2**:
  $$
  H_0(X) = \log_2 8 = 3 \text{ bits}.
  $$

- **Base \(e\)**:
  $$
  H_0(X) = \ln 8 \approx 2.079 \text{ nats}.
  $$

- **Base 10**:
  $$
  H_0(X) = \log_{10} 8 \approx 0.903 \text{ hartleys or dits}.
  $$

### Conclusion

The Hartley function is a foundational concept in information theory that measures the information content of a set with equally likely outcomes. It is particularly useful for understanding the maximum possible uncertainty in a uniform distribution and serves as a special case of more general entropy measures like Shannon and Rényi entropy. Its simplicity and intuitive appeal make it a valuable tool in various fields, including information theory, combinatorics, and cryptography.

For further reading, you may refer to the following dedicated pages:
- [[Hartley Function]]
- [[Shannon Entropy]]
- [[Rényi Entropy]]
- [[Combinatorics]]
- [[Cryptography]]
- [[Information Theory]]


---

preceding: [[renyi entropy]]  


following: [[perplexity]]

See also: [[information theory]]