---
date: 1713206125.246259
tags:
- autogenerated
- source__oai
- conversation_id__1285e516-a4c3-4f7f-b1ef-72ddedeebeb0
- msg_aaa22b67-02fa-43f9-a366-03e704c8e84b
- msg_a47742e4-90fb-4fb3-b07c-e04632f2a994
- colorclass/vector-space
title: positive semidefinite matrices overview
---

**USER**

Positive Semidefinite


**ASSISTANT**

### Positive Semidefinite Matrices Overview

Positive semidefinite (PSD) matrices are a critical concept in linear algebra, particularly in the fields of optimization, statistics, and machine learning. These matrices have non-negative eigenvalues, a property that leads to various useful mathematical and physical interpretations, especially in systems where notions of distance, variance, and energy must be non-negative.

### Definition

A square matrix $A$ is called positive semidefinite if for all vectors $x \neq 0$, the following condition holds:

$$
x^T A x \geq 0
$$

This definition implies that when $x$ is transposed and multiplied by $A$ and then by $x$ again, the result is always non-negative.

### Properties of Positive Semidefinite Matrices

1. **Eigenvalues**: All eigenvalues of a PSD matrix are non-negative. This is a direct consequence of the definition since the quadratic form $x^T A x$ can be expressed in terms of the eigenvalues $\lambda_i$ of $A$, leading to $x^T A x = \sum \lambda_i x_i^2 \geq 0$.
2. **Symmetry**: Every PSD matrix is symmetric, i.e., $A = A^T$. This symmetry is essential for many of the properties and applications of PSD matrices, including spectral decomposition.
3. **Cholesky Decomposition**: If a matrix is positive semidefinite, it can be decomposed into $A = LL^T$ where $L$ is a lower triangular matrix. This decomposition is unique if $A$ is also positive definite.
4. **Schur Complement**: For a block matrix to be positive semidefinite, its Schur complement must also be positive semidefinite.

### Applications

- **[[Statistics]]**: Covariance matrices are PSD because variance and covariance are inherently non-negative.
- **[[Optimization]]**: Many optimization problems require solving conditions where the Hessian matrix (second derivative matrix of a function) is PSD, ensuring that a function is convex.
- **[[Quantum Mechanics]]**: Density matrices in quantum mechanics are PSD, reflecting the probability distributions inherent in quantum states.
- **[[Machine Learning]]**: Kernel matrices in support vector machines and other kernel-based methods are PSD, corresponding to the idea that these matrices represent inner products in some feature space.

### Theoretical Implications and Usage

- **Positive Definite Matrices**: A stronger condition than positive semidefiniteness is positive definiteness, where $x^T A x > 0$ for all $x \neq 0$. This implies that all eigenvalues are strictly positive.
- **Semidefinite Programming**: PSD matrices are at the heart of semidefinite programming, a subfield of optimization where one maximizes or minimizes a linear objective function subject to the constraint that a linear matrix inequality involving a PSD matrix holds.

### Mathematical Representation

Consider a matrix $A$ in $\mathbb{R}^{n \times n}$, the quadratic form associated with $A$ is given by:

$$
x^T A x = \sum_{i=1}^n \sum_{j=1}^n a_{ij} x_i x_j
$$

To ensure $A$ is PSD, it must satisfy the condition that this sum is non-negative for all non-zero $x \in \mathbb{R}^n$.

### Conclusion

Positive semidefinite matrices encapsulate an essential concept of non-negativity in quadratic forms and have profound implications across mathematics, physics, and engineering. Their structural properties and constraints are crucial in ensuring the stability and feasibility of solutions in various mathematical and engineering contexts.


---

preceding: [[covariance matrix overview]]  


following: [[space of positive semidefinite matrices overview]]

See also: [[support vector machines]]


See also: [[quadratic forms]]


See also: [[space of positive semidefinite matrices overview]]