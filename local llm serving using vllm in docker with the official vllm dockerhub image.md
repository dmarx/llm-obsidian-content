---
date: 1726444053.142644
tags:
- autogenerated
- source__oai
- conversation_id__66e757d1-0534-8008-bb6a-afa8342b5ee2
- msg_fa85aa95-a02e-4d39-a7c8-f338fb1088ff
- msg_aaa2d50a-5ea2-4b79-a179-9c1035cc7652
- msg_536b0205-2367-41b1-a55c-b31ee500a175
- colorclass/target-audience
title: local llm serving using vllm in docker with the official vllm dockerhub image
---

**ASSISTANT**

it possible to train on smaller hardware setups.

---

### **Learning Outcomes**:

1. **Understanding LLM Data Formats**: Students will become familiar with common data formats used for training and fine-tuning large language models.
   
2. **Applying LoRA**: By applying LoRA, students will learn how to fine-tune large models efficiently without needing extensive computational resources.
   
3. **Leveraging Hugging Face**: The assignment introduces students to the Hugging Face ecosystem, teaching them how to work with **datasets**, **pre-trained models**, and **parameter-efficient fine-tuning** techniques.
   
4. **Practical Customization**: Students will learn how to customize pre-trained models for specific tasks, such as instruction-following or text generation, making it applicable to real-world scenarios.

---

### **Assignment Extension Ideas**:

1. **Multi-Task Fine-Tuning with LoRA**: Fine-tune the model on multiple tasks (e.g., translation, summarization) using different LoRA configurations, allowing a single model to perform multiple tasks efficiently.
   
2. **Custom Dataset Creation**: Create a custom dataset for a specific domain (e.g., legal, medical) and fine-tune a large model with LoRA for domain-specific tasks.
   
3. **Low-Rank Adaptation on Larger Models**: Apply LoRA to models larger than GPT-2 (e.g., GPT-3 or T5) to experiment with scaling and fine-tuning large-scale language models.

---

### **Summary**:

This assignment covers two critical aspects of modern LLM development: understanding common training data formats and applying **LoRA** (Low-Rank Adaptation) to fine-tune large models using the Hugging Face ecosystem. By learning how to apply LoRA, students can efficiently fine-tune large models without requiring massive computational resources, making it practical for various specialized tasks such as instruction-following, dialogue generation, and domain-specific applications.


**USER**

local llm serving using vllm in docker with the official vllm dockerhub image


**ASSISTANT**

### Serving Local LLMs with vLLM in Docker Using the Official DockerHub Image

In this assignment, we'll walk through the process of deploying a **local large language model (LLM)** using **vLLM**—an optimized inference system for fast and efficient model serving. The goal is to serve the LLM using **vLLM** in a **Docker container**, utilizing the official vLLM image from DockerHub.

### **Objective**:
1. Learn how to set up and serve a large language model (LLM) locally using **vLLM**.
2. Use Docker to create a containerized environment for efficient and scalable model serving.
3. Deploy the LLM using the **official vLLM image** from DockerHub, minimizing the complexity of environment setup.

---

### **Overview of vLLM**

**vLLM** is a highly optimized serving framework for large language models. It uses techniques like efficient memory management and GPU utilization to accelerate LLM inference. This makes it ideal for deploying LLMs locally with performance optimizations suitable for modern hardware.

Key features of **vLLM** include:
- **Memory-efficient inference**: By optimizing how memory is shared and reused across batches, vLLM can serve larger models or more requests with fewer resources.
- **Scalability**: It allows for high-performance inference, especially when serving LLMs on modern GPUs.

---

### **Step 1: Installing Docker**

Before we start, ensure you have **Docker** installed on your machine. If Docker is not installed, follow these instructions:

- **For Linux**:
  ```bash
  sudo apt-get update
  sudo apt-get install docker.io
  sudo systemctl start docker
  sudo systemctl enable docker
  ```

- **For macOS and Windows**: Download and install Docker Desktop from the official [Docker website](https://www.docker.com/products/docker-desktop).

To verify Docker is installed correctly, run:

```bash
docker --version
```

---

### **Step 2: Pull the Official vLLM Docker Image**

The official vLLM Docker image is available on **DockerHub**. We will pull this image to set up the environment for serving an LLM.

1. Open your terminal and run the following command to pull the official **vLLM** image:

   ```bash
   docker pull vllmorg/vllm:latest
   ```

   This will download the latest version of vLLM with all necessary dependencies for serving LLMs.

---

### **Step 3: Preparing a Model**

vLLM supports a wide range of models from the Hugging Face model hub. You can serve any pre-trained model available on Hugging Face, such as **GPT-2**, **GPT-3**, or **LLAMA**.

For this guide, let's serve a **GPT-based model** from Hugging Face.

We’ll specify the model by setting an environment variable inside Docker, allowing the container to download and serve the model.

---

### **Step 4: Run vLLM in Docker with the Model**

To start serving the model, you need to:

1. **Run the vLLM Docker container** and specify the model you want to serve using an environment variable (`MODEL_ID`). We will also map a port from the container to the host system so that you can access the API locally.

2. **Model URL**: We will use **gpt2** as an example, but you can replace it with any model from Hugging Face.

Run the following command to start the Docker container with vLLM:

```bash
docker run -it --gpus all -p 8000:8000 \
  -e MODEL_ID=gpt2 \
  vllmorg/vllm:latest \
  python3 -m vllm.entrypoints.api_server --model $MODEL_ID
```

### **Explanation of the Command**:
- `docker run -it`: Runs the Docker container in interactive mode.
- `--gpus all`: Enables access to the GPU inside the container (necessary for fast inference).
- `-p 8000:8000`: Maps port 8000 from the container to port 8000 on your host machine so you can access the API.
- `-e MODEL_ID=gpt2`: Sets the environment variable `MODEL_ID` to `gpt2`, which tells vLLM which model to serve.
- `vllmorg/vllm:latest`: Uses the latest vLLM image from DockerHub.
- `python3 -m vllm.entrypoints.api_server --model $MODEL_ID`: Starts the vLLM server using the specified model.

This command will:
- Download the **GPT-2** model from Hugging Face.
- Start the vLLM API server to handle inference requests.

Once the model is loaded, the API will be available at `http://localhost:8000`.

---

### **Step 5: Sending Requests to the Model**

After starting the vLLM server, you can send requests to the model using HTTP requests. Here’s an example using **cURL** to interact with the model.

To send a prompt and get a response:

```bash
curl -X POST http://localhost:8000/generate \
     -H 'Content-Type: application/json' \
     -d '{"prompt": "The future of AI is", "max_tokens": 50}'
```

This request sends the prompt **"The future of AI is"** to the model, and the model will generate up to **50 tokens** in response.

### **Expected Response**:
The server will respond with a generated continuation of the text, something like:

```json
{
  "generated_text": "The future of AI is bright, with advancements in machine learning, deep learning, and natural language processing..."
}
```

---

### **Step 6: Customizing the Model Serving**

You can adjust the following options to customize the model serving:
- **MODEL_ID**: Set this to any model hosted on Hugging Face (e.g., `gpt2-large`, `gpt-j`, `bigscience/bloom`).
- **max_tokens**: Adjust the number of tokens generated by the model.
- **temperature**: Control the randomness of the output by adjusting the temperature parameter (default is `1.0`).

To change the temperature, use the following `cURL` command:

```bash
curl -X POST http://localhost:8000/generate \
     -H 'Content-Type: application/json' \
     -d '{"prompt": "The future of AI is", "max_tokens": 50, "temperature": 0.7}'
```

This will generate less random (but more deterministic) responses.

---

### **Step 7: Stopping and Managing the Docker Container**

To stop the Docker container running vLLM, press `Ctrl + C` in the terminal where the container is running, or find the container's ID using:

```bash
docker ps
```

Then stop the container with:

```bash
docker stop <container_id>
```

To restart the container later, simply run the same `docker run` command as in **Step 4**.

---

### **Advantages of Using vLLM with Docker**

1. **Easy Setup**: The official vLLM Docker image simplifies the environment setup, bundling all dependencies and libraries needed to run LLMs efficiently.
2. **GPU Acceleration**: Using Docker with `--gpus all` enables fast inference on NVIDIA GPUs.
3. **Scalability**: You can deploy multiple instances of the vLLM container on different machines or with different models for scalability.
4. **Isolation**: Docker provides isolation, ensuring that the environment is consistent across different systems and eliminating dependency conflicts.

---

### **Learning Outcomes**:

1. **Understanding vLLM**: Students will learn how to serve large language models efficiently using the vLLM framework, which optimizes memory and GPU use.
   
2. **Containerization with Docker**: By using Docker, students will gain hands-on experience in deploying containerized AI applications, a crucial skill for production environments.
   
3. **API Interaction**: Students will learn how to interact with a served LLM through an API, sending requests and receiving generated text using tools like cURL.
   
4. **Customizable Inference**: Students will explore how to customize LLM inference by adjusting parameters such as temperature and token limits to control the model's behavior.

---

### **Assignment Extension Ideas**:

1. **Deploying Multiple Models**: Extend the assignment by serving multiple models concurrently, each on different ports, and experiment with model switching.
   
2. **Scaling with Kubernetes**: Deploy the vLLM container in a Kubernetes cluster, enabling scalable serving of LLMs across multiple nodes.
   
3. **Integrating with Other APIs**: Integrate the vLLM API with other services (e.g., a chatbot frontend, or a web application) to create a full-stack AI service.

---

### **Summary**:

In this assignment, we explored how to serve large language models locally using **vLLM** in Docker. By pulling the official vLLM Docker image, configuring the model, and deploying it via an API, students learn how to efficiently serve LLMs with minimal setup using containerization techniques. Through this process, students gain valuable experience with containerized AI deployment, API interaction, and optimizing inference for real-world applications.


---

preceding: [[training custom lora with hugging face overview of common llm training data formats]]  


following: [[tool use in large language models (llms)]]