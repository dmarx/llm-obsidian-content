---
date: 1713750480.932511
tags:
- autogenerated
- source__oai
- conversation_id__5105adc8-e608-40d7-b875-84f3846ef325
- msg_aaa271c3-566f-495d-9cd7-fc848be65241
- msg_808f9bd7-2a27-40b2-b1a0-aa0dc01d3571
- colorclass/vector-space
title: how do positional embeddings work what is their purpose how do they satisfy
  that purpose what's the problem they solve feel free to treat this as a single question
  rather than several. i'd prefer a un
---

**USER**

how do [[positional embeddings]] work? what is their purpose? how do they satisfy that purpose? what's the problem they solve? feel free to treat this as a single question rather than several. I'd prefer a unified/synthesized answer tbh. pretend this is one question.


**ASSISTANT**

Positional embeddings are a technique used in models that process sequential data, particularly in the context of the Transformer architecture, to encode the order or position of elements within a sequence. Their primary purpose is to provide the model with information about the relative or absolute positions of tokens in a sequence, which is essential for understanding context and dependencies within the data.

### Purpose and Functionality

The core problem that [[positional embeddings]] solve is the lack of inherent sequential processing capability in models like the Transformer. Unlike recurrent neural networks (RNNs) or long short-term memory networks (LSTMs), which process sequences step by step and inherently capture positional information, Transformers process all tokens simultaneously. This parallel processing significantly speeds up training and handling of sequences but fails to natively account for the order of tokens, which is crucial for tasks such as language understanding, time series analysis, and any other domain where the sequence order impacts meaning.

### How Positional Embeddings Satisfy Their Purpose

Positional embeddings reintroduce the necessary sequential or order information by explicitly encoding it into the input representations. They are typically added to the token embeddings before feeding them into the Transformer layers. This addition ensures that each token's representation is not just about the token itself but also its position in the sequence.

There are mainly two types of [[positional embeddings]]:

1. **Learned Positional Embeddings**: These embeddings are parameters learned by the model during the training process. Each position in a sequence has a corresponding embedding vector, which is optimized alongside other model parameters to minimize the training loss. The advantage of learned embeddings is that they can potentially adapt to specific patterns of positional relevance dictated by the training data.

2. **Fixed Positional Embeddings**: Often implemented as sinusoidal functions based on the work in the original Transformer paper, these embeddings use sine and cosine functions of different frequencies to encode each position. The formulae for these embeddings are as follows:

    $$
    PE(pos, 2i) = \sin(pos / 10000^{2i/d_{model}})
    $$
    $$
    PE(pos, 2i+1) = \cos(pos / 10000^{2i/d_{model}})
    $$

    where $pos$ is the position, $i$ is the dimension index, and $d_{model}$ is the dimensionality of the embedding. The use of sinusoidal functions allows these embeddings to model positional relationships and enables the model to extrapolate to sequence lengths greater than those encountered during training.

### Advantages and Impact

By integrating [[positional embeddings]], Transformers can effectively model the dependencies and relationships that are influenced by the sequence order, crucial for understanding structured data in NLP, audio signal processing, and other sequential processing tasks. This method maintains the model's parallelizability and scalability while providing the necessary contextual cues that are dependent on the sequence's structure.

Overall, [[positional embeddings]] are a clever and effective solution to the problem of sequence order in models that handle all elements of the sequence in parallel, ensuring that despite the lack of inherent sequential processing, these models can still perform tasks where order and context matter significantly.


---

preceding: [[random projections]]  


following: [[neural tangent kernel]]

See also: [[signal processing]]


See also: [[embedding]]


See also: [[positional embeddings]]