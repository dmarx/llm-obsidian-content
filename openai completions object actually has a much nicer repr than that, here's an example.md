---
date: 1726440126.372016
tags:
- autogenerated
- source__oai
- conversation_id__66e757d1-0534-8008-bb6a-afa8342b5ee2
- msg_aaa252ee-0e56-4a59-a883-38e1ca91a429
- msg_050d3232-e07a-48c0-be42-d0b6a80a9427
- autogenerated
- source__oai
- conversation_id__66e757d1-0534-8008-bb6a-afa8342b5ee2
- msg_aaa215ae-d52f-40bb-99da-d8137f4feb22
- msg_4d14c3f9-8ef6-48a0-a1e9-1be43daa12ea
- colorclass/edge-case
title: openai completions object actually has a much nicer repr than that, here's
  an example
---

**USER**

the openai completions object actually has a much nicer repr than that, here's an example:

```python
Completion(id='cmpl-b28004d8793d4e29a0d3801130850c92', choices=[CompletionChoice(finish_reason='length', index=0, logprobs=None, text=' 1 2 1 2\n\nI’m here to talk about the importance of mental health and self-care in the music industry. As a musician, I know firsthand how demanding and stressful the industry can be. Long', stop_reason=None, prompt_logprobs=None)], created=1726436818, model='mistralai/Mistral-7B-Instruct-v0.2', object='text_completion', system_fingerprint=None, usage=CompletionUsage(completion_tokens=50, prompt_tokens=3, total_tokens=53))
```

something to consider moving forward.


**ASSISTANT**

You're right! The `openai.Completion` object does indeed have a much more informative and readable `repr` than simply printing the object as raw data. Since it provides a clear, structured view of the response with detailed attributes like `id`, `choices`, `usage`, etc., we can definitely take advantage of this in future assignments. This will give students more meaningful feedback without the need for immediate parsing.

Here’s how we can adjust the way we print the responses moving forward, showing students how they can access this rich representation.

---

### **Adjustment: Displaying the `Completion` Object in a User-Friendly Way**

Rather than just printing the raw `Completion` object, let’s encourage students to inspect the object directly using its built-in representation (which includes attributes like `choices`, `model`,


---

preceding: [[encourage the student to wrap conditions in parens to mitigate confusion about order of operations. we're not going to discuss order of operations so may as well introduce a mechanism to make it less]]  


following: [[openai completions object actually has a much nicer repr than that, here's an example]]


---

**USER**

the openai completions object actually has a much nicer repr than that, here's an example:

```python
Completion(id='cmpl-b28004d8793d4e29a0d3801130850c92', choices=[CompletionChoice(finish_reason='length', index=0, logprobs=None, text=' 1 2 1 2\n\nI’m here to talk about the importance of mental health and self-care in the music industry. As a musician, I know firsthand how demanding and stressful the industry can be. Long', stop_reason=None, prompt_logprobs=None)], created=1726436818, model='mistralai/Mistral-7B-Instruct-v0.2', object='text_completion', system_fingerprint=None, usage=CompletionUsage(completion_tokens=50, prompt_tokens=3, total_tokens=53))
```

something to consider moving forward.

next, let's introduce jinja templating. start from working through a non-trivial task that would be well suited to templating, but limiting yourself to the tools the student already has. the point is to demonstrate that it is a thing that they already can do. next, we introduce jinja tempating and discuss context in which it might make the learner's life easier as an llm practitioner


**ASSISTANT**

That’s a great idea! We’ll start by having students work through a **non-trivial task** using only the tools they’ve already learned, specifically string manipulation with `str.replace()` and conditional control flow. Afterward, we’ll introduce **Jinja templating** to show how it simplifies tasks like prompt engineering, which involve dynamically generating text with multiple placeholders or more complex formatting logic.

---

### **Seventh Assignment: Demonstrating the Power of Templating**

### **Objective**:
1. Guide students through a more complex string substitution task, showing them how to handle multiple placeholders using tools they already know (`str.replace()` and conditional logic).
2. Introduce **Jinja templating** and explain how it simplifies dynamic prompt generation for tasks like handling multiple placeholders and complex logic.
3. Discuss contexts where Jinja makes life easier for an LLM practitioner.

### **Step 1: A Non-Trivial Task Using Existing Tools**

#### **Scenario**:
Let’s assume we’re building a dynamic system to generate personalized messages for different cities and years. The messages should reference both the city and a specific year (e.g., events in a city or its population in a certain year). 

We want to construct prompts like these:
- "What major events happened in **Paris** in **1999**?"
- "How has the population of **Tokyo** changed since **2005**?"

#### **Initial Script: Using `str.replace()` for Multiple Placeholders**

We’ll start by showing students how to manually substitute two placeholders (`<city>` and `<year>`) using the tools they’ve already learned.

```python
import os
import openai
import fire

# Step 1: Fetch API key from environment variables
api_key = os.environ.get("OPENAI_API_KEY")
if not api_key:
    raise ValueError("API key not found. Set the OPENAI_API_KEY environment variable.")

# Step 2: Define a function that queries GPT with a list of modified prompts
def query_gpt(prompts, max_tokens=50, temperature=0.7, top_p=1.0):
    openai.api_key = api_key
    response = openai.Completion.create(
        model="text-davinci-003",
        prompt=prompts,  # Send the list of modified prompts
        max_tokens=max_tokens,
        temperature=temperature,
        top_p=top_p,
    )
    return response

# Step 3: Add CLI using Fire
def main(city="Paris", year="2023"):
    """
    Modify prompts based on the city and year using simple string substitution.

    Arguments:
    - city: The city name to replace the <city> placeholder.
    - year: The year to replace the <year> placeholder.
    """
    # Step 4: Define a list of prompts with placeholders
    prompts = [
        "What major events happened in <city> in <year>?",
        "How has the population of <city> changed since <year>?",
        "What are some famous landmarks in <city>?",
        "Can you tell me an interesting fact about <city> in <year>?"
    ]

    # Step 5: Replace both <city> and <year> placeholders
    modified_prompts = []
    for prompt in prompts:
        modified_prompt = prompt.replace("<city>", city).replace("<year>", year)
        modified_prompts.append(modified_prompt)

    # Step 6: Call the query_gpt function with the modified prompts
    response = query_gpt(modified_prompts)

    # Step 7: Print the raw response object
    print(response)

if __name__ == "__main__":
    fire.Fire(main)
```

#### **Explanation**:
- **Step 5**: The student replaces both `<city>` and `<year>` placeholders using `str.replace()`. While this approach works, it quickly becomes cumbersome as the number of placeholders increases or if the formatting needs to change dynamically.
  
For example, students might wonder:
- What if there were more placeholders or variations in how the text should be formatted?
- What if the formatting logic becomes more complex than simple `str.replace()` calls?

This approach works, but it’s repetitive, error-prone, and doesn’t scale well as complexity increases.

---

### **Step 2: Introducing Jinja Templating**

After students work through the manual approach, we can now introduce **Jinja2**, which is much better suited for dynamically generating complex text. **Jinja2** allows students to use template files or strings with placeholders that can be replaced easily using context data.

#### **What is Jinja2?**
Jinja2 is a templating engine that allows you to define placeholders and use logic (like loops and conditionals) to dynamically generate text. Instead of manually calling `str.replace()` for each placeholder, you define a template once, and Jinja2 handles replacing variables and applying logic.

---

#### **Updated Script: Using Jinja2 for Templating**

First, students will need to install the Jinja2 library:
```bash
pip install Jinja2
```

Here’s how we can refactor the previous script using **Jinja2**:

```python
import os
import openai
import fire
from jinja2 import Template

# Step 1: Fetch API key from environment variables
api_key = os.environ.get("OPENAI_API_KEY")
if not api_key:
    raise ValueError("API key not found. Set the OPENAI_API_KEY environment variable.")

# Step 2: Define a function that queries GPT with a list of modified prompts
def query_gpt(prompts, max_tokens=50, temperature=0.7, top_p=1.0):
    openai.api_key = api_key
    response = openai.Completion.create(
        model="text-davinci-003",
        prompt=prompts,  # Send the list of modified prompts
        max_tokens=max_tokens,
        temperature=temperature,
        top_p=top_p,
    )
    return response

# Step 3: Add CLI using Fire
def main(city="Paris", year="2023"):
    """
    Modify prompts using Jinja templating.
    
    Arguments:
    - city: The city name to replace the {{ city }} placeholder.
    - year: The year to replace the {{ year }} placeholder.
    """
    # Step 4: Define a Jinja template with placeholders
    template_str = """
    What major events happened in {{ city }} in {{ year }}?
    How has the population of {{ city }} changed since {{ year }}?
    What are some famous landmarks in {{ city }}?
    Can you tell me an interesting fact about {{ city }} in {{ year }}?
    """
    
    # Step 5: Create a Jinja2 Template object
    template = Template(template_str)
    
    # Step 6: Render the template with context (city, year)
    rendered_prompts = template.render(city=city, year=year)

    # Jinja renders it as one string, so split it into a list of prompts
    prompts = rendered_prompts.split("\n")
    
    # Step 7: Call the query_gpt function with the modified prompts
    response = query_gpt(prompts)

    # Step 8: Print the raw response object
    print(response)

if __name__ == "__main__":
    fire.Fire(main)
```

#### **What’s Changed?**
1. **Jinja Template**: Instead of manually calling `str.replace()` for each placeholder, we define a **Jinja template** using `{{ city }}` and `{{ year }}`.
   - This template can be easily updated or extended with more variables or logic without needing to rewrite substitution code.
   
2. **Rendering the Template**: The `template.render()` method takes the `city` and `year` as inputs, replaces the placeholders, and returns a single string. We then split the string into separate prompts using `.split("\n")`.

---

### **Comparison of Approaches**

1. **Manual `str.replace()` Approach**:
   - Works for small-scale, simple substitutions.
   - Becomes cumbersome and repetitive when you have multiple placeholders or more complex logic (e.g., optional variables).

2. **Jinja2 Templating**:
   - Scales much better, as it separates the structure (the template) from the dynamic data (context values like `city` and `year`).
   - Can handle more complex use cases, such as optional text, loops, and conditionals directly inside the template.

---

### **Why Jinja2 Makes Life Easier for LLM Practitioners**:

1. **Scalability**:
   - As the complexity of prompts grows (e.g., multi-step prompts, different structures for different use cases), Jinja2 allows LLM practitioners to easily manage templates.
   
2. **Reusability**:
   - Templates can be defined once and reused for different tasks, reducing repetition in prompt generation.

3. **Complex Logic**:
   - With Jinja2, you can add conditionals (`if-else`) and loops (`for`) directly in the template, which would be much harder to manage with manual `str.replace()` operations.

---

### **Learning Outcomes**:
1. **Manual String Substitution**: Reinforces the manual approach using `str.replace()` to show that students already have the tools to perform dynamic text generation, but it can be cumbersome.
2. **Introduction to Jinja2**: Demonstrates how Jinja2 templating simplifies dynamic prompt creation, especially when dealing with multiple placeholders and more complex text generation logic.
3. **Jinja Context**: Shows how the concept of **context** works in Jinja2, where variables (like `city` and `year`) are passed into the template to dynamically generate text.

---

### **Next Steps**


---

preceding: [[openai completions object actually has a much nicer repr than that, here's an example]]  


following: [[next, introduce custom functions. start by pointing out it's a language feature the student has already been interacting with and proceed to modify the function signature and output of existing functi]]