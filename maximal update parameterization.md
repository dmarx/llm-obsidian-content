---
date: '2024-12-01T05:55:02.475411Z'
tags:
- autogenerated
- source__claude
- conversation_id__89a0dda3-5667-4fb7-9fa7-fc583dbda261
- msg_26ee2d8e-df63-4588-a367-f9a807cde2dd
- msg_31503eef-491c-4c8b-8782-df8e366aab5c
- colorclass/information-theory
title: maximal update parameterization
---

**USER**

maximal update parameterization


**ASSISTANT**

# Maximal Update Parameterization

[[Maximal Update Parameterization]] (μP) is a principled initialization and parameterization scheme for deep neural networks that enables stable training at arbitrary width and depth by maintaining constant expected parameter updates across architectures.

## Mathematical Formulation

### Core Principle

The key insight of μP is to scale parameters to achieve O(1) updates during training:

For a layer h = φ(Wx + b), the μP parameterization is:

W̃ = αW/√fan_in
b̃ = αb

where:
- α is the [[layer scale factor]]
- fan_in is the input dimension
- W̃, b̃ are the transformed parameters

### Layer Scale Factor

The layer scale factor α is computed as:

α = √(2/N)

where N is the number of neural network parameters that affect any single output neuron.

## Comparison with Other Parameterizations

### Standard Parameterization (SP)
Traditional initialization with variance scaling:
```
W ~ N(0, 1/fan_in)
b ~ N(0, 1)
```

### Neural Network Gaussian Process (NTK) Parameterization
Scales weights to achieve constant kernel in infinite width:
```
W ~ N(0, 1)
b ~ N(0, 1)
```

### Relationship to Other Parameterizations

μP interpolates between SP and NTK parameterization:
- At infinite width: equivalent to SP
- At finite width: maintains stable gradients like NTK

## Training Dynamics

### Gradient Flow

Under μP, the gradient update magnitude remains O(1) throughout training:

||Δθ|| ∝ O(1)

where θ represents any network parameter.

### Learning Rate Scaling

The optimal learning rate η scales as:

η ∝ 1/N

where N is the number of parameters affecting each output.

## Theoretical Properties

### Width Independence

1. [[Parameter Updates]]:
   E[||Δθ||²] remains constant across different network widths

2. [[Feature Learning]]:
   Maintains capacity for feature learning at all widths

### Depth Independence

1. [[Signal Propagation]]:
   Stable forward and backward signal propagation at any depth

2. [[Gradient Scaling]]:
   Uniform gradient magnitudes across all layers

## Practical Implementations

### Initialization

```python
def muP_init(shape, alpha):
    fan_in = shape[0]
    W = np.random.normal(0, 1, shape)
    return alpha * W / np.sqrt(fan_in)
```

### Layer Transformation

For a standard neural network layer:
```python
def muP_layer(x, W, b, alpha):
    W_scaled = alpha * W / np.sqrt(W.shape[0])
    b_scaled = alpha * b
    return activation(W_scaled @ x + b_scaled)
```

## Applications

### Deep Networks
- Enables training of very deep networks without vanishing/exploding gradients
- Maintains consistent learning dynamics across architectures

### Wide Networks
- Provides stable training for networks of arbitrary width
- Preserves feature learning capability at all scales

### Architecture Search
- Allows fair comparison between different architectures
- Simplifies hyperparameter selection

## Related Concepts

- [[Neural Tangent Kernel]]
- [[Mean Field Theory]]
- [[Gradient Flow]]
- [[Parameter Initialization]]
- [[Signal Propagation]]

## Extensions

### Attention Networks
Modified μP for self-attention:
```
Q = W_Q/√(d_model)
K = W_K/√(d_model)
V = W_V/√(d_model)
```

### Residual Networks
Adjusted scaling for skip connections:
```
h = x + α·layer(x)/√(L)
```
where L is network depth.

## Open Questions

1. [[Optimization Dynamics]]:
   Understanding the precise relationship between μP and optimization trajectories

2. [[Architecture Dependencies]]:
   Analyzing how architectural choices affect optimal μP scaling

3. [[Finite-Width Effects]]:
   Characterizing deviations from theoretical predictions in practical settings

The Maximal Update Parameterization provides a principled approach to neural network initialization and scaling, enabling stable training across a wide range of architectures while maintaining capacity for feature learning.


---

preceding: [[tensor programs]]  


following: [[μp for self-attention derivation]]

See also: [[parameterization]]