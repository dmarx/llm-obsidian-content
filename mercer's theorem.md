---
date: 1715967078.97715
tags:
- autogenerated
- source__oai
- conversation_id__e536633b-8eed-4822-967e-58113d1a33c2
- msg_aaa2fdbd-c19e-4aeb-8fc0-9c8366e33bbb
- msg_2c63d671-21c2-44ea-a01f-13c14f4ff0e3
- colorclass/vector-space
title: mercer's theorem
---

**USER**

Mercer's Theorem


**ASSISTANT**

## Mercer's Theorem

[[Mercer's Theorem]] is a fundamental result in functional analysis and the theory of [[Reproducing Kernel Hilbert Spaces]] (RKHS). Named after the British mathematician James Mercer, the theorem provides conditions under which a given function can be represented as a sum of eigenfunctions of an integral operator, thereby establishing the theoretical foundation for [[kernel methods]] in machine learning.

### Statement of Mercer's Theorem

Mercer's Theorem can be stated as follows:

*Let \( k(x, z) \) be a continuous, symmetric, positive semi-definite kernel function defined on a compact domain \( \mathcal{X} \subset \mathbb{R}^n \). Then there exists a sequence of non-negative eigenvalues \( \{\lambda_i\} \) and an orthonormal set of eigenfunctions \( \{\phi_i(x)\} \) in \( L^2(\mathcal{X}) \) such that:*

$$
k(x, z) = \sum_{i=1}^{\infty} \lambda_i \phi_i(x) \phi_i(z)
$$

*for all \( x, z \in \mathcal{X} \), where the series converges absolutely and uniformly.*

### Mathematical Formalism

#### Kernel Function

A kernel function \( k(x, z) \) is a function that expresses the inner product of the images of \( x \) and \( z \) under some (potentially implicit) mapping \( \Phi \):

$$
k(x, z) = \langle \Phi(x), \Phi(z) \rangle
$$

#### Integral Operator

Given a kernel function \( k \), we can define an integral operator \( T_k \) on \( L^2(\mathcal{X}) \) by:

$$
(T_k f)(x) = \int_{\mathcal{X}} k(x, z) f(z) \, dz
$$

#### Eigenvalues and Eigenfunctions

Mercer's Theorem guarantees that the integral operator \( T_k \) has a discrete spectrum of non-negative eigenvalues \( \{\lambda_i\} \) and corresponding eigenfunctions \( \{\phi_i(x)\} \) such that:

$$
T_k \phi_i(x) = \lambda_i \phi_i(x)
$$

and the kernel function can be expressed as:

$$
k(x, z) = \sum_{i=1}^{\infty} \lambda_i \phi_i(x) \phi_i(z)
$$

### Implications and Applications

#### Kernel Methods

Mercer's Theorem provides the theoretical foundation for kernel methods in machine learning, such as [[Support Vector Machines]] (SVMs), [[Kernel PCA]], and [[Gaussian Processes]]. By expressing the kernel function as a sum of eigenfunctions, it allows these methods to operate in a high-dimensional (or even infinite-dimensional) feature space without explicitly computing the mapping \( \Phi \).

#### Feature Space Representation

The eigenfunctions \( \{\phi_i(x)\} \) form a basis for the feature space associated with the kernel. Any function in the RKHS can be represented as a linear combination of these eigenfunctions. This representation is crucial for understanding the capacity and expressiveness of kernel-based models.

### Example

Consider the Gaussian (RBF) kernel:

$$
k(x, z) = \exp \left( -\frac{\|x - z\|^2}{2\sigma^2} \right)
$$

For this kernel, Mercer's Theorem assures that there exists an expansion in terms of its eigenvalues and eigenfunctions, though the explicit forms of these eigenfunctions are often complex and depend on the specifics of the kernel and the domain \( \mathcal{X} \).

### References

- Mercer, J. (1909). *Functions of Positive and Negative Type and Their Connection with the Theory of Integral Equations*. Philosophical Transactions of the Royal Society A: Mathematical, Physical and Engineering Sciences, 209(441-458), 415-446.
- Sch√∂lkopf, B., & Smola, A. J. (2002). *Learning with Kernels: Support Vector Machines, Regularization, Optimization, and Beyond*. MIT Press.
- Wahba, G. (1990). *Spline Models for Observational Data*. SIAM.

Mercer's Theorem thus provides a powerful theoretical tool for understanding and utilizing kernel functions in various machine learning algorithms, ensuring that the use of such kernels is mathematically sound and effective.


---

preceding: [[representer theorems]]  


following: [[spectral theorem]]

See also: [[understanding kernel methods through infinite-dimensional hilbert spaces]]


See also: [[mercer's condition in kernel methods]]


See also: [[interpretation of sgd training as kernel learning]]


See also: [[mercer's theorem explained]]


See also: [[space of square integrable functions ( l^2 ) space]]


See also: [[connecting sgd training in neural networks to mercer's theorem]]