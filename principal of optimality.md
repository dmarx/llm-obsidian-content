---
date: 1717809683.283136
tags:
- autogenerated
- source__oai
- conversation_id__77e70e10-f02a-4f98-bd5c-6217b10122cf
- msg_aaa2a9b5-39cf-4e90-94f8-f4a3092f6051
- msg_9146c121-5777-4bfc-a48e-c6aada64c43b
- colorclass/system-dynamics
title: principal of optimality
---

**USER**

principal of optimality


**ASSISTANT**

The principle of optimality, introduced by Richard Bellman, is a fundamental concept in dynamic programming and optimal control theory. It states that an optimal policy has the property that, regardless of the initial state and initial decisions, the remaining decisions constitute an optimal policy with regard to the state resulting from the first decision. This principle allows problems to be broken down into simpler subproblems, which can be solved recursively.

### Formal Statement

The principle of optimality can be formally stated as follows:

For a given optimization problem, consider a sequence of decisions or controls that leads from an initial state \( \mathbf{x}_0 \) to a final state \( \mathbf{x}_T \). If this sequence is optimal, then any subsequence of this sequence must also be optimal.

### Mathematical Formulation

Consider an optimal control problem where the objective is to minimize (or maximize) a cost function \( J \). The system dynamics are given by:
\[ \mathbf{x}_{k+1} = \mathbf{f}(\mathbf{x}_k, \mathbf{u}_k, k), \]
where \( \mathbf{x}_k \) is the state at time \( k \) and \( \mathbf{u}_k \) is the control input.

The cost function is typically expressed as:
\[ J = \phi(\mathbf{x}_T) + \sum_{k=0}^{T-1} L(\mathbf{x}_k, \mathbf{u}_k, k), \]
where \( \phi \) is the terminal cost and \( L \) is the running cost.

### Dynamic Programming and the Principle of Optimality

The principle of optimality is the cornerstone of dynamic programming. In dynamic programming, the problem is solved by recursively breaking it down into simpler subproblems. The key concept is the **value function** \( V(\mathbf{x}, t) \), which represents the minimum cost to go from state \( \mathbf{x} \) at time \( t \) to the terminal state.

The recursive relationship for the value function is derived from the principle of optimality:

\[ V(\mathbf{x}_k, k) = \min_{\mathbf{u}_k} \left[ L(\mathbf{x}_k, \mathbf{u}_k, k) + V(\mathbf{x}_{k+1}, k+1) \right], \]
subject to the system dynamics:
\[ \mathbf{x}_{k+1} = \mathbf{f}(\mathbf{x}_k, \mathbf{u}_k, k). \]

### Hamilton-Jacobi-Bellman (HJB) Equation

The Hamilton-Jacobi-Bellman (HJB) equation is a continuous-time analog of the dynamic programming principle. It provides a partial differential equation that must be satisfied by the value function.

For a system with state \( \mathbf{x}(t) \) and control \( \mathbf{u}(t) \), the HJB equation is given by:

\[ \frac{\partial V}{\partial t} + \min_{\mathbf{u}} \left[ L(\mathbf{x}, \mathbf{u}, t) + \frac{\partial V}{\partial \mathbf{x}} \cdot \mathbf{f}(\mathbf{x}, \mathbf{u}, t) \right] = 0, \]
with terminal condition:
\[ V(\mathbf{x}(T), T) = \phi(\mathbf{x}(T)). \]

### Example

Let's consider a


---

preceding: [[optimal control theory]]  


following: [[hamilton-jacobi-bellman equation]]

See also: [[control theory]]


See also: [[dynamic programming]]


See also: [[system dynamics]]