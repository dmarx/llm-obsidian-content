---
date: 1719449236.304094
tags:
- autogenerated
- source__oai
- conversation_id__3d4447be-bb92-4529-b72e-378106434722
- msg_aaa225ee-b26b-48e9-b829-846a70801387
- msg_5f62c5ba-bdb4-470e-8ad5-cf43ae352648
- colorclass/vector-space
title: subgradients
---

**USER**

subgradients


**ASSISTANT**

### Subgradients

#### Introduction
In convex analysis and optimization, the concept of subgradients extends the notion of gradients to non-differentiable functions. For a convex function, the subgradient generalizes the idea of a derivative, providing a means to describe the slope of the function even at points where it is not differentiable.

#### Definition
Let \( f: \mathbb{R}^n \to \mathbb{R} \) be a convex function. The subgradient of \( f \) at a point \( x \in \mathbb{R}^n \) is a vector \( g \in \mathbb{R}^n \) such that:

$$
f(y) \geq f(x) + g^T (y - x) \quad \forall y \in \mathbb{R}^n
$$

Here, \( g \) is called a subgradient of \( f \) at \( x \), and the set of all subgradients at \( x \) is called the subdifferential of \( f \) at \( x \), denoted by \( \partial f(x) \).

#### Properties of Subgradients
1. **Existence**: For a convex function \( f \), the subdifferential \( \partial f(x) \) is non-empty at every point \( x \).
2. **Convexity**: The subdifferential \( \partial f(x) \) is a convex set.
3. **Continuity**: If \( f \) is differentiable at \( x \), then \( \partial f(x) = \{\nabla f(x)\} \).

#### Examples

1. **Absolute Value Function**: Consider \( f(x) = |x| \).
    - For \( x > 0 \), \( \partial f(x) = \{1\} \).
    - For \( x < 0 \), \( \partial f(x) = \{-1\} \).
    - For \( x = 0 \), \( \partial f(0) = [-1, 1] \).

2. **Indicator Function**: Let \( f(x) = I_C(x) \) be the indicator function of a convex set \( C \):
    - \( I_C(x) = 0 \) if \( x \in C \), and \( I_C(x) = \infty \) otherwise.
    - The subdifferential \( \partial I_C(x) \) at \( x \in C \) is the normal cone to \( C \) at \( x \):
    $$
    \partial I_C(x) = \{ g \in \mathbb{R}^n : g^T(y - x) \leq 0 \ \forall y \in C \}.
    $$

#### Subgradients in Optimization
In optimization, subgradients play a crucial role in subgradient methods, which are iterative algorithms used to minimize non-differentiable convex functions. These methods generalize the gradient descent method by using subgradients instead of gradients.

The iterative update rule in a subgradient method is:
$$
x_{k+1} = x_k - \alpha_k g_k,
$$
where \( g_k \in \partial f(x_k) \) is a subgradient at \( x_k \), and \( \alpha_k \) is a step size.

#### Subgradient Calculations

To find subgradients for a convex function \( f \), one often leverages the following techniques:
- **Direct Calculation**: Derive the subgradient from the definition.
- **Subdifferential Calculus**: Use known rules for combinations of functions (e.g., sum, maximum).
- **Convex Duality**: Utilize dual problem formulations and KKT conditions.

#### Further Reading
For a deeper dive into subgradients and their applications, consider exploring the following topics:
- [[Convex Functions]]
- [[Subgradient Method]]
- [[Convex Optimization]]
- [[Fenchel Duality]]
- [[Nonsmooth Analysis]]

Understanding subgradients and their properties is essential for tackling various problems in convex optimization and for extending gradient-based methods to more general settings.

See also: [[gradient]]