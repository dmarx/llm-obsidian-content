---
date: 1736562901.560919
tags:
- autogenerated
- source__oai
- conversation_id__6781d7af-b1c4-8008-adbb-ad26caf1e9b3
- msg_aaa27197-cfeb-4725-925f-71a127fede24
- msg_c414e282-383b-48f0-903c-303e68b86e10
- msg_aaa292f0-24ca-44cb-a525-2e630eb2b2ba
- msg_3258ac55-77aa-4d25-80c9-41da5dfbdf56
- colorclass/critical-thinking
title: uh good morning def con thanks for coming out appreciate it uh we have two
  speakers this morning uh tom cross and
---

**USER**

uh good morning DEF CON thanks for coming out appreciate it uh we have two speakers this morning uh Tom cross and
0:07
Greg KY and their talk is going to be on deception and counter deception please give them a good DEF CON
0:20
welcome so uh good morning everyone Thanks for uh making it out this morning
0:25
um so we're uh we're excited about this talk um I uh I I've been coming to
0:31
DEF CON you know since it got started and when um you know early DEF CON uh you
0:37
know at that time the internet was just beginning to become publicly available
0:43
um to Consumers right and we were thinking about you know what what how is the Internet going to affect society
0:48
like how what is going to happen right um and we were sort of imagining a world that we now live in today and I think
0:55
it's interesting to look back at how what we thought would happen and question you know sort of you know where
1:01
We've Ended up right I think people in this community could see both the like promise and Peril of the internet um I
1:09
remember going to lunch at DEF CON going to a food court and everyone's paying with cash and we're thinking you know in
1:16
the future we're probably going to pay electronically for everything and a little record will be kept every time we
1:21
buy something and like it will create this model of our behavior and how will that get used against us right um but at
1:27
the same time I think people were genu genuinely excited about the idea that everyone will have the world's knowledge
1:33
at their fingertips right and maybe that will make people a little smarter it might up level Humanity um and so as you
1:42
know time has gone by and we've actually got all these capabilities and everyone's using them um you know
1:48
there's certain facets of human nature that have come roaring forth uh and have
1:54
a huge impact on how this stuff actually affects us uh I I think the part the
2:00
problem is that people don't come to the internet because they want to get smarter they come to the internet because they want to be told that
2:06
they're already smart um they seek validation right um and there's a lot of people who recognize this and they're in
2:13
what I call the funhouse mirror business so they present you a world in which
2:19
you're a good person in which you should get what you want and in which the people that you dislike we're going to
2:25
show you those people in the most negative light possible over and over again again um and this world view that
2:33
is centered around you and why you're special um is highly compelling to people's egos um this is what we on some
2:42
level mean by deception and we find deception not just in the like narratives that appear on social media
2:48
but also um at every level like in the fishing emails that people are getting in the malware that's running on their
2:54
computers um ultimately the internet has become this massive deception engine uh
3:02
in which these uh false narratives are appearing at every level of abstraction
3:07
um and you can't trust anything that you see in the screen um it's I think people have recognized this they've recognized
3:14
that the internet is not giving us what we wanted it's not making us smarter uh
3:19
and in fact um the theme of this year's Defcon is is how do we engage with the
3:25
an Internet that isn't giving us what we want how do we make things better um so before I get into the overview of the
3:31
talk um let me introduce us uh so my name is Tom uh I've been coming to Defcon for a long time uh I've done a
3:39
few social media projects over the years my current uh project is called feed Seer it's a newsreader app for masted on
3:46
so uh it shows you the top links that have been posted on your feed in the past 24 hours and what people are saying
3:51
about each link uh and um there's lots of infosec folks on Macedon um I've also
3:57
you know generally had a career in infos and I've looking at a lot of cons on information security uh topics often
4:04
with this gentleman Craig hi I'm Greg KY thank you for
4:09
joining me here on araus for Defcon 32 um my background is uh I was long-term
4:17
faculty at West Point where I ran their cyber security research and education programs also worked at NSA twice and US
4:24
cyber command twice um and then I've developed and taught the information
4:29
operations course at black cap training 7 years ago and been running it uh since there and in private sessions and also
4:36
run the military strategy and tactics for cyber security course for 10 years same deal okay thanks Tom oh and Greg
4:43
and I are teaching a class on adversarial thinking at Defcon training after Defcon is over if you guys want more in Vegas um so uh what are we going
4:51
to cover in this talk um the first thing we're going to do is we're going to tap into Sumit Greg's deep expertise with
4:57
military Doctrine and thinking militaries have been thinking about Conflict for hundreds of years um and
5:02
deception in particular is an area where they have um identified maxims that
5:08
teach you how to craft effective deceptions so we're going to talk about how uh to do deception effectively um
5:15
and then we're going to flip the coin over and ask well if we understand offense really well what does that teach
5:21
us about defense um are there counter deception principles that we can drive
5:27
uh that tell us how to fight deception and um the the deception and counter
5:33
deception principles what we're going to cover are they're useful in any context where this could be occurring it could
5:38
be useful for malware attribution or in a security operations context um but you
5:44
know this year's Defcon is about engaging with the internet um and so we're going to spend the third part of the talk um trying to apply some of
5:51
these counterdeception maxims and ask what sort of capabilities um you know is the internet lacking that might be
5:58
useful or might help us defend ourselves against deception that we're facing so
6:04
um why are we talking to you guys about this like why is this relevant I think
6:10
you know hackers have the ability to identify better than like a lot
6:15
of you know General people so you guys have a unique talent at that uh and uh
6:21
um you know I I also you know I mean I I listened to Corey Dr O's talk last year which inspired this year's theme and his
6:27
talk yesterday um it's he's engaged in a very important discussion about like
6:32
policy issues um and uh I I think you know what I do is I write code and I
6:37
break code um and I want to talk about you know how I can apply those skills uh
6:43
um to engage with this problem as well so um you know hopefully uh um you know we we we add to the discussion a new
6:51
dimension um so uh Greg let's talk about
6:56
deception so deception has been around for millennial and the key idea is that
7:01
it's the act of hiding the truth to get yourself an advantage and what you're trying to do is influence your target to
7:10
make an incorrect decision right or it to take an action that you want or fail
7:16
to take an action all to your advantage and we've seen examples from our five-legged troan horse AI uh generative
7:24
AI delivers when you need a five-legged troan horse um to the Civil War where
7:30
people painted logs black to create the to try and fake cannons to the um Cuban
7:38
Missile Crisis hiding medium- range ballistic missiles and concealing them in uh ships to the uh Persian Gulf War
7:47
where the invading Iraqis were concerned about an attack by the Sea by the
7:52
Marines and instead were surprised by an attack by land from the opposite direction we see it in the uh Russia
7:59
Ukraine conflict the ghost of keev uh which was a mythical fighter pilot Ace
8:07
fighter pilot fighting the Russian aggressors over the city which was later you know proved to to be a deception
8:13
operation so we've seen this uh for Millennia and when we think about the
8:20
targets of deception humans are first come first to mind and you you know in
8:26
in information security users of you know things like fishing typo squatting
8:31
do domain mimicry spoofed login pages but also experts think malware analysts
8:38
false Flags fileless malware deceptive metadata
8:43
um let uh code injection rotating command and control infrastructure so
8:49
Specialists can be targets for deception as well but more than that it's not just the humans it's our code so our code
8:56
think malware DET uh detection systems people try and deceive that through
9:01
fileless malware polymorphic malware rotating command and control infrastructure and as we move into the
9:08
era of ai ai is also a deception Target think poisoning the training data of the
9:14
AI or jailbreaking techniques to overcome safeguards that people have put in
9:21
place and deception can occur at all levels it can occur at all levels of the
9:26
network stack it can also occur in think more broadly at the Tactical operational
9:32
and strategic level uh the Tactical level you're actively deceiving someone
9:37
you're engaged in conflict with all the way up to a strategic level where you
9:43
try to hide your basic you know National objectives your intentions your strategies and your capabilities or put
9:49
forth uh maybe to put forth an alternate reality that's more
9:54
beneficial so humans process data through this idea of the diw hierarchy
10:01
we begin with data and with by adding uh context we can create information by
10:07
adding meaning we can create knowledge and we can by adding Insight we can create wisdom well deception can poison
10:14
our ability to you know by poisoning the data that we can have incorrect information knowledge wisdom that we
10:21
think is entirely accurate so deception poisons our ability to
10:27
think and it's a professional discipline uh deception uh what we have here the
10:34
orange document is a uh Declassified CA CIA document uh that includes the de
10:41
deception maxims that we're going to uh be discussing and also there there are manuals like this is the Department of
10:48
Defense joint publication on Military deception it comes out every few years updated so this the key takeaway here is
10:54
it's a a professional discipline and looking at some of the
11:00
maxims we're just going to run through these pretty quick and then we're going to flip them and show how you might counter but one of the most important is
11:07
it's easier to maintain a pre-existing belief um in in your target than to force a change right so if someone
11:14
believes the World is Flat you can help encourage them to continue thinking the World is Flat but if you want to have
11:21
them think the world is round well that's much
11:26
harder so yeah I really think mcgruder's principle is sort of the golden rule of
11:31
deception it speaks to this like fundamental aspect of human nature um
11:37
you know say there's a politician you hate there might be different answers in the room for who that person is but everyone hates some politician right and
11:44
if I walk up to you and I say well you know that politician you hate did you hear about the stupid thing they did
11:49
this morning whatever I say next whatever it is you are going to believe it because it's aligned with what you
11:56
already think and it feels good to be right um and so uh I if if any of you
12:02
have read um azimoff's Foundation series he has a really good um model um I think
12:08
of deception that he presents in those books there are these robots that can manipulate people's emotions um and uh
12:15
that they're constantly explaining that they can't radically change people's worldview and the way that they function
12:22
is by trying to understand what that person thinks and identify some specific belief that they have that they can
12:28
emphasize or reinforced just enough to get that person to take an action that
12:33
they might have otherwise not taken I I think that's a really good representation of like effective
12:39
deception in a nutshell so one of my favorites is the
12:47
idea of exploiting the limits of human and machine sensing uh and information processing so you think we have as
12:54
humans have senses right and that are limited and like known parameters of
13:00
what we can uh sense we have limitations in our ability to process information
13:05
machines do the same they have sensors and they have information processing so if you can exploit those limits for
13:11
example uh think self-driving cars there's been some really interesting talks that you can exploit the sensing
13:16
and processing of a self-driving car and make it think see things that aren't there uh in a heist movie uh where
13:24
there's usually a motion sensor and someone moves really really slowly to
13:29
slip past the motion sensor that kind of thing and and a variation of that is
13:36
here's a picture of an inflatable tank from Ghost Army uh in World War II it
13:41
was a group of creatives uh and they were a formal deception unit and it's
13:47
their ability to create inflatable tanks and fake enemy or fake troop uh imp placements uh was because the
13:54
capabilities at the time to sense what was you know sense the environment these tanks looked real from a distance now
14:01
they'd be they'd obviously be not um then uh the third maximum is Jones's
14:07
dilemma when uh the idea is that in this competitive information environment of competing narratives that if you're
14:14
going to try and deceive someone and you you have to ideally have more false sources than real so it's this battle of
14:22
uh quantity and quality of of competing narratives another strategy is the idea
14:29
that you want to carefully uh create a story that you don't just have single data points but you're putting puzzle
14:35
pieces out there for the deception Target to connect together that tell the story that you want that you're trying
14:42
to to portray uh another is the idea of carefully designed um planned placement
14:50
of deceptive material the idea is you want to make your target work for it if
14:56
you if uh think if you wanted to have a fake diary in in your room right if you
15:02
leave it on their desk that that is an obvious place and that could be like your um that probably what you want to
15:10
do is make it hard have a lock on it have it hidden in a ventilation duct have it written in Code by the time they
15:17
find that and they've unpacked all that then they think this has absolutely got to be the real diary and we see that in
15:24
malware as well right that's certainly a strategy if you make them work for people will be more likely to believe it
15:31
the flip side of this is an orgy of evidence where things are you know many incriminating things are obviously found
15:39
well that should raise suspicions so yeah Olympic Destroyer is
15:44
a really good example of the use of of some of these techniques in order to fool malware analyst as to attribution
15:50
doing attribution on ioc's is Frau with risk as with respect to uh you know the
15:55
manufacturing of those ioc's um in this case uh there was a rich header in a binary uh that uh you know captures
16:02
information about the developer environment and they they literally copied a rich header from a Lazarus sample and put it in their their sample
16:09
um and so someone who knows Rich headers and like uh has a collection of them and knows that they could be used to you
16:16
know attribute malware might find this and you know be excited that they connected the dots right and so now
16:22
they're emotionally invested in the idea that this is true um and uh you know they they go out with this narrative it
16:27
turns out that in fact in this this case the rich header didn't match the rest of the binary and the attribution was
16:32
completely deceptive um so again uh you know avoid uh if you're trying to
16:39
protect yourself against deception situations where you're emotionally invested in the consequences of your
16:47
work so I particularly like this one U maximum 6A 6B there are two sides of the
16:53
same coin the first is ambiguity so the idea is you want to increased doubt in
17:00
the person's mind in your Target's Mind by providing many possible truths okay make sense they're they they're UNC
17:07
there's this cloud of uncertainty the flip side of that is you want to decrease doubt and focus the Target on a
17:14
particular um falsehood so you want them instead of being in a sea of doubt you
17:20
want them absolutely positively sure that they're right uh that they sure
17:25
about this falsehood they think that they you know that it's true another ideas that
17:32
in in professionalized deception that there's the idea of husbanding deception
17:37
assets that they're always limited resources you need to save them for the time and at the best time and the best
17:43
place to be effective you can see that in cyber in information security capabilities OD day right uh but imagine
17:52
if you had the ability to place deceptive material on a web server you can't if you use it you're using that
17:58
that capability uh and he might not be able to use it again so you save these for the time and place you think will be
18:05
most effective uh Maxum 8 is the idea of feedback in a professional uh deception
18:11
operation organization the attackers are monitoring their target audience looking
18:17
for feedback that the deception has been believed and they're also so they want
18:22
that feedback loop uh and they're also monitoring their friendly organization to see if they are being dis
18:31
so all right so now that we've talked a little bit about the principles of effective deception let's flip the coin
18:38
over and talk about how you practice counter deception um so uh um at a high
18:45
level I think there are four um General ways to counter deception one is through intelligence collection so if you're
18:51
actually monitoring your adversary and just watching them create their deception then you know exactly what they're doing and why they're doing it
18:58
and what the truth is right um and US cyber command has this defend of forward
19:03
uh um uh uh concept which has to do with like just going out and and directly spying on the people that are committing
19:09
the act so that you know exactly what they're up to um another thing you could do not we don't always have intelligence
19:15
capabilities um is disruption um if uh so we talked about husbanding assets if
19:21
uh you know someone has to go to certain effort to inject um deceptive narratives
19:26
um you know in places and and you can take ction to to interfere with their capabilities um then that prevents them
19:32
from being successful um a simple example is like if they have to build a bot net and order to spread something online and you're able to take the botn
19:39
net down then you're interfering with their deceptive capability um another thing you can do is is analytic
19:46
sometimes that's your only option um you're looking at the information you're collecting and you're critically analyzing it in order to figure out
19:52
whether or not it's deceptive um and a lot of our talk will focus on that because that's often the only option that you've got um another thing you can
20:00
do is deterent so if you can if you can demonstrate uh that the deception will
20:05
not be effective um perhaps your adversary will not bother to do it um so
20:11
uh in let's talk a little bit about analytic processes um in infosec we're really good at Devil's advocacy right um
20:19
and so this is the same thing it's about Playing devil's advocate with respect to things that you are that you are
20:26
deciding or that you believe um and you have to have the discipline to do that and that's one of the hardest things
20:31
about it um but uh you know if you have a belief or intelligence related conclusion and there's a set of um facts
20:39
that underpin that belief you can look at each of those facts and ask yourself how hard would it be for my adversary to
20:46
simulate that fact am I am how many ways have I measured that um you know like is
20:51
it possible to fake that uh and um you know even in malware attribution you know a rich header is easy to fake uh
20:58
you know maybe you know could they get access to a particular Source IP address or how hard would that be uh for them to
21:04
to simulate in order to fool me right um so let's talk about some of the deception maxims and I'm going to sort
21:11
of flip them around and um and create counterdeception maxims based on them and I kind of organized them into
21:17
categories so the first category uh is um maximums that suggest when deception
21:23
may be present so your human intuition is both an asset and a liability when
21:29
you're dealing with deception this is the context where it's an asset you want to develop a kind of spidey sense that
21:37
tells you uh things aren't quite right here it this looks fishy and that might
21:42
prompt you to dig deeper um and explore the hypothesis that what you're seeing might be a deception um so for example
21:49
we've talked about carefully sequencing um events to tell a story so it builds up in the target's mind um so flip that
21:56
over what happens if there's all kinds of evidence that gets disclosed all at once if you find the diary sitting out
22:02
on a table where it shouldn't be um you you know that might be a sign to you that like hey someone's trying to you
22:08
know manipulate me that the this is this is possibly deceptive right um so the
22:14
the the person operating in the deception must either do ambiguity or misdirection and in each case we can try
22:20
to counter that right so if they're engaged in ambiguity deception there's going to be a lot of narratives that are
22:26
available right and if you see multiple narratives then that might be a sign uh
22:31
that deception is taking place that some of these narratives are simulated um that was certainly the case with Olympic
22:37
Destroyer there were a bunch of organizations that had different attribution that they were publishing um
22:42
and and that's because the Mau was a intentionally sort of misleading analyst into thinking that that that that
22:49
different narratives were correct um misdirection in a misdirection deception the attacker is trying to get you to
22:55
believe a particular thing which is false and so so um if you analyze it
23:00
carefully perhaps you can discover that it's not true uh the the plus minus rule is this idea that nothing that is an
23:07
imitation or a simulation can be exactly the same as the real thing otherwise it is real and so identifying those
23:14
characteristics which are added or removed from the uh from the thing it
23:19
helps you identify that it is deceptive and again you have to have the discipline to say okay this isn't right
23:25
it's in congruent and therefore I must accept that it is not real
23:31
the place where your human intuition operates against you has to do with mental discipline humans are really good
23:37
at jumping to conclusions based on not necessarily enough evidence we kind of have to do that in order to function in
23:43
the complicated civilization that we have um but um you know that's how uh deceptive deceptive operations get you
23:50
and so you kind of have to have the discipline to slow down and not listen to sometimes to your intuition um so
23:57
this is mcgruder's principle is is the the biggest one here um it's important to apply the same critical analysis to
24:03
facts that support your assumptions that you do to facts that challenge them and that's really hard for people to do um
24:10
you know also with respect to carefully designed placement of deceptive material don't assume if fact is true just
24:15
because you had to work hard to get it ask yourself could the adversary have simulated that um I love this quote from
24:22
a book on professional counterdeception operations um the vulnerable mind fits
24:27
ambig uous information to its own preconceptions and expectations I think
24:33
that's a really clear statement of of vulnerability to
24:39
deception so humans are not that complicated which probably part of the problem right psychologists have been
24:46
studying uh biases in humans for for hundreds of years and I have a list of
24:52
examples here and if you're wondering The Prompt was uh humans swimming in a sea of piranha The Prompt that gave us
24:59
that um but I wanted to highlight two one is the idea of selection bias and
25:05
just one instance of selection bias is where you can choose um from a variety
25:11
of facts truths to paint a picture but you don't share all the truths you're just selecting certain truths and I
25:18
think we see that in many news agencies today a lot of what they say is true but which ones they choose uh paint a
25:25
picture so that's selection bias confir information bias sounds a lot like mcgruder's principle the idea that um we
25:34
are Mo are biased toward things that already um reinforce our preconceived
25:42
beliefs so the third set is uh maximums that suggest methods of preventing
25:47
deception um so the two maximums that have to do with the limits of human and
25:53
machine sensing and information processing like speak to the need to be able to measure reality from multiple uh
26:00
perspectives um you know and think about this on in a technical sense if you if you're in a security operations function
26:06
and you have sensors out there that like look at your network um you know if there's only one kind of a particular
26:11
sensor then that sensor could be uh it's data source could be manipulated it could be be be presented with false
26:17
facts if you've got multiple ways of looking at the situation then it gets harder and harder for the person that's
26:23
operating the deception fool all of the sensors that you have um with respect to
26:28
husbanding deception assets again deception operations professional ones um involve effort and resources and if
26:35
you can disrupt and Target those capabilities um then you can undermine the ability for them to be successful um
26:42
feedback is also interesting a professional deception operation is going to be watching you to determine how you behave you may choose to behave
26:49
as if the deception was effective because you don't want the deceiver to know that you were not deceived uh you
26:54
may also choose um to uh to show the adversary that you you are debunk that you are um aware of their deception
27:01
which is a deterent method um one of the things that um you know folks in DHS and sis are talking about is pre-b buunk so
27:07
we think that this deceptive narrative is going to be placed out there so in advance we're going to go ahead and explain to everyone that it's false so
27:14
that you know people just don't bother trying to uh push that narrative in the first place so in in developing this set of
27:23
trustworthy sources of data like when we're looking at things on the Internet it's really important for for us to uh
27:29
you know collect different places where we can get different points of view uh that are credible and uh objective um
27:37
and so like curating collections of experts I think is really important uh and but also a weakness that we have is
27:43
we have a tendency to believe experts who um you know are telling us what we already think and so we have to find
27:49
objective criteria for evaluating who we want to listen to um and so I I think
27:55
this is a real challenge um like even with uh with respect to experts experts
28:01
sometimes uh deceive us right so let's say there's a set of facts one two 3
28:07
four and five and they add up to a conclusion which is 15 right um so I
28:13
what I can do is I can present you some of those facts one two and three and a conclusion which is six each one of the
28:20
facts I've presented to you is absolutely true and the conclusion I'm presenting to you is a natural
28:26
consequence of the facts that I've presented right so when you're listening to an expert the challenge is that you
28:32
depend upon them not just to tell you what the conclusions are but also what the underlying facts are to educate you
28:38
about the issue um and so you're You're vulnerable to the fact that that without knowing more than they do about the
28:44
subject that you can't see the other facts that they're omitting that may lead you to a different conclusion um
28:50
and so uh you know I I one of the things that people ring their hands about a
28:55
little bit is journalistic objectivity there's this idea that like in the American civilization in particular
29:01
where we have two political parties that what should happen is that uh journalists should present both sides
29:06
and we know that like getting both of these perspectives does not get us closer to the truth right um and so
29:12
perhaps there's this role for journalism as a a professional resource that goes out and finds the other facts that are
29:18
missing and adds them into the equation to get the complete truth right um this
29:23
is something that people have been talking about for a very long time this is not an internet problem Walter Litman
29:29
was writing about this stuff in the 1920s um and he had these interesting ideas about what journalism could be but
29:35
then he also uh you know he he was um uh he was concerned that it won't be that
29:41
uh because the business model of newspapers doesn't support this kind of work um and and if if the news media
29:48
isn't going to go out and find the additional facts for me my question is can my computers do that can I can the
29:54
internet give me the facts that I need in order to objectively understand reality and so that relates to the third
30:01
part of our talk and our Call to Arms um you know like hackers have an independent view of reality that's
30:08
somewhat outside of mainstream thinking um and that enables you to see things with different eyes um there are
30:14
interesting capabilities and I'm going to talk about some examples of them um that might be useful to people to help
30:21
combat uh deception and disinformation and I think also we're we're tool makers
30:26
and Breakers um and so uh you know I think we we're not entirely cynical about the value that that new tools can
30:33
can bring um so I'm going to focus on four areas one area I it's just something
30:40
that I need to mention it's that model the adversary's capabilities and neutralize them I've mentioned that several times um there's been tremendous
30:47
work over the years here at Defcon in the misinformation Village um and there's also this thing called the disarm framework which is essentially a
30:53
kill chain for internet uh you know botn Nets and deception operations this is fantastic work we love it uh and um you
31:01
know I want to endorse it here there's lots of things that people can do in that area um because there's so much going on in that space we're not going
31:07
to delve deeper into it uh we're going to focus on some other stuff uh um particularly building tools for
31:13
information triangulation um you you know knowing like when questions have been raised
31:19
about what we're reading uh and uh identifying and collecting those that network of experts that you can use to
31:25
determine whether or not something is real so a really simple example I want to
31:31
include because it's a project that was um the first one Wiki scanner was a project that was done by somebody in this community it was a it was a small
31:38
focused project and it had interesting consequences so it shows that like you can do some hacking over a few weekends
31:44
and make something useful um so wik scanner um looked for anonymous edits to
31:49
Wikipedia and checked the um you know like who is information associated with the IP address and identified situations
31:55
where people were editing Wikipedia anonymously from the IP blocks assigned to large organizations and so you can
32:01
kind of see what narratives were being pushed in Wikipedia by certain large orgs um there were certainly interesting
32:07
results that came out of that analysis um by the way it's mothballed somebody else made a thing called Wiki Watch Dog
32:14
the source code is available for that but it's also mothballed so like this is a thing anyone in this room could pick
32:19
up and you know you know do something with um immediately and there are interesting results to be found um
32:26
sticking to the top of of Wikipedia briefly um the idea behind Wikipedia is
32:32
that um with enough eyes all bugs are shallow right so um you know everyone's reading this article and they're
32:37
removing uh fake things from it and so we get this objective truth um the problem with it is that when you visit a
32:44
page it might have been edited two seconds ago by someone pushing an agenda and you can't really tell right so um I
32:51
I wrote a paper in 2006 in which I suggested that we highlight passages that are new so that when you're reading
32:58
the article you can kind of tell which passages are less trustworthy because they haven't gone through that editing process um there were some academics
33:04
that built a thing called Wiki trust uh which like implemented a very similar set of ideas and uh with some added
33:11
analytics underneath and uh it was usable for a while so this is kind of an interesting thing and this is also moth
33:16
bald I I believe in this set of ideas uh and there's work to be done in this area
33:22
uh um but I'm also I like the highle concept that maybe my computer should tell me when I should be care about what
33:28
I'm reading um because um some objective objective criteria suggests that uh you
33:34
know this information might not be reliable right so I want to return to some of the like core information
33:41
resources that inspired the creation of the web um because uh there's a whole
33:47
lot of ideas in here for things that we don't have today um and it's a way of seeing the negative space in the
33:53
internet um so the First Resource here is a paper called we may think by vavar
33:59
Bush so vavar Bush was the guy that ran all defense related research during World War II um so he was an incredibly
34:06
important guy he wrote a paper in the 40s um which might be the most important academic paper of the 20th century um
34:13
and it's it's it's about using microfish to create um to link documents together
34:18
and create Knowledge Management Systems this guy's got a camera on his head because he imagined he wanted to take screenshots of books he was reading and
34:25
so he imagined having a camera in his head so he could screen shot documents right um so it's it's a weird Tech um
34:31
but the ideas are super interesting and he um you know had a lot of influence over to other people who are incredibly
34:38
important one is douglas engelbart douglas engelbart gave the mother of all Demos in which like he demonstrated
34:43
things like mice and modems um uh at Stanford University in the 60s um he
34:48
wrote a book about augmenting human intelligence and one of his sort of press UPS is that computers will always
34:55
augment human intelligence better than they replace it and that is an idea that I think is it makes sense to remember
35:02
these days given everything that's going on with AI and also I think social media companies sometimes forget that the purpose of computers is to make us
35:09
smarter if you forget that you'll make the wrong things um so uh Ted Nelson is
35:15
the guy who coined the term hypertext um he did so the it first appears in print
35:21
in this book which is called computer lib it's an extra- large book that's handwritten and Illustrated like a
35:28
uh it's a really interesting uh document um and you can buy a copy of it um they' they've got it printed again um a
35:35
scholarly read through these three resources will reveal a whole lot of
35:41
ideas that these people had for things that the internet should do that the internet does not yet do um late in
35:47
engelbart's life uh um he was sort of frustrated with the web and like felt like it didn't achieve its potential and
35:53
Ted Nelson is still ranting about that on YouTube and you can find him uh doing so um so if we look at some of the stuff
35:59
that these guys did so um zanado do is Nelson's project um hyperscope is something that the Douglas angelart
36:05
Institute has been working on to try to implement some of his ideas there's a bunch of things in there that we don't
36:10
have today and it's interesting to think or maybe we do but only a narrow limited context right so it's interesting to
36:16
think about like what would it take to make this a part of the internet um uh you know why why don't we have it um in
36:23
the context of deception and counter deception I'm particularly interested in back links all these guys wanted
36:29
backlinks so a web document can link out to another document from a phrase um in
36:34
their systems also when you're reading a document you can see which people Link in and what phrases they link into to so
36:41
you can see if someone built upon the ideas that you're reading um and you can also see if someone is criticizing the
36:47
thing that you're reading why don't we have that in the internet um well the answer is that it's a Content moderation
36:54
problem if anybody could annotate a web page in any way that they wanted and we'd have a bunch of spam and abuse
37:01
right um and so we need to think about the tools that we've built for managing content moderation and see if we can
37:07
couple them to uh backlinking in order to create something that's useful there have been some interesting projects over
37:13
the years that played around with backlinking either in narrow context or you know to varying degrees of success
37:20
um you know I think a key thing here is that all opinions are not created equal um there are certain opinions again I've
37:26
curated my experts whose opinions I that matter to me those are the ones that I want to see so when I choose to follow
37:32
somebody on social media I'm indicating that that what they say is relevant to me even if I don't agree with it right
37:38
so I've selected that group it would be useful to me if I could see anytime I'm viewing a document if those people have
37:45
ever commented on it right I I'd love to build that the a challenge that we face is
37:52
that in order to build and innovate on top of our social media systems we need systems that are technically financially
38:01
and culturally open to Innovation um and unfortunately many of the systems that
38:06
we uh collectively use like fail at one or more of those criteria and therefore
38:12
it makes it difficult for people to create new things that sit on top of them and we need to create new things in
38:17
order to make the internet better um so I've talked about curating
38:23
collections of experts um there are some interesting protocols that exist that people have built over time uh for um
38:30
for endorsing people like I really like LinkedIn endorsements not as a tool but as a concept um you know you can say I
38:37
follow this person and this is why right um but unfortunately that's a closed
38:42
system um some of these things out there aren't machine readable uh you know I think it would be cool if I when I'm
38:48
following somebody on social media if I could say why and that endorsement sticks to them if they accept it uh and
38:54
uh um as long as I continue to follow them and if we did that then I could do this I could say who do the people I
39:01
follow follow who are endorsed for this particular thing like law and I could
39:06
get back a list of a bunch of people who are endorsed for this particular topic and I could ask what are these people
39:11
saying about this thing right it gives me the ability to sort of like um you
39:16
know identify the reputation of people on the internet and utilize that in constructive ways to help me figure out
39:23
what's real um so I I think there are a lot there's lots of work to be done in
39:28
that space there's lots of like open potential there uh another area is llms
39:34
um you know llms have some built-in biases there's challenges dealing with them they hallucinate so it's there's
39:41
lots of caveats to what I'm saying here and I know what they are um but at the same time it is a dispassionate computer
39:48
right so can it go out and find those missing facts for me that aren't included in a narrative by reading all
39:54
the things right uh and and disp present them to me um that's a
39:59
potentially interesting thought um and I think it's harder than it sounds um but
40:05
you know there are other things we could do with llms uh I uh you know one of the things is that that I mentioned on the
40:11
last page is that there are um like if you're a university professor at an accredited University in a particular
40:16
subject area um your University website indicates that like this guy is the professor of this right but then your
40:22
social media profile there's it's not machine readable your University website is not Mach machine readable I can't
40:28
tell uh you know um automatically that you have this expertise but an llm could
40:33
go read every University website find all the professors find all their social media profiles and create a machine
40:40
readable uh you know system that allows me to again ask this question who knows Lots about economics here they are right
40:47
um and so I think that those are interesting possibilities LMS are really good at turning human unstructured
40:53
information into like very structured information that's machine readable and we can do with
40:58
it thanks so so beyond these Technical
41:03
Solutions we we have to think like how can we scale this um so for humans right
41:09
uh one is the idea of teaching Med media literacy in schools to try and have a
41:14
better informed consumers of information uh media literacy now who I have highlighted here they have a nice depth
41:22
of resources from K through 12 uh for for classroom instruction and the other
41:27
idea is to emphasize critical thinking both in school and in our day-to-day lives and this is from and I can't quite
41:34
see it Justin right um the idea there are certain questions we ask uh that we ask and help us probe through and see
41:41
through deception one is who benefits right where did this originate there's a
41:47
48 questions here and they're very thought-provoking and useful yeah so so we've um put a bunch
41:53
of uh references in the the slides uh you know which are like look look at
41:58
this from a bunch of different perspectives I'm sure that there are ideas that you have that relate to maybe
42:05
the principles that we've articulated that are different from the ones that I've come up with um and that's what I'm
42:10
excited about and why I wanted to talk to all of you about this so um hopefully uh you know somebody out there has been
42:17
maybe inspired to work on something uh and um uh you know there's there's lots more to read about all of this stuff um
42:23
you know these are our email addresses uh you know we'd love to hear from you um we're going to be hanging out outside
42:29
after the um Talk is over so if you want to talk to us or poke at some of the things we're saying we'd love to talk to
42:35
you more um and uh again I really appreciate your time and attention this morning uh um and the fact that you made
42:41
it here on the last day of uh of Defcon for a morning session thank you
42:48
[Applause]


**ASSISTANT**

This transcript provides a fascinating window into the presentation given by Tom Cross and Greg KY at DEF CON, where they explore the interplay of deception and counter-deception in both military and cybersecurity contexts. The content they cover blends practical principles of deception, historical examples, modern applications, and potential solutions for tackling deception in the information age.

Here’s a brief **summary of the key takeaways**:

### **Deception Principles**:
1. **Exploiting Preexisting Beliefs**: Reinforcing what the target already believes is more effective than attempting to change their mindset. 
   - *Example*: Misinformation that aligns with someone's political bias is more easily accepted.
2. **Limits of Human and Machine Perception**: Deception exploits the limitations of sensing and processing systems, such as spoofing self-driving cars or creating realistic physical decoys.
3. **Narrative Construction**: Effective deception involves piecing together narratives that the target assembles themselves, creating a false but coherent story.
4. **Ambiguity vs. Misdirection**: Deception either creates doubt or convinces the target to fixate on a specific falsehood.

### **Counter-Deception Strategies**:
1. **Critical Analysis**: Playing devil's advocate with your own beliefs, asking how easy it is for adversaries to simulate evidence.
2. **Triangulating Information**: Leveraging multiple independent sources and sensors to reduce the impact of manipulated data.
3. **Pre-Bunking and Feedback Loops**: Anticipating and discrediting deceptive narratives before they spread.

### **Applications in Cybersecurity**:
- Deception targets not only humans (phishing, misinformation) but also machines (AI poisoning, malware analysis).
- Techniques like rotating C2 infrastructure, polymorphic malware, and metadata manipulation can deceive analysts.

### **Call to Arms for Hackers**:
1. Build tools that enhance **information triangulation** and detect deceptive patterns.
2. Develop systems that increase transparency, such as backlinks or highlighting newly added content on Wikipedia.
3. Use open protocols for innovation, enabling decentralized but trusted mechanisms to establish expertise or verify facts.

### **Human-Centric Solutions**:
- Promote **media literacy** and **critical thinking** through education.
- Employ techniques that identify biases, such as "Who benefits from this narrative?" or "What evidence supports this claim?"

### **Final Thoughts**:
The talk emphasizes that deception and counter-deception principles have broad applicability, from military strategy to combating misinformation online. With hackers’ creativity and skill, the tools and approaches developed could fundamentally change how we engage with information in the digital age. By challenging assumptions, curating expertise, and leveraging technical innovations, we can create systems that mitigate deception’s impact while fostering a better-informed society.

---

If you'd like a deeper dive into any specific deception principles or their mathematical/technical formalizations, let me know!


**USER**

yes please


**ASSISTANT**

### **Formalizing Deception and Counter-Deception Principles**

To dive deeper into the mathematical and technical underpinnings of deception and counter-deception, we can leverage concepts from fields like [[game theory]], [[information theory]], and [[probability theory]]. Below, I outline a formalized exploration of the key principles:

---

### **1. McGruder’s Principle and Reinforcement of Preexisting Beliefs**

**Concept**: Deception is more effective when it aligns with the target's preexisting beliefs.

#### Formalization with Bayesian Updating:
Let \( B \) be the target's belief, \( E \) be the evidence presented (true or false), and \( H \) the hypothesis that aligns with \( B \). Using Bayes’ theorem:
$$
P(H|E) = \frac{P(E|H) \cdot P(H)}{P(E)}
$$

- \( P(H) \): The prior probability of the hypothesis (target's preexisting belief).
- \( P(E|H) \): The likelihood of observing evidence \( E \) if the hypothesis is true.
- \( P(E) \): The overall probability of observing \( E \).

When \( P(H) \) (the prior belief) is high, the posterior \( P(H|E) \) remains high unless \( P(E|H) \) is very low. This is why presenting evidence that aligns with \( P(H) \) (preexisting belief) has a much stronger persuasive effect than evidence that contradicts it.

**Counter-Deception**: 
Force re-evaluation of \( P(H) \) by introducing disconfirming evidence:
- Present a high-likelihood \( E' \) that contradicts \( H \), making \( P(E'|H) \) low and reducing the posterior \( P(H|E') \).
- Use multiple sources to triangulate and validate \( E' \).

---

### **2. Exploiting Sensing and Processing Limits**

**Concept**: Deception exploits human and machine limitations in data perception and processing.

#### Example: Adversarial Attacks on Machine Learning (ML)
For an ML model \( f(x) \), where \( x \) is the input and \( y = f(x) \) the output (e.g., classification), adversarial attacks involve perturbing \( x \) to create \( x' \) such that:
$$
f(x') \neq f(x), \quad \text{where } \|x - x'\| \text{ is small.}
$$

Here, \( \|x - x'\| \) is a norm (e.g., \( \ell_2 \) norm) measuring the perceptual similarity between \( x \) and \( x' \). The deception takes advantage of the model's inability to differentiate subtle changes in \( x \).

**Counter-Deception**:
- Use **ensemble models** that compare multiple sensing mechanisms.
- Introduce **robustness** through adversarial training: expose the model to adversarial examples during training, minimizing:
$$
\min_{\theta} \max_{\delta: \|\delta\| \leq \epsilon} L(f_\theta(x + \delta), y),
$$
where \( \theta \) are the model parameters, \( \delta \) the perturbation, and \( L \) the loss function.

---

### **3. Narrative Construction and Information Poisoning**

**Concept**: Deception relies on providing carefully sequenced data points that lead the target to construct the intended narrative.

#### Formalization with Markov Chains:
A sequence of information pieces \( \{X_1, X_2, ..., X_n\} \) can be modeled as a Markov chain:
$$
P(X_{n+1} | X_1, X_2, ..., X_n) = P(X_{n+1} | X_n),
$$
where the transition probabilities \( P(X_{n+1} | X_n) \) determine how the narrative evolves.

A deceiver manipulates the chain to maximize the likelihood of reaching the intended conclusion \( S \):
$$
\max \prod_{i=1}^{n-1} P(X_{i+1} | X_i), \quad \text{subject to } X_n = S.
$$

**Counter-Deception**:
- Introduce **independent paths** to evaluate \( S \) (e.g., alternative narratives \( \{X'_1, X'_2, ..., X'_m\} \)).
- Apply entropy measures to assess the uncertainty in the narrative:
$$
H(X) = -\sum_{i} P(X_i) \log P(X_i).
$$
High entropy indicates multiple plausible narratives, suggesting potential deception.

---

### **4. Ambiguity vs. Misdirection**

**Concept**: Deception either creates doubt (ambiguity) or convinces the target to focus on a specific falsehood (misdirection).

#### Game-Theoretic Model:
In a deception game, two players (deceiver and defender) choose strategies \( S_D \) and \( S_C \) respectively:
- \( S_D \): Defender’s strategy (e.g., analytic rigor, sensor placement).
- \( S_C \): Deceiver’s strategy (e.g., ambiguity or misdirection).

The utility functions for both players are:
$$
U_C(S_C, S_D) = P(\text{Deception Successful}) \cdot G - C,
$$
$$
U_D(S_C, S_D) = -P(\text{Deception Successful}) \cdot L + P(\text{Deception Detected}) \cdot R,
$$
where:
- \( G \): Gain for deceiver if deception succeeds.
- \( C \): Cost for the deceiver.
- \( L \): Loss for the defender if deception succeeds.
- \( R \): Reward for detecting deception.

Optimal strategies for both players depend on the Nash equilibrium:
$$
\{S_C^*, S_D^*\} = \arg \max U_C, U_D.
$$

**Counter-Deception**:
- Shift the cost-benefit ratio by increasing \( C \) (cost of deception) through surveillance, disruption, or deterrence.
- Reduce \( G \) by undermining the target’s belief systems or exposing deception attempts publicly.

---

### **5. Feedback Loops and Detection**

**Concept**: Professional deceivers monitor the target’s behavior for signs of belief in the deception.

#### Signal Detection Framework:
Given an observed response \( R \), the deceiver seeks to maximize the probability of effective deception:
$$
P(\text{Deception Effective} | R) = \frac{P(R | \text{Deception Effective}) P(\text{Deception Effective})}{P(R)}.
$$

The defender can manipulate \( R \) to falsify the deceiver’s assessment (e.g., acting deceived while gathering intelligence).

**Counter-Deception**:
- Use **controlled signaling** to confuse feedback loops.
- Employ **pre-bunking**: publicize anticipated false narratives before they spread, reducing their effectiveness.

---

### Conclusion

The mathematical and technical principles underlying deception and counter-deception provide a rich framework for designing effective defenses against manipulation. Techniques from game theory, probability, and machine learning can quantify risks, analyze patterns, and mitigate vulnerabilities. These tools empower defenders to uncover deceptive strategies and protect systems and individuals alike. 

Let me know if you'd like a deeper dive into any specific principle or its real-world application!


---

preceding: [[military deception theory]]

See also: [[information environment]]


See also: [[target audience]]


See also: [[media literacy]]


See also: [[feedback loops]]


See also: [[critical thinking]]


See also: [[utility functions]]