---
date: 1727217518.025293
tags:
- autogenerated
- source__oai
- conversation_id__66f33f6d-92fc-8008-8f17-a52270d28744
- msg_aaa29732-f09b-4c1e-85e6-c242f8c35115
- msg_e3097855-b958-426e-baab-9db10442c3ee
- colorclass/critical-thinking
title: cap theorem
---

**USER**

CAP theorem


**ASSISTANT**

The **CAP theorem** is a fundamental principle in distributed systems that characterizes the trade-offs between three properties of a system: **Consistency**, **Availability**, and **Partition tolerance**. Formulated by computer scientist Eric Brewer in 2000, it provides a framework to understand the limitations of distributed systems, particularly in the context of networks where failures can occur.

### Key Concepts

1. **Consistency (C)**: This property ensures that all nodes in a distributed system see the same data at the same time. After a write operation completes, every subsequent read returns that value. This is the strong form of consistency, often called *linearizability* in distributed computing, which implies that the system behaves as if operations are executed sequentially on a single node.

    - Formally, consistency means that for any two replicas \( R_i \) and \( R_j \), after a write operation \( W \) is acknowledged, all read operations \( R_i(x) \) and \( R_j(x) \) on the same data item \( x \) will return the same value.

2. **Availability (A)**: This property ensures that every request (read or write) received by a non-failing node in the system gets a response, even if that response is not the most recent value (i.e., in the event of temporary inconsistencies). In other words, the system is operational and responsive regardless of internal failures or slowdowns.

    - Mathematically, for any operation \( O(x) \), there exists a finite time \( t \) such that the operation completes and returns a result.

3. **Partition Tolerance (P)**: This property ensures that the system continues to function despite network partitions, which occur when communication between some nodes in the system is lost or delayed. In the presence of a partition, some nodes will be isolated, but the system should continue operating (though potentially with degraded consistency or availability).

    - Formally, a partition occurs when the set of nodes in the system is split into two or more disconnected subsets that cannot communicate. Partition tolerance guarantees that the system will still provide service (either C or A) despite this.

### The Trade-off

The CAP theorem states that in the presence of a **network partition**, a distributed system can guarantee at most **two** of the three properties (C, A, or P). This means:

- If a system is **partition-tolerant** (which is generally required in any large-scale, real-world distributed system), it must make a trade-off between **Consistency** and **Availability**:
    - **CP systems**: Prioritize Consistency and Partition tolerance but sacrifice Availability. These systems guarantee that the data is consistent, even if some nodes are unreachable (e.g., HBase, MongoDB in strict mode).
    - **AP systems**: Prioritize Availability and Partition tolerance but may sacrifice Consistency. These systems always respond to queries but may return stale or inconsistent data (e.g., DynamoDB, Cassandra).
    - **CA systems**: These would guarantee Consistency and Availability, but cannot tolerate network partitions. Such systems are generally unrealistic in large-scale distributed environments because network failures (partitions) are inevitable.

### Visualization

The trade-offs can be represented in a triangle diagram, with **C**, **A**, and **P** at the vertices. In a partition-tolerant environment, you can only select one side of the triangle:
- **CP** (Consistency + Partition Tolerance)
- **AP** (Availability + Partition Tolerance)

Realistically, you cannot achieve **CA** (Consistency + Availability) under network partitions.

### Proof Outline

The informal proof of the CAP theorem goes as follows:
1. Assume that a network partition has occurred, splitting nodes into two subsets.
2. To maintain **Availability**, the system must process requests in both partitions.
3. However, this can lead to inconsistencies, as one partition may update data without the other partition knowing about it.
4. To maintain **Consistency**, you would need to block operations in one or both partitions (thus sacrificing **Availability**).

This tension implies that no distributed system can guarantee **Consistency**, **Availability**, and **Partition tolerance** simultaneously when a partition occurs.

### Relaxations of CAP

Distributed system designs have evolved to relax the strict trade-offs posed by CAP. Some notable approaches include:

- **Eventual Consistency**: Systems like [[DynamoDB]] and [[Cassandra]] may sacrifice strict consistency (linearizability) for **eventual consistency**. Here, the system guarantees that, in the absence of further updates, all replicas will converge to the same value over time.
  
- **Strong vs. Weak Consistency**: [[Spanner]] (a Google system) aims to provide strong consistency by using techniques such as synchronized clocks, but it involves complex trade-offs to minimize the impact on availability.

- **BASE systems** (Basically Available, Soft-state, Eventually consistent): These systems explicitly give up on consistency for availability in a network partition scenario.

### Conclusion

In summary, the **CAP theorem** forces distributed system designers to prioritize two of the three properties: **Consistency**, **Availability**, and **Partition tolerance**. While achieving all three simultaneously is impossible during a network partition, modern systems use a variety of techniques to balance these properties, often employing **weak** or **eventual consistency** models to provide practical performance guarantees.


---


following: [[consistency patterns]]