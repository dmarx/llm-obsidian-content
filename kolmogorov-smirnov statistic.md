---
date: 1717801444.549911
tags:
- autogenerated
- source__oai
- conversation_id__77e70e10-f02a-4f98-bd5c-6217b10122cf
- msg_aaa21498-dab3-4169-b305-f56cffefdec3
- msg_c92615cb-9fca-4e8c-95f9-ddf5bab5dde5
- colorclass/philosophy-of-language
title: kolmogorov-smirnov statistic
---

**USER**

Kolmogorov-Smirnov Statistic


**ASSISTANT**

The Kolmogorov-Smirnov (KS) statistic is a non-parametric test used to compare a sample with a reference probability distribution (one-sample test) or to compare two samples (two-sample test). It quantifies the distance between the empirical distribution function of the sample and the cumulative distribution function of the reference distribution, or between the empirical distribution functions of two samples.

### One-Sample Kolmogorov-Smirnov Test

#### Definition

Given a sample $X_1, X_2, \ldots, X_n$ drawn from a distribution with cumulative distribution function (CDF) $F(x)$, the empirical distribution function (EDF) $F_n(x)$ is defined as:
$$ F_n(x) = \frac{1}{n} \sum_{i=1}^n \mathbf{1}_{(-\infty, x]}(X_i), $$
where $\mathbf{1}_{(-\infty, x]}$ is the indicator function.

The one-sample KS statistic is defined as the maximum absolute difference between the empirical distribution function and the reference CDF:
$$ D_n = \sup_{x} |F_n(x) - F(x)|. $$

#### Asymptotic Distribution

Under the null hypothesis that the sample is drawn from the distribution with CDF $F$, the KS statistic $D_n$ (appropriately scaled) converges in distribution to the Kolmogorov distribution:
$$ \sqrt{n} D_n \rightsquiggle \sup_{0 \leq t \leq 1} |B(t)|, $$
where $B(t)$ is a Brownian bridge defined on $[0,1]$. The distribution function of $\sup_{0 \leq t \leq 1} |B(t)|$ is given by:
$$ P\left(\sup_{0 \leq t \leq 1} |B(t)| \leq z \right) = 1 - 2 \sum_{k=1}^{\infty} (-1)^{k-1} e^{-2k^2z^2}. $$

### Two-Sample Kolmogorov-Smirnov Test

#### Definition

Given two independent samples $X_1, X_2, \ldots, X_m$ and $Y_1, Y_2, \ldots, Y_n$ with respective empirical distribution functions $F_m(x)$ and $G_n(x)$, the two-sample KS statistic is defined as:
$$ D_{m,n} = \sup_{x} |F_m(x) - G_n(x)|. $$

#### Asymptotic Distribution

Under the null hypothesis that the two samples come from the same distribution, the KS statistic (appropriately scaled) converges in distribution to the Kolmogorov distribution:
$$ \sqrt{\frac{mn}{m+n}} D_{m,n} \rightsquiggle \sup_{0 \leq t \leq 1} |B(t)|, $$
where $B(t)$ is a Brownian bridge.

### Practical Implementation

#### One-Sample KS Test

1. **Null Hypothesis**: The sample comes from the specified distribution $F$.
2. **Calculate the Empirical Distribution Function**:
   $$ F_n(x) = \frac{1}{n} \sum_{i=1}^n \mathbf{1}_{(-\infty, x]}(X_i). $$
3. **Compute the KS Statistic**:
   $$ D_n = \sup_{x} |F_n(x) - F(x)|. $$
4. **Determine the p-Value**:
   Use the asymptotic distribution to determine the p-value or compare $D_n$ to critical values from the KS distribution.

#### Two-Sample KS Test

1. **Null Hypothesis**: The two samples come from the same distribution.
2. **Calculate the Empirical Distribution Functions**:
   $$ F_m(x) = \frac{1}{m} \sum_{i=1}^m \mathbf{1}_{(-\infty, x]}(X_i), \quad G_n(x) = \frac{1}{n} \sum_{i=1}^n \mathbf{1}_{(-\infty, x]}(Y_i). $$
3. **Compute the KS Statistic**:
   $$ D_{m,n} = \sup_{x} |F_m(x) - G_n(x)|. $$
4. **Determine the p-Value**:
   Use the asymptotic distribution to determine the p-value or compare $D_{m,n}$ to critical values from the KS distribution.

### Applications

1. **Goodness-of-Fit Testing**:
   The KS test is widely used to determine if a sample comes from a specified distribution, such as normal, exponential, or uniform distributions.

2. **Comparing Distributions**:
   The two-sample KS test is used to compare two samples to determine if they come from the same distribution, useful in fields like biology, finance, and quality control.

3. **Model Validation**:
   The KS test can be used to validate probabilistic models by comparing the model's predicted distribution to observed data.

### Limitations

- **Sensitivity to Ties**:
  The KS test can be sensitive to ties in the data, which can occur frequently in discrete distributions.

- **Distributional Assumptions**:
  The test is based on the assumption of a continuous distribution under the null hypothesis. For discrete distributions, modifications or alternative tests (e.g., chi-squared) may be more appropriate.

### Further Reading

For a deeper understanding of the Kolmogorov-Smirnov test and its applications, consider the following references:

- "All of Statistics: A Concise Course in Statistical Inference" by [[Larry Wasserman]].
- "Nonparametric Statistical Methods" by [[Myles Hollander]] and [[Douglas A. Wolfe]].
- "Introduction to the Theory of Statistics" by [[Alexander M. Mood]], [[Franklin A. Graybill]], and [[Duane C. Boes]].

These texts provide detailed explanations, derivations, and examples of the KS test and other nonparametric methods.


---

preceding: [[brownian bridge]]  


following: [[polyakâ€“ruppert averaging]]