---
date: 1718749925.239848
tags:
- autogenerated
- source__oai
- conversation_id__52ea3b08-192d-414e-b3a9-223e5e909b00
- msg_feeb0e90-e163-4e68-9764-ac051806b9d8
- msg_b15b68d7-1bf4-411b-b721-116046ee04c8
- colorclass/information-theory
title: sufficient statistic
---

**USER**

Sufficient statistic


**ASSISTANT**

A **sufficient statistic** is a crucial concept in the field of statistical inference. It provides a way to summarize the data in such a manner that no information about the parameter(s) of interest is lost. Formally, a statistic \( T(X) \) is sufficient for a parameter \( \theta \) if the conditional distribution of the data \( X \) given the statistic \( T(X) \) does not depend on \( \theta \).

### Definition
Let \( X = (X_1, X_2, \ldots, X_n) \) be a random sample from a probability distribution with parameter \( \theta \). A statistic \( T(X) \) is sufficient for \( \theta \) if the conditional probability (or probability density) of the sample \( X \) given \( T(X) = t \) is independent of \( \theta \).

Mathematically, \( T(X) \) is sufficient for \( \theta \) if the joint probability density function (pdf) or probability mass function (pmf) of \( X \) can be factorized as:
$$
f_{X|\theta}(x|\theta) = h(x) g(T(x), \theta)
$$
where \( h(x) \) is a function that does not depend on \( \theta \) and \( g(T(x), \theta) \) is a function that depends on \( x \) only through \( T(x) \) and \( \theta \).

### Fisher's Factorization Theorem
Fisher's factorization theorem provides a practical criterion for checking sufficiency. According to the theorem, \( T(X) \) is sufficient for \( \theta \) if and only if the joint pdf or pmf of the sample \( X \) can be factorized in the form:
$$
f_{X|\theta}(x|\theta) = g(T(x), \theta) h(x)
$$

### Examples
1. **Bernoulli Distribution**: Suppose \( X_1, X_2, \ldots, X_n \) are i.i.d. Bernoulli random variables with parameter \( \theta \). The likelihood function is:
   $$
   L(\theta; x) = \theta^{\sum x_i} (1-\theta)^{n - \sum x_i}
   $$
   Here, \( T(X) = \sum_{i=1}^n X_i \) is a sufficient statistic for \( \theta \).

2. **Normal Distribution**: Suppose \( X_1, X_2, \ldots, X_n \) are i.i.d. normal random variables with mean \( \mu \) and variance \( \sigma^2 \). The likelihood function is:
   $$
   L(\mu, \sigma^2; x) = (2\pi \sigma^2)^{-n/2} \exp\left(-\frac{1}{2\sigma^2} \sum_{i=1}^n (x_i - \mu)^2 \right)
   $$
   Here, \( T(X) = \left( \sum_{i=1}^n X_i, \sum_{i=1}^n X_i^2 \right) \) is a sufficient statistic for \( (\mu, \sigma^2) \).

### Minimal Sufficiency
A sufficient statistic \( T(X) \) is said to be **minimal sufficient** if it can be expressed as a function of any other sufficient statistic. Formally, \( T(X) \) is minimal sufficient if for any two samples \( x \) and \( y \), \( T(x) = T(y) \) implies that the ratio of their likelihoods is independent of \( \theta \).

### Exponential Family
For distributions in the [[Exponential Family|exponential family]], the natural parameterization reveals sufficient statistics naturally. A distribution belongs to the exponential family if its pdf or pmf can be written as:
$$
f_{X|\theta}(x|\theta) = h(x) \exp\left( \eta(\theta)^T T(x) - A(\theta) \right)
$$
Here, \( \eta(\theta) \) is the natural parameter, \( T(x) \) is the sufficient statistic, and \( A(\theta) \) is the log-partition function.

### Practical Importance
The concept of sufficiency is crucial in simplifying statistical problems. Using sufficient statistics, one can reduce the dimensionality of the data without losing information about the parameter \( \theta \). This leads to more efficient estimation and hypothesis testing procedures. In Bayesian inference, sufficient statistics simplify the computation of posterior distributions.

### Summary
To summarize, a sufficient statistic captures all the information about a parameter \( \theta \) present in the sample. It enables efficient data reduction and plays a fundamental role in statistical inference and the theory of estimation.

For more detailed study, you might look into topics such as the [[Rao-Blackwell Theorem]], which connects sufficiency with the improvement of estimators, or delve into the properties and applications of the exponential family of distributions.


---


following: [[minimal sufficient statistic]]

See also: [[rao-blackwellization]]


See also: [[breadth-first expansion at depth 4]]


See also: [[in the moment parameter space, these paths represent different statistical properties.]]


See also: [[likelihood principle]]


See also: [[dual structures are particularly prominent in the study of exponential and mixture families of distributions, which form dual pairs on a statistical manifold. the natural parameter space of an exponen]]


See also: [[dually flat manifold structure in binary choice models]]


See also: [[data processing inequality]]


See also: [[whats $tilde{p}$ i think you may have introduced some notation without clearly explaining it.]]


See also: [[dually flat structure of binary choice models]]


See also: [[sophistication theory]]


See also: [[random variables]]


See also: [[geometric insight dual structures in statistical manifolds]]


See also: [[duality in information geometry]]


See also: [[minimal sufficient statistic]]


See also: [[gradient and hessian of the log-partition function]]


See also: [[information geometry]]


See also: [[connection between the log-partition function and free energy]]


See also: [[process complexity]]


See also: [[expectation parameters in statistical models]]


See also: [[opening the black box of deep neural networks via information theory]]


See also: [[unified neutral theory of biodiversity]]