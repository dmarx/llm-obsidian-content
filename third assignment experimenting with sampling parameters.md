---
date: 1726438051.090174
tags:
- autogenerated
- source__oai
- conversation_id__66e757d1-0534-8008-bb6a-afa8342b5ee2
- msg_aaa2f683-0531-4901-b269-3ffb65287e5e
- msg_23cf4e17-7273-41e8-abdd-2f8adb13068e
- colorclass/edge-case
title: third assignment experimenting with sampling parameters
---

**USER**

third assignment: experimenting with sampling parameters


**ASSISTANT**

For the third assignment, students will experiment with **sampling parameters** when interacting with the GPT API. This assignment will teach them how to adjust the behavior of the model using parameters like `temperature`, `top_p`, and `n`, which control randomness, diversity, and the number of responses generated.

These are critical concepts in both prompt engineering and agent design, as they influence how deterministic or creative the model’s responses are. By understanding and adjusting these parameters, students can gain more control over the outputs.

### **Third Assignment: Experimenting with Sampling Parameters**

### **Objective**:
- Introduce students to key sampling parameters (`temperature`, `top_p`, and `n`).
- Allow students to control these parameters via the command line using `fire` for experimentation.
- Demonstrate how these parameters impact the response behavior of the GPT model.

### **Sampling Parameters to Experiment With**:
1. **`temperature`**: Controls the randomness in the output. A low value (e.g., `0.1`) makes the model more deterministic, while a higher value (e.g., `0.7` or `1.0`) makes it more creative and random.
2. **`top_p` (nucleus sampling)**: Controls the cumulative probability for next-word sampling. Lower values like `0.1` make the model more focused on the top probable words, while higher values (e.g., `0.9`) allow for more diversity.
3. **`n`**: Number of completions to generate. Setting this to values greater than 1 returns multiple responses for each prompt.

### **Updated Script with Sampling Parameters**:

Students will now extend their CLI from the second assignment to include options for `temperature`, `top_p`, and `n`.

```python
import os
import openai
import fire

# Step 1: Fetch API key from environment variables
api_key = os.environ.get("OPENAI_API_KEY")
if not api_key:
    raise ValueError("API key not found. Set the OPENAI_API_KEY environment variable.")

# Step 2: Define a function to query GPT with sampling parameters
def query_gpt(prompt="Hello, world!", max_tokens=50, temperature=0.7, top_p=1.0, n=1):
    openai.api_key = api_key
    response = openai.Completion.create(
        model="text-davinci-003",
        prompt=prompt,
        max_tokens=max_tokens,
        temperature=temperature,  # Adjusts randomness
        top_p=top_p,              # Adjusts diversity
        n=n                       # Number of completions
    )
    return [choice.text.strip() for choice in response.choices]

# Step 3: Add CLI using Fire with additional parameters for sampling
def main(prompt="Hello, world!", max_tokens=50, temperature=0.7, top_p=1.0, n=1):
    """
    CLI for experimenting with OpenAI GPT-3 sampling parameters.
    
    Arguments:
    - prompt: The input prompt for the model.
    - max_tokens: Maximum number of tokens to generate.
    - temperature: Controls the randomness of the response (0.0 - 1.0).
    - top_p: Controls the diversity of the response (0.0 - 1.0).
    - n: Number of completions to generate.
    """
    responses = query_gpt(prompt, max_tokens, temperature, top_p, n)
    
    for i, response in enumerate(responses):
        print(f"\nAI Response {i+1}:\n{response}")

if __name__ == "__main__":
    fire.Fire(main)
```

### **Running the Script with Sampling Parameters**:

Students can now pass in the sampling parameters from the command line and observe how the model’s behavior changes based on these settings.

1. **With default settings** (using temperature = 0.7, top_p = 1.0, n = 1):
   ```bash
   python hello_gpt_sampling.py --prompt "Tell me a story about a dragon."
   ```

2. **Low `temperature`** for more deterministic output:
   ```bash
   python hello_gpt_sampling.py --prompt "Tell me a story about a dragon." --temperature 0.2
   ```

3. **High `temperature`** for creative and random output:
   ```bash
   python hello_gpt_sampling.py --prompt "Tell me a story about a dragon." --temperature 0.9
   ```

4. **`top_p` set to 0.5** for more focused output:
   ```bash
   python hello_gpt_sampling.py --prompt "Describe a sunset." --top_p 0.5
   ```

5. **Generate multiple completions with `n=3`**:
   ```bash
   python hello_gpt_sampling.py --prompt "Give me a fact about the universe." --n 3
   ```

### **Assignment Tasks**:
1. **Experiment with `temperature`**:
   - Ask students to run the script with different `temperature` values and observe how it affects the randomness and creativity of the responses.
   - Example prompts:
     - `"What is the capital of France?"` (expected to be more deterministic at low temperatures).
     - `"Write a poem about the moon."` (expected to be more creative at higher temperatures).

2. **Experiment with `top_p`**:
   - Ask students to compare the behavior of the model with different `top_p` values, from 0.1 to 1.0, and observe how it influences the diversity of word choices.
   - Example prompt:
     - `"Explain quantum mechanics in simple terms."` (observing how focused or diverse the explanation becomes).

3. **Experiment with `n` to generate multiple completions**:
   - Have students generate multiple responses to a single prompt by adjusting `n` and compare the differences between the completions.
   - Example prompt:
     - `"Tell me a fun fact about history."` with `n=3`.

### **Learning Outcomes**:
- **Understanding Sampling Parameters**:
  - **`temperature`**: How randomness can make responses more or less creative.
  - **`top_p`**: How this affects the diversity of responses by limiting the word selection to a more likely subset of predictions.
  - **`n`**: Generating multiple completions and comparing how diverse they are.
  
- **Experimentation**:
  - Students will directly see how model behavior shifts based on these parameters and get hands-on experience controlling LLM outputs for different use cases.
  
- **Prompt Engineering (Advanced)**:
  - By combining prompt experimentation from earlier assignments with sampling parameters, students will better understand how to fine-tune and control LLM behavior in different contexts, leading to more sophisticated and predictable outputs.

### **Assignment Extension (Optional)**:
Students can create a grid of parameter combinations to systematically experiment with different values and document their observations. For example, they could test combinations of `temperature` and `top_p` with various prompts and compare results in a table.

```bash
python hello_gpt_sampling.py --prompt "Tell me a joke." --temperature 0.3 --top_p 0.5 --n 3
```

---

### **Summary**:

This third assignment deepens students' understanding of how LLMs behave under different sampling strategies, equipping them with the skills to tune responses based on context—whether for creativity, determinism, or diversity. By allowing them to control and experiment with `temperature`, `top_p`, and `n`, students get hands-on experience with parameters critical to both prompt and agent engineering.


---

preceding: [[add a simple cli using `fire` and make the second assignment passing in a prompt parameter]]  


following: [[introduce the `list` type in the context of sending multiple prompts]]

See also: [[behavior change]]