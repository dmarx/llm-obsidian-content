---
date: 1717294033.434317
tags:
- autogenerated
- source__oai
- conversation_id__ca65c423-5293-48ca-a72e-29630dbc5375
- msg_aaa2d60e-3c27-4adb-a0d4-fc29da7d5746
- msg_d0d96cc8-fc83-40e3-9619-226edf32305c
- colorclass/decision-making-processes
title: actor-critic methods in reinforcement learning
---

**USER**

actor-critic methods


**ASSISTANT**

## Actor-Critic Methods in Reinforcement Learning

### Overview

Actor-Critic methods combine the strengths of value-based and policy-based [[reinforcement learning]] (RL) approaches. These methods maintain two separate models: the **actor**, which is responsible for selecting actions, and the **critic**, which evaluates the actions by estimating the value function. The critic provides feedback to the actor to improve its policy, leading to more efficient learning.

### Key Components

1. **Actor**: Learns the policy, \(\pi_\theta(a|s)\), which is used to select actions.
2. **Critic**: Estimates the value function, \(V^\pi(s)\) or the action-value function, \(Q^\pi(s, a)\), to evaluate the quality of actions taken by the actor.

### Objective

The objective of actor-critic methods is to optimize the policy to maximize the expected cumulative reward by updating both the actor and the critic based on feedback from the environment.

### Formal Definition

1. **Policy (\(\pi_\theta\))**: The actor's policy parameterized by \(\theta\), mapping states to actions.
2. **Value Function (\(V^\pi\) or \(Q^\pi\))**: The critic's value function parameterized by \(\phi\), estimating the expected return.

### Algorithm Components

#### 1. Actor

The actor's policy \(\pi_\theta(a|s)\) is updated using the gradient ascent on the expected return. The policy gradient theorem gives the gradient of the expected return with respect to \(\theta\):

\[
\nabla_\theta J(\theta) = \mathbb{E}_\pi \left[ \nabla_\theta \log \pi_\theta(a|s) A^\pi(s, a) \right]
\]

where \(A^\pi(s, a)\) is the advantage function, often defined as:

\[
A^\pi(s, a) = Q^\pi(s, a) - V^\pi(s)
\]

#### 2. Critic

The critic updates its value function estimate to reduce the temporal difference (TD) error, which is the difference between the estimated value and the observed return. The TD error for the value function \(V^\pi(s)\) is given by:

\[
\delta_t = R_{t+1} + \gamma V^\pi(s_{t+1}) - V^\pi(s_t)
\]

The critic minimizes the TD error by updating its parameters \(\phi\):

\[
\phi \leftarrow \phi - \beta \nabla_\phi \left( \delta_t \right)^2
\]

### Advantage Actor-Critic (A2C) Algorithm

The Advantage Actor-Critic (A2C) algorithm is a popular actor-critic method where the advantage function is used to reduce variance in the policy gradient estimates.

#### A2C Algorithm Steps

1. **Initialization**:
   - Initialize actor parameters \(\theta\).
   - Initialize critic parameters \(\phi\).
   - Set learning rates \(\alpha\) for the actor and \(\beta\) for the critic.

2. **Sampling**:
   - For each episode:
     - Initialize state \(s_0\).
     - For each time step \(t\):
       - Select action \(a_t\) according to policy \(\pi_\theta(a_t | s_t)\).
       - Execute action \(a_t\), observe reward \(R_{t+1}\) and next state \(s_{t+1}\).
       - Store \((s_t, a_t, R_{t+1}, s_{t+1})\).

3. **Return and Advantage Calculation**:
   - For each time step \(t\) in the episode:
     - Estimate value \(V^\pi(s_t)\) using the critic.
     - Compute TD error \(\delta_t = R_{t+1} + \gamma V^\pi(s_{t+1}) - V^\pi(s_t)\).
     - Compute advantage \(A^\pi(s_t, a_t) = \delta_t\).

4. **Policy and Value Update**:
   - For each time step \(t\) in the episode:
     - Update actor parameters \(\theta\):

     \[
     \theta \leftarrow \theta + \alpha \nabla_\theta \log \pi_\theta(a_t | s_t) A^\pi(s_t, a_t)
     \]

     - Update critic parameters \(\phi\) by minimizing the loss:

     \[
     \phi \leftarrow \phi + \beta \nabla_\phi \delta_t^2
     \]

### Example: Advantage Actor-Critic (A2C) Algorithm in Python

```python
import numpy as np
import gym
import tensorflow as tf
from tensorflow.keras import layers

env = gym.make('CartPole-v1')
state_dim = env.observation_space.shape[0]
n_actions = env.action_space.n

# Actor network
class Actor(tf.keras.Model):
    def __init__(self):
        super(Actor, self).__init__()
        self.dense1 = layers.Dense(24, activation='relu')
        self.dense2 = layers.Dense(24, activation='relu')
        self.logits = layers.Dense(n_actions)
    
    def call(self, state):
        x = self.dense1(state)
        x = self.dense2(x)
        return self.logits(x)

# Critic network
class Critic(tf.keras.Model):
    def __init__(self):
        super(Critic, self).__init__()
        self.dense1 = layers.Dense(24, activation='relu')
        self.dense2 = layers.Dense(24, activation='relu')
        self.value = layers.Dense(1)
    
    def call(self, state):
        x = self.dense1(state)
        x = self.dense2(x)
        return self.value(x)

actor = Actor()
critic = Critic()
actor_optimizer = tf.keras.optimizers.Adam(learning_rate=0.001)
critic_optimizer = tf.keras.optimizers.Adam(learning_rate=0.001)
gamma = 0.99

def compute_returns(rewards, gamma):
    returns = []
    G = 0
    for r in reversed(rewards):
        G = r + gamma * G
        returns.insert(0, G)
    return returns

def train_step(states, actions, rewards, next_states, done):
    returns = compute_returns(rewards, gamma)
    states = np.vstack(states)
    actions = np.array(actions)
    returns = np.array(returns)
    next_states = np.vstack(next_states)
    
    with tf.GradientTape() as tape:
        values = tf.squeeze(critic(states))
        next_values = tf.squeeze(critic(next_states))
        target = returns + gamma * next_values * (1 - done)
        critic_loss = tf.reduce_mean((target - values) ** 2)
    critic_grads = tape.gradient(critic_loss, critic.trainable_variables)
    critic_optimizer.apply_gradients(zip(critic_grads, critic.trainable_variables))

    with tf.GradientTape() as tape:
        logits = actor(states)
        action_probs = tf.nn.softmax(logits)
        action_indices = tf.range(len(actions)) * n_actions + actions
        picked_action_probs = tf.gather(tf.reshape(action_probs, [-1]), action_indices)
        advantage = target - values
        actor_loss = -tf.reduce_mean(tf.math.log(picked_action_probs) * advantage)
    actor_grads = tape.gradient(actor_loss, actor.trainable_variables)
    actor_optimizer.apply_gradients(zip(actor_grads, actor.trainable_variables))

# Training loop
for episode in range(1000):
    state = env.reset()
    states = []
    actions = []
    rewards = []
    next_states = []
    done = False
    
    while not done:
        state = np.expand_dims(state, axis=0).astype(np.float32)
        logits = actor(state)
        action = np.random.choice(n_actions, p=tf.nn.softmax(logits).numpy()[0])
        next_state, reward, done, _ = env.step(action)
        
        states.append(state)
        actions.append(action)
        rewards.append(reward)
        next_states.append(next_state)
        
        state = next_state
    
    train_step(states, actions, rewards, next_states, done)
    
    if episode % 100 == 0:
        total_reward = sum(rewards)
        print(f'Episode {episode}: Total Reward: {total_reward}')
```

### Applications of Actor-Critic Methods

1. **Robotics**: Training robots for tasks like manipulation, navigation, and control.
2. **Game AI**: Developing strategies for complex games such as Go, chess, and video games.
3. **Finance**: Optimizing trading strategies and portfolio management.
4. **Healthcare**: Personalizing treatment plans and optimizing drug discovery.
5. **Recommendation Systems**: Enhancing user recommendations and content personalization.

### Conclusion

Actor-Critic methods effectively combine the advantages of value-based and policy-based [[reinforcement learning]] by maintaining and optimizing both an actor (policy) and a critic (value function). These methods can handle complex, high-dimensional environments and learn sophisticated policies for a wide range of applications. The integration of actor-critic methods like A2C allows for efficient and stable learning, making them a powerful tool in the RL toolkit. Implementing these methods involves careful tuning of hyperparameters and understanding of both policy and value function optimization to achieve the desired performance.


---

preceding: [[policy gradient theorem]]  


following: [[value functions in [[reinforcement learning]]]]

See also: [[reinforcement learning]]