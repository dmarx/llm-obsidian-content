---
date: 1717803962.421801
tags:
- autogenerated
- source__oai
- conversation_id__77e70e10-f02a-4f98-bd5c-6217b10122cf
- msg_aaa2bff0-bea3-4cd3-82ab-b0a418b57712
- msg_6e35bb92-5549-49e6-8505-cc457ba9ac8d
- colorclass/information-theory
title: law of large numbers for markov chains
---

**USER**

law of large numbers for markov chains


**ASSISTANT**

The Law of Large Numbers (LLN) for Markov chains is a fundamental result in the theory of stochastic processes. It generalizes the classical LLN, which applies to sequences of independent and identically distributed (i.i.d.) random variables, to the setting of Markov chains, where the sequence of random variables exhibits dependence.

### Definitions and Preliminaries

#### Markov Chain

A Markov chain is a sequence of random variables \(\{X_n\}_{n \geq 0}\) taking values in a state space \(S\) and satisfying the Markov property:
$$ P(X_{n+1} = j \mid X_n = i, X_{n-1} = i_{n-1}, \ldots, X_0 = i_0) = P(X_{n+1} = j \mid X_n = i) \quad \text{for all } n \geq 0. $$

This property implies that the future state \(X_{n+1}\) depends only on the present state \(X_n\), and not on the sequence of past states.

#### Transition Matrix

The transition probabilities of a Markov chain can be described by a transition matrix \(P\), where \(P_{ij} = P(X_{n+1} = j \mid X_n = i)\).

#### Stationary Distribution

A stationary distribution \(\pi\) is a probability distribution on the state space \(S\) such that if \(X_0 \sim \pi\), then \(X_n \sim \pi\) for all \(n \geq 0\). Mathematically, \(\pi\) satisfies:
$$ \pi_j = \sum_{i \in S} \pi_i P_{ij} \quad \text{for all } j \in S. $$

### Law of Large Numbers for Markov Chains

There are different forms of the LLN for Markov chains, depending on the properties of the chain, such as ergodicity and stationarity.

#### Ergodic Markov Chains

A Markov chain is ergodic if it is irreducible (every state can be reached from every other state) and aperiodic (the chain does not cycle through states in a fixed period).

**Strong Law of Large Numbers (SLLN) for Ergodic Markov Chains**:
Let \(\{X_n\}_{n \geq 0}\) be an ergodic Markov chain with stationary distribution \(\pi\). Let \(f: S \to \mathbb{R}\) be a function such that \(E_\pi[|f(X)|] < \infty\), where \(E_\pi\) denotes expectation with respect to the stationary distribution \(\pi\). Then,
$$ \frac{1}{n} \sum_{k=0}^{n-1} f(X_k) \to E_\pi[f(X)] \quad \text{almost surely as } n \to \infty. $$

This result states that the time average of a function \(f(X_n)\) along the trajectory of the Markov chain converges almost surely to the expectation of \(f\) with respect to the stationary distribution \(\pi\).

### Proof Sketch

The proof of the SLLN for ergodic Markov chains involves several steps:

1. **Stationary Distribution**:
   If the Markov chain starts with the stationary distribution \(\pi\), then each \(X_n\) is distributed according to \(\pi\). However, even if the chain does not start in the stationary distribution, the ergodicity ensures that it will converge to \(\pi\) in distribution.

2. **Martingale Argument**:
   The sequence \(\{f(X_n)\}\) can be decomposed into a [[martingale difference sequence]] and a remainder term. The ergodicity of the chain ensures that the remainder term converges to zero.

3. **Birkhoff's Ergodic Theorem**:
   Birkhoff's Ergodic Theorem can be applied to show that the time averages converge almost surely to the space average for functions integrable with respect to the invariant measure.

### Examples

#### Example 1: Simple Random Walk

Consider a simple symmetric random walk on \(\mathbb{Z}\), where \(P(X_{n+1} = X_n + 1 \mid X_n) = P(X_{n+1} = X_n - 1 \mid X_n) = \frac{1}{2}\). This random walk does not have a stationary distribution because it is not positive recurrent (it does not return to a state in a finite expected time).

#### Example 2: Finite-State Markov Chain

Consider a finite-state Markov chain with states \(S = \{1, 2, \ldots, m\}\) and transition matrix \(P\). If the chain is irreducible and aperiodic, it has a unique stationary distribution \(\pi\). For any function \(f: S \to \mathbb{R}\), the time average \(\frac{1}{n} \sum_{k=0}^{n-1} f(X_k)\) converges almost surely to \(E_\pi[f(X)] = \sum_{i \in S} \pi_i f(i)\).

### Applications

1. **Statistical Mechanics**:
   In the study of physical systems, Markov chains model the transitions between different states of the system. The LLN ensures that time averages of observables converge to their expected values in the stationary distribution.

2. **Queueing Theory**:
   In queueing models, customer arrivals, service times, and system states can be modeled as a Markov chain. The LLN provides long-run average performance metrics of the system, such as average queue length or waiting time.

3. **Markov Chain Monte Carlo (MCMC)**:
   MCMC methods generate samples from a target distribution by constructing a Markov chain that has the target distribution as its stationary distribution. The LLN guarantees that empirical averages of the samples converge to the expected values under the target distribution.

### Further Reading

For a comprehensive study of the LLN for Markov chains and related topics, consider the following references:

- "Markov Chains: From Theory to Implementation and Experimentation" by Paul A. Gagniuc.
- "Introduction to Stochastic Processes" by Gregory F. Lawler.
- "Stochastic Processes" by Sheldon M. Ross.

These texts provide detailed theoretical analyses, proofs, and applications of the LLN for Markov chains and other stochastic processes.


---

preceding: [[central limit theorem for stochastic approximation]]  


following: [[extensions of the law of large numbers in stochastic processes theory]]

See also: [[independent and identically distributed]]


See also: [[random variables]]


See also: [[stochastic processes]]