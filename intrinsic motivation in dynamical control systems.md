---
date: 1715994294.677944
tags:
- autogenerated
- source__oai
- conversation_id__96f06d33-d9d3-4229-95db-471bbba35129
- msg_aaa2fd74-2418-483b-b510-ab16b49bf038
- msg_81316392-f40e-49b4-b8f1-83c706a4a8c6
- colorclass/information-theory
title: intrinsic motivation in dynamical control systems
---

**USER**

Intrinsic Motivation in Dynamical Control Systems
Stas Tiomkina
, Ilya Nemenmanb,c,d, Daniel Polanie
, and Naftali Tishby∗f,g
aComputer Engineering Department, Charles W. Davidson College of Engineering San Jose State University, CA, 95192,
bDepartment of Physics, cDepartment of Biology, d
Initiative in Theory and Modeling of Living Systems, Emory University,
Atlanta, GA 30322, USA , eAdaptive Systems Research Group, University of Hertfordshire, Hatfield, UK, fThe Rachel and
Selim Benin School of Computer Science and Engineering, gEdmond and Lilly Safra Center for Brain Sciences (ELSC),
Hebrew University of Jerusalem, 96906 Israel
Abstract
Biological systems often choose actions without an explicit reward signal, a phenomenon known as intrinsic
motivation. The computational principles underlying
this behavior remain poorly understood. In this study,
we investigate an information-theoretic approach to intrinsic motivation, based on maximizing an agent’s empowerment (the mutual information between its past actions and future states). We show that this approach
generalizes previous attempts to formalize intrinsic motivation, and we provide a computationally efficient algorithm for computing the necessary quantities. We test
our approach on several benchmark control problems,
and we explain its success in guiding intrinsically motivated behaviors by relating our information-theoretic
control function to fundamental properties of the dynamical system representing the combined agent-environment
system. This opens the door for designing practical artificial, intrinsically motivated controllers and for linking
animal behaviors to their dynamical properties.
Keywords— information capacity | sensitivity gain | stabilization | predictive information
Introduction
Living organisms are able to generate behaviors that solve
novel challenges without prior experience. Can this ability
be explained by a single, generic mechanism? One proposal
is that novel, useful behaviors can be generated through intrinsic motivation [1], which is defined informally as a set
of computational algorithms that are derived directly from
the intrinsic properties of the organism-environment dynamics and not specifically learned.
Increasingly, there is a move away from reinforcement learning and its extrinsically specified reward structure [2,3] in the
theory and practice of artificial agents, robots, and machine
learning more generally [4–20]. A specific class of such intrinsic motivation algorithms for artificial systems is known as
∗Prof. Naftali Tishby passed away when this work was in development. This project began under his leadership when Stas
Tiomkin was a PhD student in his group. The rest of the authors agree that he should be a senior author on this manuscript,
but his consent for this was not obtained.
empowerment maximization. It proposes that agents should
maximize the mutual information [21] between their potential actions and a subsequent future state of the world [22].
This corresponds to maximizing the diversity of future world
states achievable as a result of the chosen actions, potentiating
a broader set of behavior options in the future. Intrinsically
motivated synthetic agents develop behaviors that are atypical for inanimate engineered systems and often resemble those
of simple living systems. Interestingly, potentiating future actions is also a key part of the success of modern reward-based
training algorithms [8, 23, 24].
Despite the successes of empowerment maximization, it remains unclear how well it can be used as a general intrinsic
motivation principle. There are many different versions of intrinsic motivation related to empowerment, and their relation
to each other is unknown [20,23,25]. Additionally, most work
on empowerment maximization has relied on simulational case
studies and ad hoc approximations, and analytical results are
scarce. In order to gain insight, it is important to link empowerment to other, better-understood characterizations of
the systems in question. Finally, calculating the mutual information between two interlinked processes in the general
case is a challenging task [26,27], which has so far limited the
use of empowerment maximization to simple cases.
In this work, we unify different versions of intrinsic motivation related to the empowerment maximization paradigm.
Here our main contribution is in showing analytically that
empowerment-like quantities are linked to the sensitivity
of the agent-environment dynamics to the agent’s actions.
This connects empowerment maximization to well-understood
properties of dynamical systems. Since highly sensitive regions of the dynamics potentiate many diverse future behaviors, the connection to dynamical systems also explains why
empowerment-based intrinsic motivations succeed in generating behaviors that resemble those of living systems.
The analytical results allow us to develop a practical computational algorithm for calculating empowerment for complex scenarios in the continuous time limit, which is the second major contribution of the paper. We apply the algorithm to standard benchmarks used in intrinsic motivation
research [14, 28, 29]. Specifically, a controller based on the
efficient calculation of empowerment manages to balance an
inverted pendula without extrinsic rewards. This opens the
door for designing complex robotic intrinsically motivated
agents with systematically computed — rather than heuristically estimated — empowerment.
1
arXiv:2301.00005v1 [cs.AI] 29 Dec 2022
C
Te,Ta,∆T

x0 ≡ x(t)

= max
p(~a|x0)
I[{XTe
, XTe−1, . . . , XTa+∆T }
| {z }
X~ −future states
; {ATa−1, ATa−2, . . . , A0}
| {z }
A~−possible actions
| x0 ≡ x(t)]
Empowerment
Ta = Te, ∆T = 0 · · ·
controlled Lyapunov expt.
Ta = 1, ∆T = Te − 1
· · ·
kicked CEF
Ta = 1, ∆T = 0
Figure 1: Unified view on information theoretical intrinsic motivation, for a discretized process sequence. Starting at time
x0 (i.e. x(t)), potential actions are applied for Ta times, following that, after waiting for ∆T time steps, the future system
trajectory is considered until Te. A controlled Lyapunov exponent is a Lyapunov exponent, but only in directions controlled
by the agent, cf., (11). “Kicked CEF” refers to a variant of Causal Entropic Forcing [30], with the addition that an action
kicks the system at the beginning of a trajectory. For more details see Generalized Empowerment.
Results
A Preliminaries
Notation We consider an agent that takes on states x(t) ∈
X := R
dx , evolving in time under the dynamics f with (small)
stochastic perturbations η(t) ∈ R
dx . Via its (small) actions,
a(t) ∈ A := R
da filtered through the control gain g, the agent
can affect the dynamics of the system:
dx(t) = f(x(t))dt + g(x(t))da(t) + dη . (1)
Here dη denotes the system noise, modeled as a Wiener process. The agent’s actions a(t) are modeled by a stochastic
control process with variance σ
2
t controlled by the agent and
with a mean of zero. This models potential effect of actions
centered around the null action.
To compute various quantities of interest, we will consider
a discretized versions of this system, for which we adopt a
modified notation. To distinguish it from the continuous
version, we replace the continuous time in parentheses by
an integer index, xk := x(t + k · ∆t). Here ∆t denotes
the physical time step, and we adopted the convention that
x0 = x(t), so that the index corresponding to the current
physical time, t, is chosen as 0. We will consider trajectories
of a fixed duration, and the agent will apply actions over a
part of that trajectory. We denote by Te the time index of
the very last state of the trajectory, which we also refer to
as the time horizon. We further use Ta to denote the (discretized) duration of the action sequence. Then state, control and perturbation trajectories at finite equidistant times,
{t+k·∆t}
T
k=0, are denoted by x
Te
0 ≡ {xk}
Te
k=0, a
Ta
0 ≡ {ak}
Ta
k=0,
and η
Te
0 ≡ {ηk}
Te
k=0, respectively. For consistency with the
control theory literature, we write a trajectory in the reverse
order, e.g., x
Te
0 = (xTe
, . . . x0). When we wish to emphasize
the continuous nature of the underlying process, we will write
te ≡ t + Te · ∆t and ta ≡ t + Ta · ∆t for explicitly continuous
times.
Reinforcement Learning vs. Intrinsic Motivation To elicit
a desired behavior in an agent, one typically uses reinforcement learning (RL). RL is task-specific, and an agent needs an
extrinsic feedback about its performance from a reward function to learn the behavior. The precise construction of this
reward function is critical to achieve a desired performance in
a short training time [2]. Some of the complications include
a significant degree of arbitrariness when choosing amongst
reward functions with equivalent performance [31] and the
difficulty of translating an often vague desired behavior into
a concrete reward function. Furthermore, complex behaviors
consist of combinations of shorter sequences. Designing a reward function capable of partitioning the solution into such
parts and hence learning it in a realistic time is hard [32].
In contrast to this, in living systems, acquisition of skills
often starts with task-unspecific learning. This endows organisms with potentiating skills, which are not rewarding on
their own. This is then followed by task-oriented specialization, which combines task-unspecific behaviors into complex
and explicitly rewarding tasks [1,33]. While specific tasks are
often refined with the help of an extrinsic reinforcement, the
potentiating tasks usually are intrinsically motivated [9].
Empowerment The type of intrinsic motivation we focus
on is empowerment. Empowerment is based on informationtheoretic quantities [4,23,30,34–40]. It defines a pseudo-utility
function on the state space, based on the system dynamics
only, without resorting to a reward. Formally, we express the
dynamics of the system by the conditional probability distribution p(xTe
| a
Te−1
0
, x0) of the resulting state when one
starts in a state x0 and subsequently carries out an action sequence a
Te−1
0
. Then the empowerment C(x0) is a function of
the starting state, x0. It is given by the maximally achievable
mutual information (the channel capacity [21]) between the
control action sequence of length Te and the final state when
starting in the state x0:
C(x0) := max
p(a
Te−1
0
|x0)
I(XTe
; A
Te−1
0
|x0). (2)
Here p(·) denotes a probability density or a probability distribution function, and I is the mutual information [21]
I(XTe
; A
Te−1
0
|x0) = H(XTe
|x0) − H(XTe
| A
Te−1
0 x0). (3)
H is the entropy, and conditioning an entropy on a random
variable means the entropy of the conditional distribution,
averaged over the conditioning variable. The empowerment
C(x0) depends on both the state, x0, and the time horizon, Te.
However, for notational convenience, we omit all parameters
from the notation except for the dependency on x0.
Locally maximizing empowerment (e.g., by following its
gradient over x0) guides an agent to perform actions atypical
within the natural dynamics of the system. Indeed, since empowerment measures the diversity of achievable future states,
maximizing it increases this diversity (“empowers” the agent –
hence the name). Thus it is expected to be particularly useful
for learning potentiating tasks [9]. Crucially, empowerment
quantifies the relation between the final state and the intentional control, rather than the diversity of states due to the
stochasticity of the system. In particular, it is not just the en
tropy of a passive diffusion process in the state variables, but
of the subprocess that the agent can actively generate. Furthermore, it quantifies diversity due to potential future action
sequences, which are not then necessarily carried out.
Empowerment is typically used in the form of the empowerment maximization principle [17], treats C(x0) as a pseudoutility function. At each time step, an agent chooses an action
to greedily optimize its empowerment at the next time step.
That is, the agent climbs up in its empowerment landscape,
eventually achieving a local maximum of C:
a
∗

x(t)

= argmax
a∈A
Eη

C

f(x(t)) + g(x(t))a∆t
0 + dη . (4)
Here A is the set of permitted actions, ∆t
0
is a small time
step used to simulate the actual behavior of the system (and
which is selected independently from the time step ∆t used to
discretize (1)). An empowerment-maximizing agent generates
its behavior by repeating this action selection procedure for
each decision step it takes.
Crucially, no general analytical solutions or efficient algorithms for numerical estimation of empowerment for arbitrary
dynamical systems are known, limiting adoption of the empowerment maximization principle. Our goal is to provide a
method to calculate it under specific approximations.
B Empowerment in dynamical systems
The linear response approximation To relate empowerment
to traditional quantities used to describe dynamical systems,
we assume that the control signal a in (1) is small. This is
true in some of the most interesting cases, where the challenge is to solve a problem with only weak controls that cannot easily “force” a solution. Under this assumption, (1) is
approximated by a linear time-variant dynamics around the
trajectories of the autonomous dynamics (i.e., for a = 0). To
proceed, we now introduce the following notation. We define ¯xs as the s-th step of the trajectory in the discretized
deterministic approximation of the dynamics (1), given by
x¯s = f(¯xs−1) + g(¯xs−1)∆as−1 (5)
with ¯x0 = x0 ≡ x(t). For example, ¯x3 = f(f(f(¯x0) +
g(¯x0)∆a0) + g(¯x1)∆a1) + g(¯x2)∆a2. We denote this recursive mapping from ¯x0 to ¯xs by F, ¯xs = F(¯x0; ∆a
s−1
0
). Then
the sensitivity of the state at the time step s to the action at
the time step r can be calculated via the iterated differentiation chain rule applied to the state derivative of the dynamics
F:
∂x¯s
∂ar
=
Ys
τ=r+2
∇x¯f(¯xτ−1) g(¯xr), (6)
where ∇x¯f(¯xτ ) is the dx ×dx Jacobian matrix, which approximates f up to the linear order in the state and the control.
Specifically, the (i, j)-th entry of ∇x¯f(¯xτ ) is ∂fi(¯xτ )
∂x¯τ,j
, where
indices i, j stand for components of the vectors x and f. For
s = r + 1, the expression in (6) evaluates to ∂x¯r+1
∂ar
= g(xr).
Now we define the linear response of the sequence of the
system’s states x
s2
s1
to the sequence of the agent’s actions ∆a
r2
r1
F
s1,s2
r1,r2
(x0) =
∂x¯s2
∂ar2
∂x¯s2
∂ar2−1
. . .
∂x¯s2
∂ar1
∂x¯s2−1
∂ar2
∂x¯s2−1
∂ar2−1
. . .
∂x¯s2−1
∂ar1
.
.
.
.
.
. . . .
.
.
.
∂x¯s1
∂ar2
∂x¯s1
∂ar2−1
. . .
∂x¯s1
∂ar1


























dx·s×da·r
,
(7)
where s = s2 − s1 + 1, r = r2 − r1 + 1, s + ∆T + r − 1 = Te,
and the entries are computed via (6). Usually we consider
situations where the agent applies its controls for r time steps,
and then after a gap observes the state for s steps. That is,
s1 = r2 + 1 + ∆T, where ∆T ≥ 0 is the gap between the end
of the control sequence and the start of the observations.
Notice that traditional definitions of sensitivity of a dynamical system to its controls are blocks F
s
0
1
,s0
2
r
0
1
,r0
2
in this overall sensitivity matrix, F
s1,s2
r1,r2
. For example, if r
0
1 = r
0
2 = 0,
∆T
0 = Te − 1, and s
0
1 = Te, then s
0
2 = Te, and the sensitivity
matrix collapses to just the entries that measure the sensitivity of the current state to the controls during the immediately
preceding time step, F
Te,Te
0,0
(x0) = ∂x¯Te
∂a0
. This is also the blue
block of the overall sensitivity matrix, (7).
With the definitions above, in the linear response regime,
the effect of a sequence of (small) actions on a sequence of
states, (1), becomes
∆x
s2
s1 = F
s1,s2
r1,r2
(x0)∆a
r2
r1 + ˜η, (8)
where ∆a and ∆x are the reverse-time-ordered vectors of
small actions and the induced deviations of states (which
themselves can be vectors).Here ˜η models both the total noise
resulting from the integration of the process noise dη from (1)
and the noise of the subsequent observation of the state perturbation ∆x
s2
s1
, which we assume as Gaussian.
Generalized Empowerment Since the entire dynamics is
now linear, cf. (8), we can consider formally effects of arbitrary
length sequences of actions on arbitrary length sequences of
future states. In other words, we can define the generalized
empowerment,
C
Te,Ta,∆T
(x0) := max
p(~a|x0)
I(X
Te
Ta+∆T
; A
Ta−1
0
|x0) . (9)
Here, Ta denotes the number of time steps at which actions
are performed, ∆T is the time gap between the action sequence and the beginning of the observation of the resulting
states, and Te is the last step in that observed sequence. That
is, C
Te,Ta,∆T measures the maximum mutual information between a sequence of actions and a later sequence of states,
rather than just one final state, like empowerment does.
Plugging in (8) into (4), we observe that computing the generalized empowerment in discretized time with an arbitrary
discretization step and an arbitrary time horizon Te reduces
to a traditional calculation of the channel capacity of a linear
Gaussian channel, though with a large number of dimensions
reflecting both the duration of the signal and the duration of
the response. Specifically,
C
Te,Ta,∆T
(x0) = max
Pσi≥0
i
σi=P
1
2
Xdx
i=1
ln(1 + ρi(x0)σi). (10)
Here ρi(x0) are the singular values of the appropriate submatrix F
s
0
1
,s0
2
r
0
1
,r0
2
(x0); for example, the traditional empowerment
corresponds to the red-dashed submatrix in (7). Further, P
is the power of the control signal ∆a over the whole control
period, and σi ≥ 0 is that part of the overall power of the
control signal which is associated with the i-th singular value
(called channel power). The channel power can be computed
by the usual water-filling procedure [21]. Note that here we
denote P as power, as per control-theoretic convention, but
since we fix the time interval over which it is applied, the units
of P are those of energy. As per our weak control assumption,
we assume P to be suitably small.
With (10), calculation of any generalized empowerment becomes tractable, at least in principle. This also shows explicitly that the (generalized) empowerment is a function of the
sensitivity matrix F, and with it of quantities used to characterize dynamics, such as the Lyapunov exponents.
To compute C
Te,Ta,∆T
(x0) efficiently for an arbitrary dynamical system (1) and arbitrary long time horizons and arbitrary small discretization steps, we start by discretizing the
time and calculating the linear response matrix F. While in
this paper we do this by analytical differentiation, numerical
differentiation can be used whenever f is unknown. We then
calculate the singular values of F; this is straightforward on
modern computers for dimensionalities of up to a few hundred.
Finally, we apply the “water filling” procedure to find the set
of channel powers σi to match the available total power P in
(10), and from there we calculate the (generalized) empowerment value. We will employ this approach for all examples in
this paper.
Connecting Generalized Empowerment to Related Quantities Generalized empowerment with different durations of
action and observation sequences is related to various quantities describing dynamical systems, including those defining
intrinsic motivation [8, 20, 23, 41]. For example, Causal Entropic Forcing (CEF) [20] is defined as actions that maximize
the entropy of future trajectories of a system. With Ta = 1
and ∆T = 0, C
Te,Ta,∆T
in (9) measures the immediate consequences of a single action on a trajectory with a fixed time
horizon Te. Maximizing C
Te,Ta,∆T
is then equivalent to choosing actions that maximize susceptibility, and not the entropy
of trajectories with a given time horizon. In other words, one
can interpret C
Te,1,0
as a “kicked”, or agent-controllable, version of CEF, where just the first action can be selected by
the agent at any time, and uncontrolled future variability is
discarded in action planning (see Fig. 1 for an illustration).
Such kicked CEF corresponds to the green submatrix in (7).
Now consider the top right corner (blue) of (7) with Te =
Ta = 1, or, equivalently, s
0
2 = s2 and s
0
1 = s
0
2 − 1. In the limit
of a very long horizon, s2 → ∞, the appropriate submatrix of
F is
Λ ≡ lim s2→∞ 
∂x¯s2
∂ar1
∂x¯s2
∂ar1
†
! 1
s2
, (11)
where † is the transpose, and ∂x¯s2
∂ar1
is given by (6). In the
special case that the control gain is the identity, g(x) = x,
the logarithm of the eigenvalues of Λ reduces to the usual
characteristic Lyapunov exponents of the dynamical system
[42]. However, once a more general control gain is applied,
the action-controlled perturbation, ar1 may be able to affect
only a part of the state space. This means that Λ not only
is a generalized empowerment with specific indices, but it is
also a specialization of the concept of Lyapunov exponents to
the controllable subspace. Thus we refer to the log-spectrum
of Λ as the control Lyapunov exponents, cf. Fig. 1.
In summary, (9) and the linearization, (7), provide a unified
view of various sensitivties of the dynamics to the controls,
and hence on various versions of intrinsic motivation.
C Intrinsic motivation in power-constrained agents
An agent controlling a system with unconstrained actions
can trivially reach any state in a controllable dynamical system [43] by simply forcing their desired outcome without sophisticated control. Thus to render the setup interesting, we
consider only power-constrained, or weak agents. To show
that empowerment maximization, in the linearized regime, is
an efficient control principle, we use it to stabilize a family
of inverted pendula (single pole, double pole, and cart-pole),
which are simple, paradigmatic models of important phenomena, such as human walking [44].
Solutions for the stabilization problem are known. They
require to accumulate energy by swinging the pendulum back
and forth into resonance without overshooting and then to
keep the pendulum upright. When details of the system are
not specified a priori, this solution needs to be learned by
the agent. Finding such an indirect control policy by traditional reinforcement learning is nontrivial [3], since the increasing oscillations require a long time for the balancing to
take place, and the acquisition of informative rewards indicating success is significantly delayed. As we will show, it is
precisely in such situations that intrinsic motivation based on
empowerment is especially useful, since it is determined from
only comparatively local properties of the dynamics along the
present trajectory and its potential future variations.
Inverted pendulum We start with a relatively simple task
of swinging up and stabilizing an inverted pendulum without
an external reward. With an angle of θ (in radians) from the
upright vertical, the equations of motion of the pendulum are

dθ(t)
d ˙θ(t)

=

˙θ(t)dt
g
l
sin(θ(t)) dt +
da(t)
ml2 +
dW(t)
ml2
!
, (12)
where ˙θ is the angular velocity of the pendulum, m is its mass,
l is the length, a is the torque applied by the agent, g is the
free fall acceleration, and dW(t) is a Wiener process.
We apply a (stochastically chosen) control signal a(t) for
the duration Te and observe the final state ˜θ = θ + ˜ηobs,
where ˜ηobs is the standard Gaussian observation noise at the
final state. Empowerment is then given by the maximally
achievable mutual information between a(t) and ˜θ at a given
power level for a(t), i.e., the channel capacity between the
two.
The observation noise effectively determines the resolution,
at which the end state is considered. Note that in our linear
approximation the process noise dW(t) undergoes the same
gain sequence as the control signal, and thus it rescales the
empowerment landscape and changes the behavior of the system. Thus to compare empowerment values in different states,
it is essential to include the observation noise.
We now apply our empowerment-based control protocol,
(4), to the inverted pendulum. We calculate the empowerment
landscape by using the time-discretized version of Eqs. (1, 12).
For this, we map the deterministic part of the dynamics (f, g
in (1)) onto discrete time as per (5). We then compute the
4
Figure 2: Intrinsic motivation based control in the power-constrained regime. Top row: generalized empowerment landscapes
in the linear approximation for empowerment (left), controlled Lyapunov exponent (middle), and kicked CEF (right) versions
of the problem, plotted against θ (horizontal axis) and ˙θ (vertical axis), measured in rad and rad/s, respectively. Black dots
in each panel are the final state, and white lines are the trajectories of the pendulum, starting at the bottom denoted by the
red dots. Bottom row: the control signals chosen from the generalized empowerment maximization as a function of time.
Here the time horizon is te = 0.5s.
channel capacity by applying (10) using the singular values
from (8), where states are given by (θ, ˙θ) ∈ R
dx , and actions
consist of applying a torque a. The landscapes for the original empowerment, the controlled Lyapunov exponent, and
the kicked CEF versions of the problem, all with the time
horizons of te = 0.5 s and the discretization ∆t = 10−3
are
shown in Fig. 2. Then, from each state, we choose the control action to greedily optimize the generalized empowerment.
The panels in the upper row in this Figure also show trajectories obtained this way. The lower row shows time traces
of the control signal derived from the generalized empowerment maximization. In all cases, initially, the agent drives
the pendulum at the maximum allowable torque, which we
set to be power-constrained to ±1 N m. Around 13, 10, and
10 seconds after the start (for the three versions of the empowerment, respectively), the pendulum accumulates enough
energy to reach the vertical, and the agents reduce the torques
to very small values, a  1 N m, which are now sufficient to
keep the pendulum in the upright position and prevent it from
falling. It is striking that the generalized empowerment landFigure 3: Convergence of the method for ∆t → 0 and
te = 0.5s. As time resolution is refined fourfold at every stage,
one arrives at a well-defined value for the empowerment estimation as ∆t → 0. The numerical stability of this limit
approximation is consistent throughout the landscape.
scapes and their induced trajectories are qualitatively similar
to those that would be generated by an optimal value function, derived by standard optimal control techniques based on
a reward specifically designed to achieve the top position [3].
In our analysis, we chose a particular discretization ∆t =
10−3
s, and we need to show that our results depend only
weekly on this choice. For this, we repeat our analysis at different ∆t. Figure 3 shows the dependence of the maximum
value of the original empowerment (black dot in left panel of
Fig. 2) on ∆t. To the extent that the estimate converges to a
well-defined number linearly as ∆t → 0, the discrete time dynamics provides a consistent approximation to the continuous
time dynamics.
Double Pendulum Now we show that the empowerment
maximization formalism is capable of dealing with more challenging problems, such as a power-constrained control of a
(potentially chaotic) double pendulum [16], Fig. 4, with equations of motion:
d¨θ1(t) = −
1
d1(t)

d2(t)
¨θ2(t) + φ1(t)

, (13)
d¨θ2(t) = 1
m2`
2
c2 + I2 −
d
2
2
(t)
d1(t)

da(t) + dW(t) + d
2
2(t)
d1(t)
φ1(t)
− m2`1`c2
˙θ1(t)
2
sin θ2(t) − φ2(t)

,
with
d1(t) =m1`
2
c1 + m2(`
2
1 + `
2
c2 + 2`1`c2
cos θ2(t)) + I1 + I2,
d2(t) =m2(`
2
c2 + `1`c2
cos θ2(t)) + I2,
φ1(t) = − m2`1`c2
˙θ(t)
2
sin θ2(t)−2m2`1`c2
˙θ2(t)
˙θ1(t) sin θ2(t)
+ (m1`c1 + m2`1)g cos θ1(t) + φ2(t),
φ2(t) =m2`c2 g cos(θ1(t) + θ2(t)).
We add Wiener noise, dW(t), and permit the controller to
apply a scalar control signal |a(t)| ≤ 1, at the joint between
the two links. In the equations of motion, mi, `i, `ci
, and Ii
stand for the mass, the length, the length to center of mass,
and the moment of inertia of the i-th link, i ∈ [1, 2], respec5
Figure 4: Top left: Double pendulum with control torque on the joint between the links with dynamics given by (13) Top
right: Slices through the empowerment landscape of a double pendulum. Each subplot shows a particular slice in the 4D
landscape, when two other coordinates are zero. For example, the plot with axes ˙θ2,
˙θ1 is shown for θ2 = 0 rad and θ1 = 0 rad.
Bottom: Traversing the state space of the double pendulum according to (4). The first and the second 15s are shown with
different scale for the instantaneous empowerment. The initial and the final positions are both links down and both links up,
respectively. Torque is applied to the middle joint only.
tively. Figure 4 shows the landscape for the original empowerment for selected slices of the phase space. This landscape
is more complex than for the single-pendulum. Nonetheless it
retains the property that, following the local gradient in the
state space directly, one ultimately reaches the state of the
maximum empowerment, which is precisely where both links
of the pendulum are balanced upright. The vertical position,
however, is a priori not sufficient to guarantee the balancing since the control only applies torque at the joint linking
the pendulum halves. That is, the controller cannot move
the pendulum in arbitrary directions through the state space.
Surprisingly, this concern notwithstanding, the algorithm still
balances the pendulum, cf. Fig. 4.
Cart-Pole We have additionally verified that the empowerment maximization also balances an inverted pendulum on a
moving cart, cf. Fig.5. Here the control signal (force) is applied to the cart. Thus the pendulum is now affected only
indirectly. The dynamics of this system is:
dx¨(t) =m sin θ(t)(`
˙θ
2
(t) + g cos θ(t)) + da(t) + dW(t)
M + m sin2
θ(t)
, (14)
d¨θ(t) = − da(t) cos θ(t) − m` ˙θ
2
(t) cos θ(t) sin θ(t)
− (M + m)g sin θ(t),
where x(t), θ(t), m, M, `, g, |a(t)| ≤ 1 are the x coordinate of
the center of mass of the cart, the angle of the pole, the pole
mass, the cart mass, the pole length, the free fall acceleration,
and the force applied to the cart.
Discussion
In this study, we focused on a class of intrinsic motivation
models that mimic decision-making abilities of biological organisms in various situations without explicit reward signals.
We used an information-theoretic formulation in which the
controller starts with knowledge of the (stochastic) dynamical equations describing the agent and the environment, and
then selects actions that “empower” the agent. That is, the
controller improves its ability to affect the system in the future, as measured by the mutual information between the action sequence and the subsequent responses. This leads the
system to the most sensitive points in the state space, which
we showed solves a problem known to be difficult for simple
reinforcement learning algorithms: balancing inverted pendula. Depending on which subsets of the past actions and
future responses are used to drive the intrinsic motivation,
our approach interpolates between the original formulation
of empowerment maximization, maximization of the “kicked”
version of Causal Entropic Forcing, and maximization of the
“controlled” subset of the Lyapunov exponents of the agentenvironment pair. This provides insight into which properties
of the dynamical system are responsible for the behaviors pro6
Figure 5: Left: Cart-Pole system with control force, ~a(t), applied to the cart only, which moves on the rail (or on the edge
of a table), allowing the pole to rotate in the x-y plane. Its dynamics is given by (14). Right: Traversing the state space of
the pendulum on a cart according to empowerment maximization. The initial and the final state of the pole are down and
up, respectively. The horizontal axis is time in seconds t ∈ [0, 20]s.
duced by these different motivation functions.
One big challenge in using information-theoretic quantities
is computing them, which can be difficult to do either analytically or from data. Our paper makes a significant contribution to solving this problem in the context of empowerment by
providing an explicit algorithm for computing various versions
of empowerment, for arbitrary lengths of pasts and futures,
using the small noise/small control approximation to the dynamics, while still treating the dynamics as nonlinear. This
is often the most interesting regime, modeling weak, powerconstrained controllers. Crucially, our algorithm is local, so
that climbing up the empowerment gradient only requires estimation of the dynamics in the vicinity of the current state
of the system. This should be possible in real control applications by using the data directly, possibly with the help of
deep neural networks to approximate the relevant dynamical
landscapes [45–47]. Therefore, knowing the exact form of the
dynamical system, which could be a potential limitation of
our approach, is not strictly required. This opens up opportunities for scaling our method to more complex scenarios.
Our work suggests that, in addition to the Lyapunov spectrum, defined via the trajectory divergence in time due to a
small arbitrary perturbation, one may want to consider the
optimal Lyapunov spectrum, where the initial perturbation is
optimally aligned with the controllable directions in the dynamics. We defer a systematic study of optimal Lyapunov
spectra to future work.
A potential extension of our analysis relates to social interactions. Interacting agents have their own intrinsic motivations and affect each other’s ability to achieve their goals.
Understanding how multiple agents interact, each trying to
empower itself in the presence of others, and whether and
when this leads to cooperation or conflict is a promising area
for future research. Crucially, the ability to affect someone
else’s empowerment may provide insight into what distinguishes social interactions from purely physical interactions
among nearby individuals.
Acknowledgements ST was supported in part by California State University, and the College of Engineering at SJSU.
IN was supported in part by the Simons Foundation Investigator award, the Simons-Emory Consortium on Motor Control, and NIH grant 2R01NS084844. DP acknowledges partial
support by the EC H2020-641321 socSMCs FET Proactive
project and the Pazy Foundation.
References
[1] P.-Y. Oudeyer and F. Kaplan. What is intrinsic motivation? a typology of computational approaches. Frontiers
in neurorobotics, 1:6, 2009.
[2] R. S. Sutton and A. G. Barto. Reinforcement learning:
An introduction. MIT press, 2018.
[3] K. Doya. Reinforcement learning in continuous time and
space. Neural computation, 12(1):219–245, 2000.
[4] S. Mohamed and D. J. Rezende. Variational information maximisation for intrinsically motivated reinforcement learning. In Advances in neural information processing systems, pages 2125–2133, 2015.
[5] K. Gregor, D J. Rezende, and D. Wierstra. Variational
intrinsic control. arXiv preprint arXiv:1611.07507, 2016.
[6] K. Baumli, D. Warde-Farley, S. Hansen, and V. Mnih.
Relative variational intrinsic control. Proceedings of the
AAAI Conference on Artificial Intelligence, 35(8):6732–
6740, 2021.
[7] T. Kwon. Variational intrinsic control revisited. International Conference on Learning Representations, (ICLR),
2021, 2021.
[8] A. Sharma, S. Gu, S Levine, V. Kumar, and K. Hausman.
Dynamics-aware unsupervised discovery of skills. arXiv
preprint arXiv:1907.01657, 2019.
[9] A. Sharma, M Ahn, S. Levine, V. Kumar, K. Hausman, and S. Gu. Emergent real-world robotic skills
via unsupervised off-policy reinforcement learning. RSS,
Robotics: Science and Systems 2020, 2020.
[10] B. Eysenbach, A Gupta, J. Ibarz, and S. Levine. Diversity is all you need: Learning skills without a reward
function. International Conference on Learning Representations, (ICLR), 2019, 2018.
[11] R. Houthooft, X. Chen, Y. Duan, J. Schulman,
F. De Turck, and P. Abbeel. Vime: Variational information maximizing exploration. Advances in neural information processing systems, 29, 2016.
[12] J. Achiam, H. Edwards, D. Amodei, and P. Abbeel.
Variational option discovery algorithms. arXiv preprint
arXiv:1807.10299, 2018.
[13] J. Choi, A. Sharma, H. Lee, S. Levine, and S. S. Gu. Variational empowerment as representation learning for goalconditioned reinforcement learning. International Conference on Machine Learning, pages 1953–1963, 2021.
[14] C. Salge, C. Glackin, and D. Polani. Empowerment–
an introduction. In Guided Self-Organization: Inception,
pages 67–114. Springer, 2014.
[15] A. S. Klyubin, D Polani, and C. L. Nehaniv. Empowerment: A universal agent-centric measure of control. IEEE Congress on Evolutionary Computation,
2005, 1:128–135, 2005.
[16] T. Jung, D. Polani, and P. Stone. Empowerment for continuous agent—environment systems. Adaptive Behavior,
19(1):16–39, 2011.
7
[17] A. S. Klyubin, D. Polani, and C. L. Nehaniv. Keep
your options open: An information-based driving principle for sensorimotor systems. PLoS ONE, 3(12):e4018,
Dec 2008.
[18] R. Zhao, P. Abbeel, and S. Tiomkin. Efficient empowerment estimation for unsupervised stabilization. International Conference Learning Representations, 2020.
[19] R. Zhao, S. Tiomkin, and P. Abbeel. Dynamical system
embedding for efficient intrinsically motivated artificial
agents. Advances in Neural Information Processing Systems, (NeurIPS), DeepRL, 2019.
[20] A. D. Wissner-Gross and C. E. Freer. Causal entropic
forces. Physical review letters, 110(16):168702, 2013.
[21] T. M. Cover and J. A. Thomas. Elements of information
theory. John Wiley & Sons, 2012.
[22] A. S. Klyubin, D. Polani, and C. L. Nehaniv. Empowerment: A universal agent-centric measure of control. In
2005 IEEE Congress on Evolutionary Computation, volume 1, pages 128–135. IEEE, 2005.
[23] B. Eysenbach, A. Gupta, J. Ibarz, and S. Levine. Diversity is all you need: Learning skills without a reward
function. In ICLR, 2019.
[24] Y. Du, S. Tiomkin, E. Kiciman, D. Polani, P. Abbeel,
and A. D. Dragan. Ave: Assistance via empowerment.
Neural Information Processing Systems, NeurIPS 2020,
2020.
[25] C. Salge and D. Polani. Empowerment as replacement
for the three laws of robotics. Frontiers in Robotics and
AI, 4:25, 2017.
[26] W. Bialek, I. Nemenman, and N. Tishby. Predictability, complexity, and learning. Neural computation,
13(11):2409–2463, 2001.
[27] C. Holmes and I. Nemenman. Estimation of mutual information for real-valued data with error bars and controlled
bias. Phys Rev E, 100:022404, 2019.
[28] T. Jung, D. Polani, and P. Stone. Empowerment for continuous agent—environment systems. Adaptive Behavior,
19(1):16–39, 2011.
[29] R. Zhao, K. Lu, P. Abbeel, and S. Tiomkin. Efficient empowerment estimation for unsupervised stabilization. In
International Conference on Learning Representations,
ICLR, 2021.
[30] A. D. Wissner-Gross and C. E. Freer. Causal entropic
forces. Physical review letters, 110(16):168702, 2013.
[31] A. Y. Ng, D. Harada, and S. Russell. Policy invariance
under reward transformations: Theory and application
to reward shaping.
[32] P. Dayan and G. E. Hinton. Feudal reinforcement learning. In Advances in neural information processing systems, pages 271–278, 1993.
[33] T. D. Kulkarni, K. Narasimhan, A. Saeedi, and J. Tenenbaum. Hierarchical deep reinforcement learning: Integrating temporal abstraction and intrinsic motivation.
In Advances in Neural Information Processing Systems,
NeurIPS, 2016.
[34] Y. Burda, H. Edwards, D. Pathak, A. Storkey, T. Darrell,
and A. A. Efros. Large-scale study of curiosity-driven
learning. In ICLR, 2019.
[35] D. Pathak, P. Agrawal, A. A. Efros, and T. Darrell.
Curiosity-driven exploration by self-supervised prediction. In ICML, 2017.
[36] M. Karl, M. Soelch, P. Becker-Ehmck, D. Benbouzid,
P. van der Smagt, and J. Bayer. Unsupervised real-time
control through variational empowerment. 2018.
[37] C. Salge, C. Glackin, and D. Polani. Approximation of
empowerment in the continuous domain. Advances in
Complex Systems, 16(02n03):1250079, 2013.
[38] C. Salge, C. Glackin, and D. Polani. Empowerment–
an introduction. In Guided Self-Organization: Inception,
pages 67–114. Springer, 2014.
[39] T. Anthony, D. Polani, and C. L. Nehaniv. General
self-motivation and strategy identification: Case studies
based on sokoban and pac-man. IEEE Transactions on
Computational Intelligence and AI in Games, 6(1):1–17,
2014.
[40] H. J. Charlesworth and M. S. Turner. Intrinsically motivated collective motion. Proceedings of the National
Academy of Sciences, 116(31):15362–15367, 2019.
[41] J. Schmidhuber. Formal theory of creativity, fun, and
intrinsic motivation (1990–2010). IEEE Transactions on
Autonomous Mental Development, 2(3):230–247, 2010.
[42] H. D. Abarbanel, R. Brown, and M. B. Kennel. Local lyapunov exponents computed from observed data. Journal
of Nonlinear Science, 2(3):343–365, 1992.
[43] L. C. Evans. An introduction to mathematical optimal
control theory version 0.2. 1983.
[44] A. D. Kuo. The six determinants of gait and the inverted
pendulum analogy: A dynamic walking perspective. Human movement science, 26(4):617–656, 2007.
[45] B. C. Daniels and I. Nemenman. Automated adaptive
inference of phenomenological dynamical models. Nature
communications, 6(1):1–8, 2015.
[46] S. L. Brunton, J. L. Proctor, and J. N. Kutz. Discovering
governing equations from data by sparse identification of
nonlinear dynamical systems. Proceedings of the national
academy of sciences, 113(15):3932–3937, 2016.
[47] B. Chen, K. Huang, S. Raghupathi, I. Chandratreya,
Q. Du, and H. Lipson. Automated discovery of fundamental variables hidden in experimental data. Nature
Computational Science, 2(7):433–442, 2022.
8


**ASSISTANT**

## Intrinsic Motivation in Dynamical Control Systems

### Abstract
Biological systems often choose actions without explicit reward signals, a phenomenon known as intrinsic motivation. The computational principles underlying this behavior remain poorly understood. In this study, we investigate an information-theoretic approach to intrinsic motivation, based on maximizing an agent’s empowerment—the mutual information between its past actions and future states. We show that this approach generalizes previous attempts to formalize intrinsic motivation, and we provide a computationally efficient algorithm for computing the necessary quantities. We test our approach on several benchmark control problems, and we explain its success in guiding intrinsically motivated behaviors by relating our information-theoretic control function to fundamental properties of the dynamical system representing the combined agent-environment system. This opens the door for designing practical artificial, intrinsically motivated controllers and for linking animal behaviors to their dynamical properties.

### Keywords
- Information capacity
- Sensitivity gain
- Stabilization
- Predictive information

### Introduction
Living organisms can generate behaviors that solve novel challenges without prior experience. This ability might be explained by a single, generic mechanism. One proposal is that novel, useful behaviors can be generated through intrinsic motivation, which is defined informally as a set of computational algorithms derived directly from the intrinsic properties of the organism-environment dynamics and not specifically learned.

In the theory and practice of artificial agents, robots, and machine learning more generally, there is a move away from reinforcement learning (RL) and its extrinsically specified reward structure. A specific class of intrinsic motivation algorithms for artificial systems is known as empowerment maximization, which proposes that agents should maximize the mutual information between their potential actions and subsequent future states of the world. This corresponds to maximizing the diversity of future world states achievable as a result of the chosen actions, potentiating a broader set of behavior options in the future.

### Empowerment and Its Computation

#### Empowerment Definition
Empowerment defines a pseudo-utility function on the state space, based on the system dynamics only, without resorting to a reward. Formally, the dynamics of the system are expressed by the conditional probability distribution $p(x_{T_e} | a_0^{T_e-1}, x_0)$ of the resulting state when starting in a state $x_0$ and subsequently carrying out an action sequence $a_0^{T_e-1}$. The empowerment $C(x_0)$ is then defined as:
$$
C(x_0) := \max_{p(a_0^{T_e-1} | x_0)} I(X_{T_e}; A_0^{T_e-1} | x_0),
$$
where $I$ is the mutual information, and $H$ is the entropy function.

#### Linear Response Approximation
To relate empowerment to traditional quantities used to describe dynamical systems, we assume the control signal $a$ is small. Under this assumption, the dynamics are approximated by a linear time-variant system around the trajectories of the autonomous dynamics:
$$
\frac{\partial \bar{x}_s}{\partial a_r} = \prod_{\tau=r+2}^{s} \nabla_{\bar{x}} f(\bar{x}_{\tau-1}) g(\bar{x}_r).
$$
We define the linear response of the sequence of the system’s states $x_{s_1}^{s_2}$ to the sequence of the agent’s actions $a_{r_1}^{r_2}$ as:
$$
\mathbb{F}_{s_1,s_2}^{r_1,r_2}(x_0) = \begin{pmatrix}
\frac{\partial \bar{x}_{s_2}}{\partial a_{r_2}} & \frac{\partial \bar{x}_{s_2}}{\partial a_{r_2-1}} & \cdots & \frac{\partial \bar{x}_{s_2}}{\partial a_{r_1}} \\
\frac{\partial \bar{x}_{s_2-1}}{\partial a_{r_2}} & \frac{\partial \bar{x}_{s_2-1}}{\partial a_{r_2-1}} & \cdots & \frac{\partial \bar{x}_{s_2-1}}{\partial a_{r_1}} \\
\vdots & \vdots & \ddots & \vdots \\
\frac{\partial \bar{x}_{s_1}}{\partial a_{r_2}} & \frac{\partial \bar{x}_{s_1}}{\partial a_{r_2-1}} & \cdots & \frac{\partial \bar{x}_{s_1}}{\partial a_{r_1}}
\end{pmatrix}.
$$

#### Generalized Empowerment
For more general scenarios, we define the generalized empowerment as:
$$
C_{T_e, T_a, \Delta T}(x_0) := \max_{p(\mathbf{a} | x_0)} I(X_{T_e}^{T_a + \Delta T}; A_0^{T_a - 1} | x_0),
$$
where $T_a$ is the number of time steps at which actions are performed, $\Delta T$ is the time gap between the action sequence and the beginning of the observation of the resulting states, and $T_e$ is the last step in that observed sequence.

### Practical Algorithm for Empowerment Calculation
To compute empowerment efficiently for arbitrary dynamical systems, we start by discretizing the time and calculating the linear response matrix $\mathbb{F}$. We then calculate the singular values of $\mathbb{F}$ and apply the "water filling" procedure to find the set of channel powers $\sigma_i$ to match the available total power $P$:
$$
C_{T_e, T_a, \Delta T}(x_0) = \max_{\sigma_i \geq 0, \sum_i \sigma_i = P} \frac{1}{2} \sum_{i=1}^{d_x} \ln(1 + \rho_i(x_0) \sigma_i),
$$
where $\rho_i(x_0)$ are the singular values of the appropriate submatrix of $\mathbb{F}$.

### Benchmark Problems
#### Inverted Pendulum
For the inverted pendulum, we have the equations of motion:
$$
\begin{pmatrix}
\dot{\theta}(t) \\
\ddot{\theta}(t)
\end{pmatrix} = \begin{pmatrix}
\dot{\theta}(t) \\
\frac{g}{l} \sin(\theta(t)) + \frac{a(t)}{ml^2}
\end{pmatrix} + \mathbf{d\eta},
$$
where $\theta$ is the angle, $m$ is the mass, $l` is the length, $a$ is the torque, and $\mathbf{d\eta}$ is the noise.

#### Double Pendulum
The double pendulum dynamics are given by:
$$
\ddot{\theta}_1(t) = -\frac{1}{d_1(t)} \left[ d_2(t) \ddot{\theta}_2(t) + \phi_1(t) \right], \quad \ddot{\theta}_2(t) = \frac{1}{m_2 l_c^2 + I_2 - \frac{d_2(t)^2}{d_1(t)}} \left[ a(t) + \phi_2(t) \right],
$$
where $\theta_i$, $m_i$, $l_i$, $l_{ci}$, and $I_i$ are the angles, masses, lengths, lengths to center of mass, and moments of inertia of the links, respectively.

### Conclusion
In this study, we explored an information-theoretic approach to intrinsic motivation through the concept of empowerment. We provided a computationally efficient algorithm to estimate empowerment and tested it on various benchmark control problems. Our results show that empowerment maximization can guide intrinsically motivated behaviors, linking them to well-understood properties of dynamical systems. This approach could lead to the design of practical artificial controllers and provide insights into animal behavior.

---

This summary has linked key concepts such as [[intrinsic motivation]], [[reinforcement learning]], [[empowerment maximization]], and [[dynamical systems]] to their respective detailed explanations. If you require further elaboration on any of these concepts or additional specific examples, please let me know!


---


following: [[intrinsic motivation]]

See also: [[intrinsic motivation]]


See also: [[mutual information]]


See also: [[indirect control]]


See also: [[control theory]]


See also: [[channel capacity]]


See also: [[system dynamics]]


See also: [[lyapunov exponents]]


See also: [[representation learning]]