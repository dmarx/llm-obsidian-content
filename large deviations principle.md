---
date: 1721687933.200549
tags:
- autogenerated
- source__oai
- conversation_id__a2d4a2dc-4888-4b9b-aa34-ef6394b04fb7
- msg_aaa252f4-d10a-4e8f-8a5f-759710becf37
- msg_1ba1fb53-bdc2-4946-be9a-d514347ae34d
- colorclass/information-theory
title: large deviations principle
---

**USER**

large deviations principle


**ASSISTANT**

## Large Deviations Principle

### Overview

The Large Deviations Principle (LDP) is a central concept in probability theory that describes the asymptotic behavior of the probabilities of rare events. It provides a framework for understanding how the probability of certain rare events decays exponentially with the system size or another large parameter. This principle is particularly useful in statistical physics, information theory, finance, and various fields that deal with complex systems and rare events.

### Formal Definition

Consider a sequence of random variables \(\{X_n\}_{n \geq 1}\) taking values in a topological space \(\mathcal{X}\). The sequence \(\{X_n\}\) is said to satisfy the Large Deviations Principle with rate function \(I(x)\) if for any measurable set \(A \subset \mathcal{X}\),

1. **Lower Bound**:
    $$
    \liminf_{n \to \infty} \frac{1}{n} \log \Pr(X_n \in A) \geq - \inf_{x \in A^\circ} I(x),
    $$
    where \(A^\circ\) is the interior of \(A\).

2. **Upper Bound**:
    $$
    \limsup_{n \to \infty} \frac{1}{n} \log \Pr(X_n \in A) \leq - \inf_{x \in \overline{A}} I(x),
    $$
    where \(\overline{A}\) is the closure of \(A\).

The function \(I: \mathcal{X} \to [0, \infty]\) is called the **rate function**. It is lower semi-continuous, meaning that for every \(\alpha \in \mathbb{R}\), the set \(\{x : I(x) \leq \alpha\}\) is closed.

### Intuition

The Large Deviations Principle quantifies how the probabilities of deviations from typical behavior decay exponentially. For large \(n\), the probability that \(X_n\) deviates significantly from its typical value is approximately:

$$
\Pr(X_n \approx x) \sim \exp(-n I(x)),
$$

where \(I(x)\) is the rate function that determines the exponential rate of decay of the probabilities.

### Rate Function

The rate function \(I(x)\) plays a crucial role in the LDP. It captures the "cost" of deviations from the typical behavior. The rate function is often derived from the cumulant generating function or through variational principles.

### Key Results

#### Cramér's Theorem

Cramér's theorem provides the LDP for the sample mean of i.i.d. random variables. Let \(\{X_i\}\) be i.i.d. random variables with mean \(\mu\) and cumulant generating function \(\Lambda(\lambda) = \log \mathbb{E}[\exp(\lambda X_i)]\). The sample mean \(\overline{X}_n = \frac{1}{n} \sum_{i=1}^n X_i\) satisfies an LDP with rate function:

$$
I(x) = \sup_{\lambda} (\lambda x - \Lambda(\lambda)).
```

#### Gartner-Ellis Theorem

The Gartner-Ellis theorem extends Cramér's theorem to sequences of non-i.i.d. random variables. It provides conditions under which the sequence \(\{X_n\}\) satisfies an LDP, based on the limit of the scaled cumulant generating function:

$$
\Lambda_n(\lambda) = \frac{1}{n} \log \mathbb{E}[\exp(n \lambda X_n)].
$$

If the limit \(\Lambda(\lambda) = \lim_{n \to \infty} \Lambda_n(\lambda)\) exists, then \(\{X_n\}\) satisfies an LDP with rate function:

$$
I(x) = \sup_{\lambda} (\lambda x - \Lambda(\lambda)).
```

#### Sanov's Theorem

Sanov's theorem describes the LDP for empirical measures. For a sequence of i.i.d. random variables \(\{X_i\}\) with common distribution \(P\), the empirical measure \(\hat{P}_n\) satisfies an LDP with rate function given by the Kullback-Leibler divergence \(D(Q||P)\):

$$
I(Q) = D(Q||P) = \sum_{x \in \mathcal{X}} Q(x) \log \frac{Q(x)}{P(x)},
```

where \(Q\) is a probability measure.

### Applications

#### Statistical Physics

In statistical mechanics, the LDP is used to study thermodynamic fluctuations, phase transitions, and the behavior of macroscopic quantities in large systems. For example, the probability of large deviations in energy, magnetization, or particle number can be analyzed using the LDP.

#### Information Theory

The LDP is applied in information theory to understand the asymptotic behavior of error probabilities in coding and communication systems. It helps in deriving bounds on the probabilities of decoding errors and the performance of information-theoretic protocols.

#### Finance

In finance, the LDP is used to analyze the risk of extreme events, such as large losses or gains in financial portfolios. It provides a theoretical foundation for assessing the probabilities of rare but impactful financial outcomes.

### Examples

#### Example 1: Sum of Random Variables

Consider a sequence of i.i.d. random variables \(\{X_i\}\) with mean \(\mu\) and variance \(\sigma^2\). The sample mean \(\overline{X}_n\) satisfies an LDP with rate function:

$$
I(x) = \frac{(x - \mu)^2}{2\sigma^2}.
$$

This indicates that the probability of observing a sample mean far from \(\mu\) decays exponentially with a rate determined by the variance \(\sigma^2\).

#### Example 2: Empirical Measure

Let \(\{X_i\}\) be a sequence of i.i.d. random variables with probability distribution \(P\). The empirical measure \(\hat{P}_n\) satisfies an LDP with rate function given by the Kullback-Leibler divergence \(D(Q||P)\). The probability of observing an empirical measure \(Q\) that deviates from \(P\) is given by:

$$
\Pr(\hat{P}_n \approx Q) \sim \exp(-n D(Q||P)).
```

### Further Reading

- [[Large Deviations Theory]]
- [[Sanov's Theorem]]
- [[Gartner-Ellis Theorem]]
- [[Cramér's Theorem]]
- [[Varadhan's Lemma]]
- [[Relative Entropy]]
- [[Spin Glass Models]]
- [[Sherrington-Kirkpatrick Model]]
- [[Replica Symmetry Breaking]]

The Large Deviations Principle is a powerful framework for understanding the probabilities of rare events and extreme fluctuations in complex systems. For a comprehensive exploration, refer to specialized texts in probability theory, statistical mechanics, and the theory of disordered systems.


---

preceding: [[varadhan's lemma]]  


following: [[legendre-fenchel transform]]

See also: [[probability measure]]


See also: [[statistical physics]]


See also: [[probability theory]]


See also: [[information theory]]


See also: [[large deviations theory]]


See also: [[generating function]]


See also: [[random variables]]