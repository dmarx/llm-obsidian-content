---
date: 1721688258.625696
tags:
- autogenerated
- source__oai
- conversation_id__a2d4a2dc-4888-4b9b-aa34-ef6394b04fb7
- msg_aaa245f5-b818-4e18-bebc-e5caeb2ca90d
- msg_6b59a78e-adc4-4235-ac40-412e1dba4ed3
- colorclass/information-theory
title: jensen-shannon divergence
---

**USER**

Jensen-Shannon Divergence


**ASSISTANT**

## Jensen-Shannon Divergence

### Overview

The Jensen-Shannon Divergence (JSD) is a method for measuring the similarity between two probability distributions. It is a symmetrized and smoothed version of the Kullback-Leibler (KL) divergence, making it a more stable and interpretable measure. JSD is widely used in various fields, including machine learning, information theory, and bioinformatics, due to its advantageous properties over the KL divergence.

### Definition

For two probability distributions \(P\) and \(Q\) defined on the same probability space \(\mathcal{X}\), the Jensen-Shannon divergence is defined as:

$$
JSD(P \| Q) = \frac{1}{2} D(P \| M) + \frac{1}{2} D(Q \| M),
```

where \(M = \frac{1}{2} (P + Q)\) is the average of the two distributions, and \(D(\cdot \| \cdot)\) denotes the KL divergence. Specifically,

$$
D(P \| Q) = \sum_{x \in \mathcal{X}} P(x) \log \frac{P(x)}{Q(x)}.
```

Thus,

$$
JSD(P \| Q) = \frac{1}{2} \sum_{x \in \mathcal{X}} P(x) \log \frac{P(x)}{M(x)} + \frac{1}{2} \sum_{x \in \mathcal{X}} Q(x) \log \frac{Q(x)}{M(x)}.
```

### Properties

1. **Symmetry**: Unlike the KL divergence, the Jensen-Shannon divergence is symmetric:
    $$
    JSD(P \| Q) = JSD(Q \| P).
    ```

2. **Non-negativity**: \(JSD(P \| Q) \geq 0\), with equality if and only if \(P = Q\).

3. **Boundedness**: The Jensen-Shannon divergence is bounded between 0 and 1 (when using log base 2):
    $$
    0 \leq JSD(P \| Q) \leq 1.
    ```

4. **Interpretability**: JSD provides a more intuitive measure of similarity between distributions because it is symmetrized and bounded.

### Interpretation

The Jensen-Shannon divergence can be interpreted as a measure of the average relative entropy between each distribution and the average distribution \(M\). It effectively measures the divergence of the distributions from their mean. Because it is symmetric and bounded, it is often preferred over the KL divergence in applications where these properties are beneficial.

### Applications

#### Machine Learning

In machine learning, the Jensen-Shannon divergence is used to compare probability distributions, such as in clustering algorithms, generative models, and information retrieval.

#### Bioinformatics

In bioinformatics, JSD is used to compare gene expression profiles, sequence alignment scores, and other biological data distributions.

#### Natural Language Processing

In natural language processing, JSD is used to compare text documents, measure topic similarity, and evaluate language models.

### Examples

#### Example 1: Discrete Distributions

Consider two discrete probability distributions \(P\) and \(Q\) over the same space \(\mathcal{X} = \{x_1, x_2, \ldots, x_n\}\):

$$
P = \left( \frac{1}{2}, \frac{1}{2}, 0 \right), \quad Q = \left( \frac{1}{4}, \frac{1}{4}, \frac{1}{2} \right).
```

The average distribution \(M\) is:

$$
M = \frac{1}{2}(P + Q) = \left( \frac{3}{8}, \frac{3}{8}, \frac{1}{4} \right).
```

The KL divergences are:

$$
D(P \| M) = \frac{1}{2} \log \frac{\frac{1}{2}}{\frac{3}{8}} + \frac{1}{2} \log \frac{\frac{1}{2}}{\frac{3}{8}} = \log \frac{4}{3},
$$

$$
D(Q \| M) = \frac{1}{4} \log \frac{\frac{1}{4}}{\frac{3}{8}} + \frac{1}{4} \log \frac{\frac{1}{4}}{\frac{3}{8}} + \frac{1}{2} \log \frac{\frac{1}{2}}{\frac{1}{4}} = \frac{1}{2} \log \frac{8}{9} + \frac{1}{2} \log 2 = \log \frac{4}{3}.
```

So, the Jensen-Shannon divergence is:

$$
JSD(P \| Q) = \frac{1}{2} \left( \log \frac{4}{3} \right) + \frac{1}{2} \left( \log \frac{4}{3} \right) = \log \frac{4}{3}.
```

#### Example 2: Gaussian Distributions

Consider two normal distributions \(P = N(\mu_1, \sigma^2)\) and \(Q = N(\mu_2, \sigma^2)\). The Jensen-Shannon divergence can be computed as:

$$
JSD(P \| Q) = \frac{1}{2} D\left(N(\mu_1, \sigma^2) \| N\left(\frac{\mu_1 + \mu_2}{2}, \sigma^2\right)\right) + \frac{1}{2} D\left(N(\mu_2, \sigma^2) \| N\left(\frac{\mu_1 + \mu_2}{2}, \sigma^2\right)\right).
```

Using the KL divergence for normal distributions, the calculations can be performed to find the JSD.

### Further Reading

- [[Relative Entropy]]
- [[Kullback-Leibler Divergence]]
- [[Large Deviations Theory]]
- [[Sanov's Theorem]]
- [[Gartner-Ellis Theorem]]
- [[Cram√©r's Theorem]]
- [[Legendre-Fenchel Transform]]
- [[Information Theory]]

The Jensen-Shannon divergence provides a robust and interpretable measure of similarity between probability distributions, making it useful in a wide range of applications. For a comprehensive exploration, refer to specialized texts in information theory, machine learning, and statistics.


---

preceding: [[relative entropy]]---
date: 1714769556.125139
tags:
- autogenerated
- source__oai
- conversation_id__212e3628-2bdc-43ef-a6d3-6e47e5e4dfec
- msg_aaa2b6d3-97b8-4380-bb33-65b4eeb132b4
- msg_61fb9511-f0a7-4eb8-8bbc-ade2ea1e61ee
title: jensen-shannon divergence
---


**USER**

Jensen-Shannon Divergence


**ASSISTANT**

The Jensen-Shannon Divergence (JSD) is a popular method of measuring the similarity between two probability distributions. It is based on the Kullback-Leibler divergence (KLD), but unlike KLD, JSD is symmetric and always finite, making it a more practical and robust measure for many applications in statistics, information theory, and data science.

### Definition

Jensen-Shannon Divergence is defined between two probability distributions \( P \) and \( Q \) over the same probability space. It is the average of the Kullback-Leibler divergences from each of these distributions to their average. Mathematically, it is expressed as:
$$
JSD(P \| Q) = \frac{1}{2} D_{KL}(P \| M) + \frac{1}{2} D_{KL}(Q \| M)
$$
where \( M \) is the average of \( P \) and \( Q \), given by \( M = \frac{1}{2}(P + Q) \). Here, \( D_{KL} \) denotes the Kullback-Leibler divergence.

### Properties

1. **Symmetry**: Unlike KLD, JSD is symmetric, meaning \( JSD(P \| Q) = JSD(Q \| P) \). This property makes it useful in applications where the direction of divergence is not important, such as measuring the distance or similarity between two datasets.

2. **Boundedness**: JSD is always finite and bounded between 0 and \( \log 2 \), where 0 indicates that \( P \) and \( Q \) are identical, and \( \log 2 \) (when the logarithm is base 2) suggests the maximum divergence.

3. **Root**: The square root of JSD, \( \sqrt{JSD(P \| Q)} \), is a true metric, satisfying the triangle inequality, which makes it suitable for metric-based clustering algorithms and other geometric data analysis techniques.

### Applications

- **Machine Learning**: JSD is widely used in machine learning, particularly in scenarios involving clustering and classification where it is important to measure how different or similar the distributions are.

- **Statistical Analysis**: In statistics, JSD provides a way to quantify the difference between observed data distributions and theoretical models.

- **Quantum Information Theory**: JSD is used to measure the closeness of quantum states, which are represented as probability distributions over possible outcomes of measurements.

- **Genomics and Bioinformatics**: JSD can measure the difference in genetic expression profiles from different conditions or populations, aiding in understanding biological diversity and disease mechanisms.

- **Natural Language Processing**: It is utilized in text mining to compare documents or texts by treating the presence of words or phrases as probability distributions.

### Example Calculation

To provide a more concrete understanding, consider two simple discrete probability distributions:
- \( P = \{0.5, 0.5\} \)
- \( Q = \{0.9, 0.1\} \)

The midpoint \( M \) would be \( \{0.7, 0.3\} \). The JSD computation would involve calculating \( D_{KL}(P \| M) \) and \( D_{KL}(Q \| M) \) and then taking their average. This measure would reflect the difference in the spread of probability mass across the two distributions.

### Summary

Jensen-Shannon Divergence offers a symmetric, bounded, and metric-compatible way of measuring the similarity between probability distributions, making it a versatile and robust tool for analyzing data where the assessment of distributional variability is crucial. Its properties and applications make it a favored choice in various fields of research and analysis.


---

preceding: [[disequilibrium measures]]  


following: [[geometric interpretation of jensen-shannon divergence]]

See also: [[information theory]]


See also: [[generative models]]


See also: [[geometric interpretation]]


See also: [[geometric interpretation of jensen-shannon divergence]]