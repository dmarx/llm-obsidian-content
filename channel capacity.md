---
date: 1719333815.25543
tags:
- autogenerated
- source__oai
- conversation_id__b7c336c1-f2dc-4e0a-b2e0-bbe32516848b
- msg_aaa20743-4dd5-44cd-a4c0-5c9defa2431b
- msg_fbb08fd0-4567-4abe-9c7e-a4d4f32cd630
- colorclass/information-theory
title: channel capacity
---

**USER**

channel capacity


**ASSISTANT**

### Channel Capacity

Channel capacity is a fundamental concept in information theory, representing the maximum rate at which information can be reliably transmitted over a communication channel. It quantifies the highest achievable data rate for which the probability of error can be made arbitrarily small.

#### Definition

The capacity \(C\) of a communication channel is defined as:
$$ C = \max_{P_X} I(X; Y), $$
where:
- \(I(X; Y)\) is the mutual information between the channel input \(X\) and the channel output \(Y\).
- The maximization is over all possible input distributions \(P_X\).

#### Mutual Information

Mutual information \(I(X; Y)\) measures the amount of information that the output \(Y\) contains about the input \(X\). It is defined as:
$$ I(X; Y) = H(Y) - H(Y|X), $$
where:
- \(H(Y)\) is the entropy of \(Y\).
- \(H(Y|X)\) is the conditional entropy of \(Y\) given \(X\).

Alternatively, mutual information can be expressed as:
$$ I(X; Y) = H(X) + H(Y) - H(X, Y), $$
where \(H(X, Y)\) is the joint entropy of \(X\) and \(Y\).

#### Entropy and Conditional Entropy

- **Entropy \(H(X)\)**: A measure of the uncertainty or randomness of a random variable \(X\).
  $$ H(X) = -\sum_{x \in \mathcal{X}} P(x) \log P(x), $$
  where \(\mathcal{X}\) is the alphabet of \(X\).

- **Conditional Entropy \(H(Y|X)\)**: The average uncertainty remaining about \(Y\) when \(X\) is known.
  $$ H(Y|X) = -\sum_{x \in \mathcal{X}} P(x) \sum_{y \in \mathcal{Y}} P(y|x) \log P(y|x), $$
  where \(\mathcal{Y}\) is the alphabet of \(Y\).

#### Discrete Memoryless Channel (DMC)

A Discrete Memoryless Channel (DMC) is characterized by:
- A finite input alphabet \(\mathcal{X}\) and output alphabet \(\mathcal{Y}\).
- A set of transition probabilities \(P(Y|X)\), representing the probability of observing \(Y=y\) given that \(X=x\) was transmitted.

For a DMC, the channel capacity is:
$$ C = \max_{P_X} \sum_{x \in \mathcal{X}} \sum_{y \in \mathcal{Y}} P(x) P(y|x) \log \frac{P(y|x)}{P(y)}, $$
where \(P(y)\) is the marginal probability of \(Y=y\):
$$ P(y) = \sum_{x \in \mathcal{X}} P(x) P(y|x). $$

#### Examples of Channel Capacity Calculation

1. **Binary Symmetric Channel (BSC)**
   - In a BSC, each bit is transmitted correctly with probability \(1-p\) and flipped with probability \(p\).
   - The capacity of a BSC is:
     $$ C = 1 - H(p), $$
     where \(H(p) = -p \log p - (1-p) \log (1-p)\) is the binary entropy function.

2. **Binary Erasure Channel (BEC)**
   - In a BEC, each bit is either received correctly with probability \(1-\epsilon\) or erased with probability \(\epsilon\).
   - The capacity of a BEC is:
     $$ C = 1 - \epsilon. $$

#### Achieving Channel Capacity

Shannon's Noisy Channel Coding Theorem states that for any rate \(R < C\) and any \(\epsilon > 0\), there exists a coding scheme such that the probability of decoding error can be made arbitrarily small for sufficiently large block lengths. Conversely, for any rate \(R > C\), the probability of error cannot be made arbitrarily small, implying that reliable communication is impossible at rates above the channel capacity.

#### Practical Implications

1. **Design of Coding Schemes**
   - Practical coding schemes like [[Turbo Codes]], [[Low-Density Parity-Check (LDPC) Codes]], and [[Polar Codes]] are designed to approach the channel capacity while maintaining manageable complexity.

2. **Network Design**
   - Understanding channel capacity helps in the design and optimization of communication networks, including wireless systems, fiber-optic communications, and data storage systems.

3. **Information Theory**
   - Channel capacity is a central concept in information theory, underpinning many advanced topics and applications, such as multiple-input multiple-output (MIMO) systems and network coding.

In summary, channel capacity represents the theoretical upper bound on the data rate for reliable communication over a noisy channel. It is a crucial concept in information theory, guiding the design and analysis of practical communication systems and coding schemes.


---

preceding: [[shannon's noisy channel coding theorem]]  


following: [[hamming bound]]

See also: [[suggestibility]]


See also: [[fluctuation theorems]]


See also: [[conservative logic]]


See also: [[availability heuristic]]


See also: [[conditional entropy]]


See also: [[beta-vae an overview]]


See also: [[mutual information]]


See also: [[information capacity of a system]]


See also: [[information theory and life]]


See also: [[information theory]]


See also: [[spectral analysis]]


See also: [[large deviations theory]]


See also: [[intrinsic motivation in dynamical control systems]]


See also: [[provide an intuitive interpretation of a scaling parameter whose value is expressed in the units  flops  (parameter  token).]]


See also: [[channel capacity in the context of empowerment]]


See also: [[information theory in compressive sensing]]


See also: [[theory of free probability]]


See also: [[relationship between channel capacity and gradient flux in sgd]]


See also: [[eigenvalue dynamics]]


See also: [[deception]]


See also: [[entropy in information theory]]


See also: [[how have noether's symmetry principles been applied to information theory]]


See also: [[partially observable markov decision process (pomdp)]]