---
date: 1713850447.550207
tags:
- autogenerated
- source__oai
- conversation_id__9f8c6c88-3adc-4b4b-af9c-e569eada9468
- msg_aaa20517-b221-4bc3-bf2c-b8e7dc2650d0
- msg_f2ea5e01-7ec4-46f6-bf08-e20ed9c68caa
- colorclass/information-theory
title: hilbert-schmidt independence criterion
---

**USER**

Hilbert-Schmidt Independence Criterion


**ASSISTANT**

The Hilbert-Schmidt Independence Criterion (HSIC) is a statistical test used to measure the dependence between two sets of variables. It is a kernel-based method that generalizes classical measures of correlation and independence, like the Pearson correlation coefficient, to capture nonlinear relationships and interactions between variables. Here, I'll provide a detailed explanation of the mathematical framework underlying HSIC, relevant for your applied mathematics and statistics background.

### Mathematical Formulation of HSIC

HSIC leverages the concept of reproducing kernel Hilbert spaces (RKHS) to analyze relationships between data mapped into high-dimensional feature spaces. The key idea is to transform the original data into a space where linear relations can capture the nonlinear interactions in the original space.

#### Reproducing Kernel Hilbert Spaces

First, let's discuss RKHS, which is crucial for understanding HSIC. A function space $\mathcal{H}$ is an RKHS if there exists a kernel function $k(x, x')$ such that:

1. $k(\cdot, x') \in \mathcal{H}$ for all $x'$ in the input space $X$.
2. The kernel has the reproducing property: $f(x) = \langle f, k(\cdot, x) \rangle_{\mathcal{H}}$ for all $f \in \mathcal{H}$ and $x \in X$.

Here, $\langle \cdot, \cdot \rangle_{\mathcal{H}}$ denotes the inner product in $\mathcal{H}$.

#### Definition of HSIC

Given two random variables $X$ and $Y$ with joint distribution $P_{xy}$ and marginal distributions $P_x$ and $P_y$, HSIC is defined using feature maps $\phi(x)$ and $\psi(y)$ corresponding to RKHSs $\mathcal{H}_X$ and $\mathcal{H}_Y$ respectively. The HSIC value is calculated as:

$$
\text{HSIC}(X, Y) = \|C_{xy}\|^2_{\text{HS}}
$$

where $C_{xy} = \mathbb{E}_{xy}[\phi(x) \otimes \psi(y)] - \mathbb{E}_x[\phi(x)] \otimes \mathbb{E}_y[\psi(y)]$ is the cross-covariance operator in the tensor product space $\mathcal{H}_X \otimes \mathcal{H}_Y$. The norm $\|\cdot\|_{\text{HS}}$ is the Hilbert-Schmidt norm, a generalization of the Frobenius norm for operators.

#### Estimation from Data

For a set of $n$ paired observations $(x_i, y_i)$, the empirical estimate of HSIC is given by:

$$
\text{HSIC}_n = \frac{1}{(n-1)^2} \text{tr}(KHLH)
$$

where $K$ and $L$ are the Gram matrices corresponding to $\phi(x_i)$ and $\psi(y_i)$, and $H$ is the centering matrix $H = I_n - \frac{1}{n} 11^T$. This formula involves computing the trace of the product of the kernel matrices after they have been centered.

### Applications and Implications

HSIC is used in machine learning for feature selection, independence testing, and causal discovery, among other applications. Its ability to detect nonlinear relationships makes it particularly valuable in complex data analysis scenarios.

### Summary

In essence, HSIC provides a powerful, non-parametric way to assess independence between variables using kernel methods. The use of RKHS allows it to capture complex, high-dimensional relationships that traditional statistical measures might miss.

For more on related topics, consider exploring [[Kernel Methods in Machine Learning|kernel methods]] or the [[Theory of Reproducing Kernel Hilbert Spaces|theory of RKHS]].


---


following: [[role of hsic in cka]]

See also: [[random variables]]