---
date: 1728188726.525818
tags:
- autogenerated
- source__oai
- conversation_id__67020eec-152c-8008-81e6-501ffacccda2
- msg_aaa25cf5-d7dd-4cca-8a90-a2fced2d4080
- msg_0b45792a-b389-4ffc-8ed7-383df5ee3755
- colorclass/megatron-nvidia's-large-scale-transformer-model-framework
title: '`torch.distributed` - pytorch''s distributed communication package'
---

**USER**

torch.distributed


**ASSISTANT**

### `torch.distributed` - PyTorch's Distributed Communication Package

#### **1. Overview**
`torch.distributed` is PyTorch's package designed for distributed training and parallelism across multiple processes, GPUs, and nodes. It provides a high-level interface for building and running distributed training workloads, allowing users to leverage multiple computing resources to accelerate model training and handle larger datasets. It supports a variety of communication backends, including:
- **NCCL:** Optimized for multi-GPU communication on a single node or across nodes.
- **Gloo:** A CPU-based backend that also supports communication between GPUs using CUDA.
- **MPI:** Uses the Message Passing Interface, suitable for traditional HPC environments.

#### **2. Key Concepts**
Before diving into `torch.distributed`, itâ€™s crucial to understand the core concepts behind distributed training in PyTorch:

- **Distributed Data Parallel (DDP):** A parallel training paradigm where each process holds a copy of the model. Each process works on a different subset of the data (a "shard"), computes the gradients, and synchronizes them across all processes using collective communication operations (typically `all_reduce`). This is the most common method of data parallelism.
- **Process Groups:** A group of processes that communicate with each other. PyTorch allows users to create custom process groups, enabling more granular control over which processes participate in specific communication operations.
- **World Size:** The total number of processes involved in the distributed computation. In a multi-GPU training setup, each GPU typically corresponds to a single process.
- **Rank:** A unique identifier assigned to each process within a process group. This allows processes to coordinate which part of the data they process and to facilitate communication.

#### **3. Basic Usage**
The `torch.distributed` module provides the functionality needed to set up distributed environments and perform communication operations. Here's a basic breakdown of how to initialize and use `torch.distributed`:

1. **Initialization:**
   To set up a distributed environment, `torch.distributed.init_process_group` must be called in each process. This function sets up the communication backend and process groups.

    ```python
    import torch.distributed as dist

    dist.init_process_group(backend='nccl', init_method='tcp://localhost:23456', world_size=4, rank=0)
    ```

    - **backend:** Specifies the backend to use (e.g., `'nccl'`, `'gloo'`, `'mpi'`).
    - **init_method:** Specifies how processes connect to one another. Commonly, `tcp://` is used with a known IP address and port.
    - **world_size:** The total number of processes involved.
    - **rank:** The rank of the current process. It should be unique for each process in the group.

2. **Collective Communication:**
   PyTorch provides several collective communication operations that facilitate data exchange among processes, including:
   
   - **`all_reduce(tensor)`:** Aggregates the contents of the input tensor across all processes. By default, it sums the tensors.
     ```python
     tensor = torch.ones(4).cuda() * rank
     dist.all_reduce(tensor, op=dist.ReduceOp.SUM)
     ```
     This sums the tensor values across all processes and distributes the result to every process.

   - **`broadcast(tensor, src)`:** Copies the tensor from the source process (`src`) to all other processes.
     ```python
     dist.broadcast(tensor, src=0)
     ```
   
   - **`all_gather(tensor_list, tensor)`:** Gathers tensors from all processes into a list.
     ```python
     tensor_list = [torch.zeros(4).cuda() for _ in range(world_size)]
     dist.all_gather(tensor_list, tensor)
     ```
   
   - **`reduce(tensor, dst)`:** Aggregates the tensor across all processes and stores the result on the destination process (`dst`).
     ```python
     dist.reduce(tensor, dst=0, op=dist.ReduceOp.SUM)
     ```

3. **Distributed Data Parallel (DDP):**
   PyTorch provides the `torch.nn.parallel.DistributedDataParallel` (DDP) wrapper to facilitate data parallel training with minimal code changes. DDP wraps the model and handles gradient synchronization across processes using `torch.distributed`.

    ```python
    import torch
    import torch.nn as nn
    from torch.nn.parallel import DistributedDataParallel as DDP

    # Initialize the distributed process group
    dist.init_process_group(backend='nccl')

    # Set up model, move it to the appropriate GPU, and wrap it with DDP
    model = nn.Linear(10, 1).cuda()
    model = DDP(model)

    # Training loop
    for data, target in train_loader:
        data, target = data.cuda(), target.cuda()
        output = model(data)
        loss = criterion(output, target)
        loss.backward()
        optimizer.step()
    ```

   In this example, DDP automatically handles gradient synchronization using `all_reduce` operations.

#### **4. Communication Backends**
`torch.distributed` supports various backends, each with specific use cases:

- **NCCL (NVIDIA Collective Communications Library):** Optimized for multi-GPU communication on NVIDIA hardware. It is the recommended backend for single-node or multi-node training on GPUs.
- **Gloo:** A CPU-based backend that also supports communication between GPUs. Useful in environments without NCCL (e.g., when using CPUs for distributed training).
- **MPI (Message Passing Interface):** Commonly used in HPC environments where MPI is already available.

The choice of backend depends on the hardware and the nature of the distributed setup.

#### **5. Multi-Node Training with `torch.distributed`**
To extend distributed training across multiple nodes (machines), `init_process_group` must be configured to connect all nodes via a network. This typically involves specifying the `init_method` using a `tcp://` address or a file that all nodes can access.

Example command to launch distributed training across multiple nodes:

```bash
python -m torch.distributed.launch --nproc_per_node=8 --nnodes=2 --node_rank=0 --master_addr="192.168.1.1" --master_port=23456 train.py
```
- **`--nproc_per_node`:** Number of processes to launch on each node (typically set to the number of GPUs).
- **`--nnodes`:** Total number of nodes participating in the training.
- **`--node_rank`:** Rank of the current node (each node gets a unique rank).
- **`--master_addr` and `--master_port`:** Address and port for the master process that coordinates the nodes.

#### **6. Best Practices for Using `torch.distributed`**
- **Ensure Correct Environment:** Properly set environment variables like `CUDA_VISIBLE_DEVICES` to control GPU allocation for each process.
- **Synchronous Training:** Use `DistributedDataParallel` for synchronized training where gradients are averaged across GPUs to maintain a consistent model state.
- **Avoid Stale Processes:** Always ensure `dist.destroy_process_group()` is called at the end of the script to clean up resources.
- **Gradient Accumulation:** For very large models, accumulate gradients over several batches before synchronizing to reduce communication overhead.

#### **7. Conclusion**
`torch.distributed` provides robust primitives and tools for scaling deep learning workloads across multiple GPUs and nodes. It supports several backends like NCCL, Gloo, and MPI, allowing flexibility in different environments. With `DistributedDataParallel`, PyTorch simplifies the process of synchronous multi-GPU training, handling the complexities of gradient synchronization under the hood. Understanding `torch.distributed` and leveraging its features effectively is key to building scalable and efficient deep learning models.


---

preceding: [[nccl (nvidia collective communications library)]]  


following: [[megatron nvidia's large-scale transformer model framework]]