---
date: '2024-11-23T23:33:06.327237Z'
tags:
- autogenerated
- source__claude
- conversation_id__34b23437-5024-480e-b2d6-099a0b620355
- msg_83aec471-f25d-4fc3-bcc9-23614584fa2a
- msg_1b47f5c3-0f4b-4b08-9f78-0f5748368de1
- colorclass/information-theory
title: references for batch size optimization and training dynamics
---

**USER**

references


**ASSISTANT**

# References for Batch Size Optimization and Training Dynamics

## Foundational Papers

1. [[Scaling Laws for Neural Language Models]] (Kaplan et al., 2020)
- Established fundamental relationships between compute, model size, and performance
- Introduced empirical scaling laws for language models
- Demonstrated sqrt(k) efficiency scaling

2. [[On Large-Batch Training for Deep Learning: Generalization Gap and Sharp Minima]] (Keskar et al., 2017)
- First major analysis of large batch training issues
- Introduced concept of sharp vs flat minima
- Connected batch size to generalization ability

3. [[Accurate, Large Minibatch SGD: Training ImageNet in 1 Hour]] (Goyal et al., 2017)
- Practical demonstration of large batch training
- Introduced linear learning rate scaling rule
- Established best practices for large batch optimization

## Theoretical Foundations

4. [[An Analysis of Constant Step Size SGD in the Non-convex Regime]] (Zhang et al., 2019)
- Rigorous analysis of SGD dynamics
- Proved convergence bounds
- Connected batch size to optimization trajectory

5. [[Train Longer, Generalize Better: Closing the Generalization Gap in Large Batch Training]] (Hoffer et al., 2017)
- Analysis of training time vs batch size trade-offs
- Demonstrated importance of epoch count
- Introduced budget-aware training strategies

## Hardware Efficiency

6. [[A Systematic Approach to Blocking and Tuning Convolution Operations on CPUs]] (Anderson et al., 2018)
- Detailed analysis of hardware utilization
- Mathematical models of throughput
- Optimization strategies for different architectures

7. [[NVIDIA Deep Learning Performance Guide]] (NVIDIA, 2023)
- Hardware-specific optimization strategies
- Memory bandwidth analysis
- Practical throughput measurements

## Statistical Analysis

8. [[Statistical Mechanics of Learning]] (Engel & Van den Broeck, 2001)
- Theoretical framework for batch learning
- Connection to statistical physics
- Information-theoretic bounds

9. [[Information-theoretic Understanding of Batch Size in Stochastic Gradient Descent]] (Zhang & Liang, 2019)
- Information flow analysis
- Optimal batch size derivation
- Connection to generalization

## Practical Methods

10. [[Dynamic Batch Size Adaptation for Distributed Training]] (Smith et al., 2018)
- Adaptive batch sizing algorithms
- Runtime optimization techniques
- Distributed training considerations

11. [[A Disciplined Approach to Neural Network Hyper-Parameters]] (Smith, 2018)
- Systematic hyperparameter optimization
- Batch size selection methodology
- Learning rate relationships

## Recent Developments

12. [[Gradient Noise Scale]] (McCandlish et al., 2019)
- Noise scale analysis
- Optimal batch size prediction
- Scaling properties of neural networks

13. [[PowerSGD: Practical Low-Rank Gradient Compression]] (Vogels et al., 2019)
- Compression techniques
- Efficiency improvements
- Communication optimization

## Review Papers

14. [[A Comprehensive Survey of Neural Network Training Efficiency]] (Chen et al., 2021)
- Survey of training methods
- Efficiency metrics
- Future research directions

15. [[Optimization Methods for Large-Scale Machine Learning]] (Bottou et al., 2018)
- Comprehensive optimization review
- Theoretical foundations
- Practical guidelines

## Related Topics

16. [[The Effect of Network Width on Stochastic Gradient Descent]] (Park et al., 2019)
- Interaction between width and batch size
- Optimization dynamics
- Scaling relationships

17. [[Deep Learning with Limited Numerical Precision]] (Gupta et al., 2015)
- Mixed precision training
- Memory efficiency
- Hardware considerations

## Meta-Analyses

18. [[Revisiting Small Batch Training for Deep Neural Networks]] (Masters & Luschi, 2018)
- Critical analysis of batch size myths
- Experimental validation
- Best practices synthesis

19. [[An Empirical Model of Large-Batch Training]] (McCandlish et al., 2018)
- Comprehensive empirical analysis
- Predictive modeling
- Performance optimization

These references form a comprehensive foundation for understanding the interplay between batch size, computational efficiency, and training dynamics in modern machine learning systems.


---

preceding: [[optimal batch size under fixed time budget with efficiency-throughput trade-off]]  


following: [[large minibatch sgd training imagenet in 1 hour]]

See also: [[statistical physics]]