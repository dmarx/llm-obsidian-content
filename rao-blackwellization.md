---
date: 1720850629.134734
tags:
- autogenerated
- source__oai
- conversation_id__4d853f0d-781b-4652-84ec-fa8598fc634d
- msg_aaa21b6d-f868-42b5-8230-9115db5899fa
- msg_9e430c80-4e47-4f8f-b703-1062938eb312
- colorclass/informed-decision
title: rao-blackwellization
---

**USER**

Rao-Blackwellization


**ASSISTANT**

Rao-Blackwellization is a technique used in statistics and Bayesian inference to reduce the variance of an estimator. Named after C.R. Rao and David Blackwell, it leverages the idea that if you have an unbiased estimator, you can often create a new estimator that has lower variance and is also unbiased by conditioning on a sufficient statistic.

### Definition and Theoretical Background

Consider a parameter $\theta$ that we want to estimate based on a sample $\mathbf{X} = (X_1, X_2, \ldots, X_n)$ from a probability distribution $P_\theta$. An estimator $\hat{\theta} = g(\mathbf{X})$ is a function of the sample. 

#### Sufficient Statistic

A statistic $T(\mathbf{X})$ is sufficient for the parameter $\theta$ if the conditional distribution of $\mathbf{X}$ given $T(\mathbf{X})$ does not depend on $\theta$. In other words, $T(\mathbf{X})$ captures all the information about $\theta$ that is present in $\mathbf{X}$.

Mathematically, $T(\mathbf{X})$ is sufficient for $\theta$ if the conditional distribution $P(\mathbf{X} \mid T(\mathbf{X}))$ is independent of $\theta$. This is formalized by the Fisher-Neyman factorization theorem:

$$
P(\mathbf{X} \mid \theta) = h(\mathbf{X}) \cdot g(T(\mathbf{X}), \theta)
$$

where $h(\mathbf{X})$ does not depend on $\theta$, and $g(T(\mathbf{X}), \theta)$ is a function of the sufficient statistic and the parameter.

#### Rao-Blackwell Theorem

The Rao-Blackwell theorem states that if $\hat{\theta}$ is an unbiased estimator of $\theta$, and $T(\mathbf{X})$ is a sufficient statistic for $\theta$, then the conditional expectation $\tilde{\theta} = \mathbb{E}[\hat{\theta} \mid T(\mathbf{X})]$ is also an unbiased estimator of $\theta$, and its variance is less than or equal to the variance of $\hat{\theta}$.

Mathematically, 

$$
\mathrm{Var}(\tilde{\theta}) \leq \mathrm{Var}(\hat{\theta})
$$

with equality if and only if $\hat{\theta}$ is a function of $T(\mathbf{X})$.

### Practical Application

The process of Rao-Blackwellization involves the following steps:

1. **Identify an Unbiased Estimator**: Start with an unbiased estimator $\hat{\theta}$ for the parameter $\theta$.
2. **Find a Sufficient Statistic**: Determine a sufficient statistic $T(\mathbf{X})$ for the parameter $\theta$.
3. **Condition on the Sufficient Statistic**: Compute the conditional expectation of the original estimator given the sufficient statistic, $\tilde{\theta} = \mathbb{E}[\hat{\theta} \mid T(\mathbf{X})]$.

### Example: Estimating the Mean of a Normal Distribution

Suppose $\mathbf{X} = (X_1, X_2, \ldots, X_n)$ are i.i.d. samples from a normal distribution $N(\mu, \sigma^2)$, where $\mu$ is the mean and $\sigma^2$ is the variance. 

1. **Unbiased Estimator**: The sample mean $\bar{X} = \frac{1}{n} \sum_{i=1}^n X_i$ is an unbiased estimator of $\mu$.
2. **Sufficient Statistic**: The sample mean $\bar{X}$ is itself a sufficient statistic for $\mu$.
3. **Conditional Expectation**: Since $\bar{X}$ is already a function of the sufficient statistic $\bar{X}$, Rao-Blackwellization in this case does not change the estimator. Hence, $\tilde{\mu} = \mathbb{E}[\bar{X} \mid \bar{X}] = \bar{X}$.

In this trivial example, the Rao-Blackwellized estimator is the same as the original estimator.

### Advanced Example: Estimating the Variance of a Normal Distribution

For the same sample $\mathbf{X}$ from $N(\mu, \sigma^2)$, consider the problem of estimating $\sigma^2$:

1. **Unbiased Estimator**: The sample variance $S^2 = \frac{1}{n-1} \sum_{i=1}^n (X_i - \bar{X})^2$ is an unbiased estimator of $\sigma^2$.
2. **Sufficient Statistic**: The pair $(\bar{X}, S^2)$ is a sufficient statistic for $(\mu, \sigma^2)$.
3. **Conditional Expectation**: The Rao-Blackwellized estimator for $\sigma^2$ would be $\mathbb{E}[S^2 \mid \bar{X}, S^2]$. However, since $S^2$ is already a function of the sufficient statistic $(\bar{X}, S^2)$, the Rao-Blackwellized estimator is again $S^2$.

### Conclusion

Rao-Blackwellization is a powerful technique in statistical estimation. By conditioning an unbiased estimator on a sufficient statistic, we can often obtain a new estimator with lower variance. This method is widely used in various applications, including Bayesian inference, where it can significantly improve the efficiency of estimators.

For further exploration of the topic, consider diving into [[Unbiased Estimator]], [[Sufficient Statistic]], and [[Bayesian Inference]] for more detailed discussions and additional examples.

See also: [[sufficient statistic]]