---
date: 1717809552.977173
tags:
- autogenerated
- source__oai
- conversation_id__77e70e10-f02a-4f98-bd5c-6217b10122cf
- msg_aaa22efb-9b32-42da-b91c-ad628cac10f7
- msg_157fc5ee-6f1a-457a-a545-5954e99f9f78
- colorclass/system-dynamics
title: optimal control theory
---

**USER**

optimal control theory


**ASSISTANT**

Optimal control theory is a branch of mathematics that deals with finding control laws for a dynamical system over a period of time such that an objective function is optimized. It has applications across various fields, including engineering, economics, and biology. The objective is to determine the control inputs that will cause the system to follow an optimal trajectory.

### Basic Concepts

1. **Dynamical System**:
   A dynamical system is described by differential equations that define how the state of the system evolves over time. The state vector \( \mathbf{x}(t) \) and the control vector \( \mathbf{u}(t) \) are typically time-dependent.

   \[
   \dot{\mathbf{x}}(t) = \mathbf{f}(\mathbf{x}(t), \mathbf{u}(t), t), \quad \mathbf{x}(t_0) = \mathbf{x}_0
   \]

2. **Objective Function**:
   The goal is to minimize (or maximize) an objective function \( J \), which is often called the cost function or performance index.

   \[
   J = \phi(\mathbf{x}(t_f)) + \int_{t_0}^{t_f} L(\mathbf{x}(t), \mathbf{u}(t), t) \, dt
   \]

   Here, \( \phi \) is the terminal cost, and \( L \) is the running cost.

3. **Control Constraints**:
   The control inputs are often subject to constraints.

   \[
   \mathbf{u}(t) \in \mathcal{U} \quad \forall t \in [t_0, t_f]
   \]

### Principle of Optimality

The principle of optimality, introduced by Richard Bellman, states that an optimal policy has the property that, regardless of the initial state and initial decision, the remaining decisions constitute an optimal policy with regard to the state resulting from the first decision.

### Hamiltonian Formulation

The Hamiltonian function \( H \) is central to many optimal control problems, combining the state dynamics and the cost function:

\[
H(\mathbf{x}, \mathbf{u}, \mathbf{\lambda}, t) = L(\mathbf{x}, \mathbf{u}, t) + \mathbf{\lambda}^T \mathbf{f}(\mathbf{x}, \mathbf{u}, t)
\]

Here, \( \mathbf{\lambda} \) is the costate vector (also called the adjoint vector or Lagrange multiplier).

### Pontryagin's Maximum Principle

Pontryagin's Maximum Principle provides necessary conditions for an optimal control problem. It states that if \( \mathbf{u}^*(t) \) is an optimal control, then there exists a costate vector \( \mathbf{\lambda}(t) \) such that:

1. **State Equation**:
   \[
   \dot{\mathbf{x}}^*(t) = \frac{\partial H}{\partial \mathbf{\lambda}}(\mathbf{x}^*(t), \mathbf{u}^*(t), \mathbf{\lambda}(t), t)
   \]

2. **Costate Equation**:
   \[
   \dot{\mathbf{\lambda}}(t) = -\frac{\partial H}{\partial \mathbf{x}}(\mathbf{x}^*(t), \mathbf{u}^*(t), \mathbf{\lambda}(t), t)
   \]

3. **Optimal Control**:
   \[
   \mathbf{u}^*(t) = \arg \min_{\mathbf{u}} H(\mathbf{x}^*(t), \mathbf{u}, \mathbf{\lambda}(t), t)
   \]
   if minimizing, or
   \[
   \mathbf{u}^*(t) = \arg \max_{\mathbf{u}} H(\mathbf{x}^*(t), \mathbf{u}, \mathbf{\lambda}(t), t)
   \]
   if maximizing.

### Dynamic Programming

Dynamic programming is an alternative approach to solving optimal control problems, based on Bellman's principle of optimality. The key concept is the value function \( V(\mathbf{x}, t) \), which represents the minimum cost to go from state \( \mathbf{x} \) at time \( t \) to the terminal state.

The Hamilton-Jacobi-Bellman (HJB) equation provides a way to compute the value function:

\[
\frac{\partial V}{\partial t} + \min_{\mathbf{u}} \left[ L(\mathbf{x}, \mathbf{u}, t) + \frac{\partial V}{\partial \mathbf{x}} \cdot \mathbf{f}(\mathbf{x}, \mathbf{u}, t) \right] = 0
\]

For the infinite horizon problem, the HJB equation simplifies to:

\[
0 = \min_{\mathbf{u}} \left[ L(\mathbf{x}, \mathbf{u}) + \nabla V(\mathbf{x}) \cdot \mathbf{f}(\mathbf{x}, \mathbf{u}) \right]
\]

### Linear Quadratic Regulator (LQR)

The Linear Quadratic Regulator (LQR) is a special case of optimal control problems where the dynamics are linear, and the cost is quadratic. It is one of the most widely used methods due to its simplicity and powerful theoretical properties.

- **Dynamics**:
  \[
  \dot{\mathbf{x}}(t) = A\mathbf{x}(t) + B\mathbf{u}(t)
  \]

- **Cost Function**:
  \[
  J = \int_{t_0}^{t_f} \left[ \mathbf{x}(t)^T Q \mathbf{x}(t) + \mathbf{u}(t)^T R \mathbf{u}(t) \right] dt
  \]

The optimal control law is given by:
\[
\mathbf{u}^*(t) = -R^{-1} B^T P(t) \mathbf{x}(t)
\]

where \( P(t) \) is the solution to the Riccati differential equation:
\[
-\dot{P}(t) = A^T P(t) + P(t) A - P(t) B R^{-1} B^T P(t) + Q
\]

### Applications

Optimal control theory is used in numerous applications, including:

- **Robotics**: For trajectory planning and control of robotic arms and autonomous vehicles.
- **Economics**: In optimal investment strategies and resource allocation.
- **Aerospace**: For flight path optimization and spacecraft trajectory planning.
- **Biological Systems**: For modeling and controlling biological processes and ecosystems.

### Conclusion

Optimal control theory provides a powerful framework for making decisions in dynamic systems to achieve desired objectives efficiently. By leveraging concepts like the Hamiltonian, Pontryagin's Maximum Principle, dynamic programming, and specialized methods like LQR, optimal control theory offers solutions to complex, real-world problems across various disciplines.


---

preceding: [[hessian approximation]]  


following: [[principal of optimality]]

See also: [[hamilton–jacobi–bellman equations]]


See also: [[stochastic optimal control theory]]


See also: [[control theory]]


See also: [[dynamic programming]]


See also: [[resource allocation]]


See also: [[pontryagin maximal principle]]


See also: [[variational methods]]