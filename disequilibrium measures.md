---
date: 1714769391.351983
tags:
- autogenerated
- source__oai
- conversation_id__212e3628-2bdc-43ef-a6d3-6e47e5e4dfec
- msg_aaa29faf-69f0-4098-b26d-9ad0860f1c21
- msg_65484a4e-e69b-40b9-b7b8-a9f10b8b1baf
- colorclass/information-theory
title: disequilibrium measures
---

**USER**

Disequilibrium Measures


**ASSISTANT**

Disequilibrium is a key concept in various areas of science, especially in statistical mechanics, information theory, and complexity science. It generally refers to a state or condition that is far from equilibrium, where equilibrium denotes a state where all competing influences are balanced. In the context of complexity measures, such as the LMC (Lopez-Ruiz, Mancini, and Calbet) complexity, disequilibrium is used to quantify how much a system's probability distribution deviates from a certain reference distribution, typically the uniform distribution. This deviation is crucial for understanding the structure and behavior of complex systems.

### Definition and Calculation

Disequilibrium is quantified using a measure of distance between the probability distribution of a system and the uniform distribution. The choice of the distance metric can vary, but some common measures include:

1. **Euclidean Distance**: This is a straightforward method where the disequilibrium \( D(X) \) for a discrete random variable \( X \) with probabilities \( p_i \) is calculated as the Euclidean distance from the uniform distribution \( u_i = 1/N \) (where \( N \) is the number of states or outcomes for \( X \)):
   $$
   D(X) = \sqrt{\sum_{i=1}^N (p_i - u_i)^2}
   $$
   This measure emphasizes the geometric spread of the probabilities away from uniformity.

2. **Squared Euclidean Distance**: Sometimes the square of the Euclidean distance is used to provide a smoother measure:
   $$
   D(X) = \sum_{i=1}^N (p_i - u_i)^2
   $$
   This variant gives more weight to larger deviations, enhancing the impact of states with significantly higher or lower probabilities than expected under uniformity.

3. **Kullback-Leibler Divergence**: While not traditionally categorized under disequilibrium measures, KL divergence can also serve this purpose by measuring how one probability distribution diverges from another, typically a uniform distribution:
   $$
   D_{KL}(P \| U) = \sum_{i=1}^N p_i \log \frac{p_i}{u_i}
   $$
   This divergence is not symmetric and specifically measures the information lost when a uniform model is assumed incorrectly.

4. **Jensen-Shannon Divergence**: This is a smoothed and symmetric version of the KL divergence and is defined as the average divergence to the mean of two distributions:
   $$
   D_{JS}(P \| U) = \frac{1}{2} D_{KL}(P \| M) + \frac{1}{2} D_{KL}(U \| M)
   $$
   where \( M = \frac{1}{2}(P + U) \). It's particularly useful for assessing the middle ground between two distributions.

### Applications in Complexity Measures

In complexity measures like the LMC complexity, disequilibrium provides a necessary contrast to entropy. While entropy measures the overall uncertainty or randomness of a system, disequilibrium measures how concentrated the distribution is in terms of deviating from uniformity:

- **Balancing Order and Chaos**: In complexity science, the most interesting systems are those that exhibit a balance between order (low entropy, high disequilibrium) and chaos (high entropy, low disequilibrium). High complexity is often observed in systems where both entropy and disequilibrium are significant, indicating a rich, structured, yet unpredictable behavior.

- **Insights into System Dynamics**: Understanding disequilibrium helps in identifying how far a system is from its most disordered state (equilibrium), which can be crucial for predicting system behavior in processes like phase transitions, reaction kinetics, or evolutionary dynamics.

In summary, disequilibrium measures are essential for analyzing and interpreting complex systems, particularly in combination with entropy, to provide a comprehensive picture of system complexity. This dual analysis helps in understanding various phenomena from thermodynamics to biological evolution and social dynamics.


---

preceding: [[lopez-ruiz, mancini, and calbet (lmc) complexity]]  


following: [[jensen-shannon divergence]]

See also: [[information theory]]


See also: [[equilibrium]]