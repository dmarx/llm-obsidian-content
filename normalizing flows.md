---
date: 1721161926.28543
tags:
- autogenerated
- source__oai
- conversation_id__4f78839e-971a-4476-b886-e0eef3bd4caf
- msg_98aff620-1a82-4813-ba6b-ea1b97914cbd
- msg_d897fc2c-b0eb-46e7-8d0a-80879cc23969
- colorclass/information-theory
title: normalizing flows
---

**USER**

Normalizing flows


**ASSISTANT**

Normalizing flows are a powerful method for constructing flexible and expressive probability distributions by transforming a simple base distribution through a series of invertible transformations. This technique is particularly useful in [[variational inference]] and other probabilistic modeling tasks, where the goal is to approximate complex posterior distributions.

## Key Concepts

### Basic Idea

A normalizing flow is a sequence of invertible and differentiable mappings \( f_1, f_2, \ldots, f_K \) that transform a simple base distribution \( p_Z(z) \) (e.g., a standard normal distribution) into a more complex distribution \( p_X(x) \). The final distribution \( x \) is obtained as:

$$
x = f_K \circ f_{K-1} \circ \ldots \circ f_1(z)
$$

### Change of Variables Formula

The change of variables formula is used to compute the probability density of \( x \) under the transformed distribution. If \( x = f(z) \) and \( z = f^{-1}(x) \), the density \( p_X(x) \) is given by:

$$
p_X(x) = p_Z(z) \left| \det \left( \frac{\partial f^{-1}(x)}{\partial x} \right) \right|
$$

or equivalently,

$$
p_X(x) = p_Z(z) \left| \det \left( \frac{\partial f(z)}{\partial z} \right) \right|^{-1}
$$

### Log-Probability

In practice, we often work with the log-probability:

$$
\log p_X(x) = \log p_Z(z) - \log \left| \det \left( \frac{\partial f(z)}{\partial z} \right) \right|
$$

where \( z = f^{-1}(x) \).

## Types of Normalizing Flows

### Planar Flows

Planar flows introduce a single invertible transformation that reshapes the distribution along a single direction. The transformation is defined as:

$$
f(z) = z + u h(w^T z + b)
$$

where \( u \), \( w \), and \( b \) are parameters, and \( h \) is a nonlinear activation function (e.g., \( \tanh \)). The Jacobian determinant for planar flows is:

$$
\det \left( \frac{\partial f(z)}{\partial z} \right) = \left| 1 + u^T h'(w^T z + b) w \right|
$$

### Radial Flows

Radial flows introduce a transformation that reshapes the distribution radially around a point \( z_0 \). The transformation is defined as:

$$
f(z) = z + \beta h(\alpha, r)(z - z_0)
$$

where \( \alpha \), \( \beta \), and \( z_0 \) are parameters, \( r = \|z - z_0\| \), and \( h(\alpha, r) \) is a scalar function (e.g., \( h(\alpha, r) = \frac{1}{\alpha + r} \)). The Jacobian determinant for radial flows is:

$$
\det \left( \frac{\partial f(z)}{\partial z} \right) = \left| 1 + \beta h(\alpha, r) + \beta h'(\alpha, r) r \right|
$$

### Affine Coupling Layers

Affine coupling layers partition the input vector \( z \) into two parts \( (z_A, z_B) \). The transformation is defined as:

$$
f(z_A, z_B) = (z_A, z_B \odot \exp(s(z_A)) + t(z_A))
$$

where \( s \) and \( t \) are scaling and translation functions, respectively, and \( \odot \) denotes element-wise multiplication. The Jacobian determinant for affine coupling layers is easy to compute:

$$
\det \left( \frac{\partial f(z)}{\partial z} \right) = \exp \left( \sum s(z_A) \right)
$$

### Invertible Neural Networks

Invertible neural networks use special architectures that ensure the invertibility of the transformation. Examples include:

- **RealNVP**: Uses a series of affine coupling layers.
- **MAF (Masked Autoregressive Flow)**: Applies an autoregressive transformation where each variable is transformed based on preceding variables.
- **IAF (Inverse Autoregressive Flow)**: Similar to MAF but designed for variational autoencoders.

## Training Normalizing Flows

### Variational Inference

In [[variational inference]], normalizing flows are used to construct flexible variational distributions \( q(\theta \mid \lambda) \) to approximate the posterior distribution. The ELBO with normalizing flows is:

$$
\text{ELBO}(\lambda) = \mathbb{E}_{q(\theta \mid \lambda)}[\log p(x, \theta)] - \mathbb{E}_{q(\theta \mid \lambda)}[\log q(\theta \mid \lambda)]
$$

The variational distribution \( q(\theta \mid \lambda) \) is obtained by applying a series of flow transformations to a simple base distribution.

### Maximum Likelihood

Normalizing flows can also be trained directly using maximum likelihood estimation. Given a dataset \( \{x_i\} \), the log-likelihood is maximized:

$$
\sum_{i=1}^N \log p_X(x_i) = \sum_{i=1}^N \left[ \log p_Z(f^{-1}(x_i)) - \log \left| \det \left( \frac{\partial f^{-1}(x_i)}{\partial x_i} \right) \right| \right]
$$

Gradient-based optimization methods (e.g., Adam) are used to optimize the parameters of the flow transformations.

## Applications of Normalizing Flows

### Density Estimation

Normalizing flows can be used for density estimation, learning complex distributions from data. By transforming a simple distribution into a complex one, flows can model intricate data distributions.

### Variational Autoencoders (VAEs)

Normalizing flows enhance the expressiveness of the posterior approximation in VAEs, leading to better generative models.

### Bayesian Inference

Normalizing flows improve the flexibility and accuracy of variational distributions in Bayesian inference, allowing better approximation of complex posteriors.

### Generative Models

Normalizing flows are used in generative models to sample from complex distributions, enabling applications in image generation, audio synthesis, and more.

## Conclusion

Normalizing flows provide a powerful and flexible framework for constructing complex probability distributions through a sequence of invertible transformations. By leveraging the change of variables formula and advanced parameterization strategies, normalizing flows enable efficient and accurate approximation of posterior distributions in [[variational inference]] and other probabilistic modeling tasks. Their applications span density estimation, Bayesian inference, and generative modeling, making them a versatile tool in modern machine learning and statistics.


---

preceding: [[parameterization]]  


following: [[realnvp]]

See also: [[density estimation]]


See also: [[clifford group equivariant neural networks]]


See also: [[score matching]]


See also: [[generative models]]


See also: [[change of variables]]