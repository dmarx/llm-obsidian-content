---
date: 1717810376.590348
tags:
- autogenerated
- source__oai
- conversation_id__77e70e10-f02a-4f98-bd5c-6217b10122cf
- msg_aaa2f5aa-d18b-47d6-bc82-0603e77b29af
- msg_7917d682-24a3-4d44-b2e2-37965f59b236
- colorclass/system-dynamics
title: stochastic optimal control theory
---

**USER**

stochastic [[optimal control theory]]


**ASSISTANT**

Stochastic [[optimal control theory]] extends traditional optimal control to systems affected by uncertainty. This field is crucial for designing control policies that are robust to the inherent randomness in many real-world systems, such as financial markets, weather, and biological processes.

### Problem Setup

In stochastic optimal control, the system dynamics are influenced by both control inputs and stochastic processes (e.g., noise). The state dynamics can be modeled using stochastic differential equations (SDEs).

#### Stochastic Differential Equation (SDE)

Consider the system:
\[ d\mathbf{x}(t) = \mathbf{f}(\mathbf{x}(t), \mathbf{u}(t), t) dt + \mathbf{G}(\mathbf{x}(t), t) d\mathbf{w}(t), \]
where:
- \( \mathbf{x}(t) \in \mathbb{R}^n \) is the state vector.
- \( \mathbf{u}(t) \in \mathbb{R}^m \) is the control vector.
- \( \mathbf{f}(\mathbf{x}, \mathbf{u}, t) \) represents the deterministic part of the dynamics.
- \( \mathbf{G}(\mathbf{x}, t) \) represents the diffusion matrix.
- \( \mathbf{w}(t) \) is a standard Wiener process (or Brownian motion).

The objective is to minimize the expected value of a cost functional \( J \) over a given time horizon.

### Cost Functional

The cost functional for a stochastic control problem is typically of the form:
\[ J(\mathbf{u}) = \mathbb{E} \left[ \phi(\mathbf{x}(T)) + \int_{0}^{T} L(\mathbf{x}(t), \mathbf{u}(t), t) dt \right], \]
where:
- \( \phi(\mathbf{x}(T)) \) is the terminal cost.
- \( L(\mathbf{x}, \mathbf{u}, t) \) is the running cost.
- \( \mathbb{E} \) denotes the expectation operator.

### Dynamic Programming and the Hamilton-Jacobi-Bellman (HJB) Equation

The dynamic programming principle can be applied to derive the Hamilton-Jacobi-Bellman equation for the stochastic control problem. The value function \( V(\mathbf{x}, t) \) represents the minimum expected cost-to-go from state \( \mathbf{x} \) at time \( t \).

#### HJB Equation for Stochastic Systems

The HJB equation in the context of stochastic control is:
\[ \frac{\partial V}{\partial t} + \min_{\mathbf{u}} \left[ L(\mathbf{x}, \mathbf{u}, t) + \nabla V \cdot \mathbf{f}(\mathbf{x}, \mathbf{u}, t) + \frac{1}{2} \text{Tr} \left( \mathbf{G}^T \mathbf{H} V \mathbf{G} \right) \right] = 0, \]
with terminal condition:
\[ V(\mathbf{x}, T) = \phi(\mathbf{x}(T)). \]

Here:
- \( \nabla V \) is the gradient of the value function with respect to the state.
- \( \mathbf{H} V \) is the Hessian matrix of second-order partial derivatives of the value function.
- \( \text{Tr}(\cdot) \) denotes the trace of a matrix.

### Linear Quadratic Gaussian (LQG) Control

A special case of stochastic optimal control is the Linear Quadratic Gaussian (LQG) problem, which combines Linear Quadratic Regulator (LQR) control with Gaussian noise.

#### System Dynamics

The dynamics of an LQG system are given by:
\[ d\mathbf{x}(t) = (A\mathbf{x}(t) + B\mathbf{u}(t)) dt + \mathbf{G} d\mathbf{w}(t), \]
where:
- \( \mathbf{w}(t) \) is a Wiener process representing process noise.

#### Cost Functional

The cost functional for the LQG problem is:
\[ J = \mathbb{E} \left[ \mathbf{x}(T)^T F \mathbf{x}(T) + \int_{0}^{T} (\mathbf{x}(t)^T Q \mathbf{x}(t) + \mathbf{u}(t)^T R \mathbf{u}(t)) dt \right], \]
where \( Q \geq 0 \), \( R > 0 \), and \( F \geq 0 \) are the weighting matrices.

#### Solution to the LQG Problem

1. **State Feedback Control (LQR)**:
   The optimal control law \( \mathbf{u}(t) = -K \mathbf{x}(t) \) is obtained by solving the deterministic LQR problem:
   \[ \dot{P}(t) = -Q - A^T P - P A + P B R^{-1} B^T P, \]
   with boundary condition \( P(T) = F \).

2. **Kalman Filter**:
   The optimal state estimate \( \hat{\mathbf{x}}(t) \) is obtained using a Kalman filter:
   \[ d\hat{\mathbf{x}}(t) = (A \hat{\mathbf{x}}(t) + B \mathbf{u}(t) + L(t) (\mathbf{y}(t) - C \hat{\mathbf{x}}(t))) dt, \]
   where \( \mathbf{y}(t) = C \mathbf{x}(t) + \mathbf{v}(t) \) is the measurement with measurement noise \( \mathbf{v}(t) \).

   The filter gain \( L(t) \) is obtained by solving the Riccati equation:
   \[ \dot{P}(t) = A P + P A^T - P C^T R_v^{-1} C P + \mathbf{G} \mathbf{G}^T, \]
   with initial condition \( P(0) = P_0 \).

### Example

Consider an LQG problem with:
\[ A = \begin{bmatrix} 0 & 1 \\ 0 & -1 \end{bmatrix}, \quad B = \begin{bmatrix} 0 \\ 1 \end{bmatrix}, \quad \mathbf{G} = \begin{bmatrix} 0 \\ 1 \end{bmatrix}, \]
\[ Q = \begin{bmatrix} 1 & 0 \\ 0 & 1 \end{bmatrix}, \quad R = 1, \quad F = \begin{bmatrix} 1 & 0 \\ 0 & 1 \end{bmatrix}. \]

1. **State Feedback Control**:
   Solve the Riccati equation to find \( P(t) \).

2. **Kalman Filter**:
   Implement the Kalman filter to estimate the state.

### Summary

Stochastic [[optimal control theory]] extends deterministic optimal control to systems with inherent randomness, allowing the design of control policies that are robust to uncertainty. Key concepts include the Hamilton-Jacobi-Bellman equation for stochastic systems and the Linear Quadratic Gaussian control problem. These tools are essential for managing complex systems in various fields, including finance, engineering, and biology.


---

preceding: [[bellman equations]]  


following: [[decentralized stochastic control]]

See also: [[optimal control theory]]


See also: [[control theory]]


See also: [[partial derivatives]]


See also: [[dynamic programming]]


See also: [[stochastic processes]]


See also: [[system dynamics]]