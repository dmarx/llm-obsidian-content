---
date: 1726441163.567803
tags:
- autogenerated
- source__oai
- conversation_id__66e757d1-0534-8008-bb6a-afa8342b5ee2
- msg_996322d1-df19-482c-9ade-60edc20ea19e
- msg_aaa22dc7-f05c-4e4a-8dcf-134eb42c71f6
- msg_ecedd7a0-e0bc-40c2-b42a-77d5e7f11f71
- colorclass/edge-case
title: we glossed over system prompt previously. let's dive deeper into what that
  is, how it works, what to expect with respect to pre-existing system prompts embedded
  in product apis the learner may interac
---

**ASSISTANT**

, students learn how to use **`list.append()`** to maintain a per-agent **conversation history**. By tracking both prompts and responses, agents can now maintain state across multiple interactions, enabling more context-aware conversations. This not only reinforces list manipulation but also demonstrates how conversation logs are essential in LLM-based applications.


**USER**

we glossed over system prompt previously. let's dive deeper into what that is, how it works, what to expect with respect to pre-existing system prompts embedded in product APIs the learner may interact with as an llm professional, and tips for authoring them. then discuss `role` in the context of LLM interactions and utilize role in the conversation history. I think there's a way to submit the history back to the api right? for context? or is the context pre-filled according to a session token or something like that? the student should basically have a CLI chatbot with session-persistent history at this point.


**ASSISTANT**

Great! This is a fantastic opportunity to dive deeper into the **system prompt** and the concept of **roles** in LLM interactions. We'll explore how system prompts work, what they do, and what to expect when interacting with LLMs embedded in product APIs. Additionally, we'll discuss tips for writing effective system prompts, introduce the concept of **roles** (like `user`, `assistant`, and `system`), and explain how session-persistent history works.

In this context, we’ll also update the `Agent` class to submit the conversation history back to the API, allowing for context-aware, session-based conversations.

---

### **System Prompt: What It Is and How It Works**

#### **What Is a System Prompt?**
A **system prompt** is a special kind of prompt used in LLM interactions to set the behavior or tone of the model. It helps define the assistant's "role" in the conversation and provides instructions on how it should respond to user inputs.

- The **system prompt** serves as the model’s initial guidance, ensuring consistency in its responses.
- It is usually **hidden** from the user and is designed to shape the assistant’s behavior without appearing in the conversation history.

#### **How Does It Work?**
- **Pre-existing system prompts**: Some APIs (like ChatGPT’s API) already embed a default system prompt that influences the assistant's responses. For example, it might define the assistant as helpful, informative, and polite.
- **Custom system prompts**: Developers can often define their own system prompts to customize the assistant’s behavior for a particular use case (e.g., making the assistant more formal, humorous, etc.).

#### **What to Expect in APIs**:
1. **Embedded System Prompts**: Many product APIs will have pre-set system prompts (e.g., ChatGPT’s default assistant behavior) that you can either override or complement with your own.
2. **Editable System Prompts**: In some APIs, you’ll have full control over the system prompt, allowing you to shape the assistant’s personality and behavior. This is useful for custom agents or domain-specific applications.

---

### **Authoring Effective System Prompts**

When writing system prompts, you want to:
- **Be specific**: Clearly define the assistant's role and how it should behave. For example, “You are an expert technical assistant for software engineers. Answer questions with detailed explanations.”
- **Be concise**: Don’t overload the model with excessive instructions, as it may confuse the model’s behavior.
- **Maintain consistency**: Use language that promotes the behavior you expect. If the assistant is meant to be formal, avoid casual language in the system prompt.

#### **Example System Prompts**:
- “You are a helpful assistant who answers questions concisely and clearly.”
- “You are a playful assistant. Your goal is to engage users with witty and fun responses while being informative.”
- “You are an AI assistant for academic research. You must always provide references and stick to factual information.”

---

### **Role in LLM Interactions**

In many conversational models (e.g., OpenAI’s Chat API), interactions are structured with **roles**:
1. **System**: Defines the assistant’s initial instructions or behavior (via the system prompt).
2. **User**: Represents the human asking questions or giving instructions.
3. **Assistant**: The LLM's responses based on the user inputs and the system's instructions.

In OpenAI’s **chat** API, for example, the conversation history is typically sent back to the model, with each entry in the history associated with one of these roles (`system`, `user`, or `assistant`).

#### **How to Use Roles in API Interactions**:
- **System** role is used to establish context (via system prompt) at the start of the conversation.
- **User** role is used to represent each of the user’s queries or instructions.
- **Assistant** role is used to represent the assistant’s responses to the user.

### **Maintaining Conversation History and Sending Context**

To maintain context across multiple interactions, the API can be provided with the entire **conversation history** (including system prompt, user queries, and assistant responses) each time it is called. This allows the model to retain continuity in the conversation.

OpenAI’s Chat API allows us to send the entire conversation history back, so the model can keep track of previous interactions.

---

### **Step-by-Step: Building a Session-Persistent CLI Chatbot with Conversation History**

Let’s now update our agent to:
1. Support the **system** role via system prompts.
2. Use roles (`system`, `user`, `assistant`) in the conversation history.
3. Send the conversation history back to the API for context-aware, session-persistent interactions.

#### **Updated `Agent` Class with Role-Based History**:

```python
import openai
from jinja2 import Template

class Agent:
    def __init__(self, name, system_prompt_template):
        self.name = name
        self.system_prompt_template = system_prompt_template
        self.history = []  # Conversation history, including system, user, and assistant roles

        # Add the system prompt as the first entry in the conversation
        self.history.append({"role": "system", "content": self.get_system_prompt()})
    
    def get_system_prompt(self):
        template = Template(self.system_prompt_template)
        return template.render(name=self.name)
    
    def query_openai(self, user_input, max_tokens=100, temperature=0.7, top_p=1.0, **kwargs):
        """
        Send the conversation history to the OpenAI API, including the user's input,
        and maintain conversation history.

        Arguments:
        - user_input: The current input from the user.
        - max_tokens: Maximum number of tokens to generate.
        - temperature: Controls randomness in the output.
        - top_p: Controls diversity of word choices.
        - **kwargs: Additional options to pass to the OpenAI API.

        Returns:
        - The assistant's response as a string.
        """
        # Append the user's input to the history
        self.history.append({"role": "user", "content": user_input})
        
        # Send the full conversation history (system, user, and assistant roles) to the API
        response = openai.ChatCompletion.create(
            model="gpt-3.5-turbo",  # Using a chat model that supports role-based history
            messages=self.history,
            max_tokens=max_tokens,
            temperature=temperature,
            top_p=top_p,
            **kwargs
        )
        
        # Extract the assistant's reply from the API response
        assistant_reply = response.choices[0].message["content"]
        
        # Append the assistant's reply to the history
        self.history.append({"role": "assistant", "content": assistant_reply})
        
        return assistant_reply
    
    def get_history(self):
        """
        Retrieve the conversation history, excluding system messages.

        Returns:
        - A list of user and assistant interactions in the conversation.
        """
        # Filter out system messages (just to show user and assistant dialogue)
        return [entry for entry in self.history if entry["role"] != "system"]
```

---

### **How the Updated Agent Class Works**:

1. **System Prompt**: When an agent is initialized, the system prompt (with the role `"system"`) is automatically added to the conversation history.
   
2. **Roles in History**:
   - Each user input is recorded with the `"user"` role.
   - Each assistant response is recorded with the `"assistant"` role.

3. **Context-Aware API Calls**:
   - Every time a user inputs a message, the full conversation history (including system, user, and assistant messages) is sent back to the API. This enables the model to understand the full context of the conversation, making it session-persistent.

---

### **Example Usage of the Chatbot with Session-Persistent History**:

```python
# Define the system prompt template
system_prompt = "You are {{ name }}, an AI assistant that specializes in educational tutoring."

# Create an agent with the system prompt
agent = Agent(name="TutorBot", system_prompt_template=system_prompt)

# Start a conversation with the agent
print("Welcome to TutorBot! Type 'exit' to quit.\n")

while True:
    # Get user input
    user_input = input("User: ")
    
    # Exit the conversation if the user types 'exit'
    if user_input.lower() == "exit":
        break
    
    # Send the user input to the agent and get the assistant's response
    response = agent.query_openai(user_input=user_input, max_tokens=100)
    print(f"TutorBot: {response}")

# Print the conversation history at the end
print("\nConversation History:")
for entry in agent.get_history():
    print(f"{entry['role'].capitalize()}: {entry['content']}")
```

#### **Example Output**:
```bash
Welcome to TutorBot! Type 'exit' to quit.

User: What is a black hole?
TutorBot: A black hole is a region in space where the gravitational pull is so strong that nothing, not even light, can escape from it...

User: How are black holes formed?
TutorBot: Black holes are formed when massive stars collapse under their own gravity at the end of their life cycle...

User: exit

Conversation History:
User: What is a black hole?
Assistant: A black hole is a region in space where the gravitational pull is so strong that nothing, not even light, can escape from it...
User: How are black holes formed?
Assistant: Black holes are formed when massive stars collapse under their own gravity at the end of their life cycle...
```

---

### **Session-Persistent Conversations**:
- The system prompt and conversation history (with roles) are sent back to the API in every interaction, allowing


---

preceding: [[introduce `list.append` and demonstrate how to maintain and utilize per-agent interactionconversation histories]]  


following: [[assignment per-agent personalities]]