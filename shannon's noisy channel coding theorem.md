---
date: 1719333727.762367
tags:
- autogenerated
- source__oai
- conversation_id__b7c336c1-f2dc-4e0a-b2e0-bbe32516848b
- msg_aaa2f68b-90b9-4b8e-bd18-571f65412eaa
- msg_b8810b7c-832d-4659-b11d-e578b516d679
- colorclass/information-theory
title: shannon's noisy channel coding theorem
---

**USER**

Shannon's noisy channel coding theorem


**ASSISTANT**

### Shannon's Noisy Channel Coding Theorem

Shannon's Noisy Channel Coding Theorem is a fundamental result in information theory that provides the limits of reliable communication over a noisy channel. It was first introduced by Claude Shannon in his groundbreaking 1948 paper "A Mathematical Theory of Communication". The theorem establishes the concept of channel capacity and proves that reliable communication is possible at rates below this capacity, even in the presence of noise.

#### Key Concepts

1. **Channel Model**
   - A communication channel is typically modeled as a stochastic process where an input sequence is transmitted and a possibly corrupted output sequence is received.
   - **Discrete Memoryless Channel (DMC)**: A common model where the channel's output depends only on the current input and not on previous inputs or outputs. It is characterized by a transition probability matrix $P(Y|X)$, where $X$ and $Y$ are the input and output alphabets respectively.

2. **Channel Capacity**
   - The channel capacity $C$ is defined as the maximum rate at which information can be reliably transmitted over the channel. Formally, it is given by:
     $$ C = \max_{P_X} I(X; Y), $$
     where $I(X; Y)$ is the mutual information between the input $X$ and the output $Y$, and the maximization is over all possible input distributions $P_X$.

3. **Mutual Information**
   - Mutual information $I(X; Y)$ quantifies the amount of information that the output $Y$ contains about the input $X$. It is defined as:
     $$ I(X; Y) = H(X) - H(X|Y), $$
     where $H(X)$ is the entropy of $X$ and $H(X|Y)$ is the conditional entropy of $X$ given $Y$.

#### Theorem Statement

**Noisy Channel Coding Theorem**:
For a discrete memoryless channel with capacity $C$, for any $\epsilon > 0$ and any rate $R < C$, there exists a coding scheme such that the probability of decoding error can be made arbitrarily small for sufficiently large block lengths $n$.

#### Formal Statement

Let $W$ be the message set, with $|W| = 2^{nR}$. For any rate $R < C$ and any $\epsilon > 0$, there exists an encoding function $f: W \to X^n$ and a decoding function $g: Y^n \to W$ such that the probability of decoding error $P_e$ satisfies:
$$ P_e = \Pr(g(Y^n) \neq W) < \epsilon, $$
for sufficiently large block length $n$.

#### Proof Outline

1. **Random Codebook Generation**
   - Generate a random codebook by selecting $2^{nR}$ codewords independently according to the input distribution $P_X$.

2. **Encoding and Transmission**
   - Each message $w \in W$ is mapped to a codeword $X^n(w)$ and transmitted over the channel.

3. **Decoding**
   - The receiver observes the output $Y^n$ and selects the codeword that is most likely to have been transmitted, based on the maximum likelihood or maximum a posteriori criterion.

4. **Error Probability Analysis**
   - The probability of decoding error is analyzed using the concept of typicality and the Asymptotic Equipartition Property (AEP). For sufficiently large $n$, the error probability can be made arbitrarily small, demonstrating that reliable communication is possible.

#### Implications

- **Achievability**: For any rate $R < C$, there exist codes that allow reliable communication.
- **Converse**: Any rate $R > C$ will result in a non-zero probability of error for any coding scheme, making reliable communication impossible.

#### Practical Considerations

While Shannon's theorem provides theoretical limits, practical coding schemes such as [[Linear Codes]], [[Convolutional Codes]], and [[Turbo Codes]] are designed to approach these limits. Modern error-correcting codes like [[Low-Density Parity-Check (LDPC) Codes]] and [[Polar Codes]] can achieve performance close to the Shannon limit in various communication systems.

In summary, Shannon's Noisy Channel Coding Theorem is a cornerstone of information theory, establishing the fundamental limits of data transmission over noisy channels and providing a theoretical foundation for the development of practical coding schemes that enable reliable communication in the presence of noise.


---

preceding: [[coding theory]]  


following: [[channel capacity]]

See also: [[conditional entropy]]


See also: [[mutual information]]


See also: [[information theory]]