---
date: 1715969791.142368
tags:
- autogenerated
- source__oai
- conversation_id__e536633b-8eed-4822-967e-58113d1a33c2
- msg_aaa25275-5df8-456b-9ace-2621a579922d
- msg_f2b83f25-3924-4a96-8cf8-1c97871bae56
- colorclass/information-theory
title: bienaymé formula
---

**USER**

Bienaymé formula


**ASSISTANT**

## Bienaymé Formula

The [[Bienaymé Formula]] is a fundamental result in probability theory, named after the French mathematician [[Irénée-Jules Bienaymé]]. It provides a way to compute the variance of the sum of random variables, particularly when the variables are not necessarily independent.

### Statement of the Formula

The Bienaymé Formula states that for any set of random variables \( X_1, X_2, \ldots, X_n \), the variance of their sum is given by:

$$
\mathrm{Var}\left(\sum_{i=1}^n X_i\right) = \sum_{i=1}^n \mathrm{Var}(X_i) + 2 \sum_{1 \le i < j \le n} \mathrm{Cov}(X_i, X_j)
$$

### Detailed Explanation

#### Variance and Covariance

- **Variance**: For a random variable \( X \), the variance \( \mathrm{Var}(X) \) measures the spread of \( X \) around its mean \( \mathbb{E}[X] \). It is defined as:

$$
\mathrm{Var}(X) = \mathbb{E}[(X - \mathbb{E}[X])^2]
$$

- **Covariance**: For two random variables \( X \) and \( Y \), the covariance \( \mathrm{Cov}(X, Y) \) measures the degree to which \( X \) and \( Y \) change together. It is defined as:

$$
\mathrm{Cov}(X, Y) = \mathbb{E}[(X - \mathbb{E}[X])(Y - \mathbb{E}[Y])]
$$

#### Application of the Formula

Given the random variables \( X_1, X_2, \ldots, X_n \), the formula can be derived by expanding the square of the sum:

$$
\mathrm{Var}\left(\sum_{i=1}^n X_i\right) = \mathrm{Var}\left(X_1 + X_2 + \cdots + X_n\right)
$$

Using the definition of variance:

$$
\mathrm{Var}\left(\sum_{i=1}^n X_i\right) = \mathbb{E}\left[\left(\sum_{i=1}^n (X_i - \mathbb{E}[X_i])\right)^2\right]
$$

Expanding the square inside the expectation:

$$
\mathrm{Var}\left(\sum_{i=1}^n X_i\right) = \mathbb{E}\left[\sum_{i=1}^n (X_i - \mathbb{E}[X_i])^2 + 2 \sum_{1 \le i < j \le n} (X_i - \mathbb{E}[X_i])(X_j - \mathbb{E}[X_j])\right]
$$

Breaking this into separate expectations:

$$
\mathrm{Var}\left(\sum_{i=1}^n X_i\right) = \sum_{i=1}^n \mathbb{E}[(X_i - \mathbb{E}[X_i])^2] + 2 \sum_{1 \le i < j \le n} \mathbb{E}[(X_i - \mathbb{E}[X_i])(X_j - \mathbb{E}[X_j])]
$$

Recognizing the definitions of variance and covariance:

$$
\mathrm{Var}\left(\sum_{i=1}^n X_i\right) = \sum_{i=1}^n \mathrm{Var}(X_i) + 2 \sum_{1 \le i < j \le n} \mathrm{Cov}(X_i, X_j)
$$

### Special Case: Independent Random Variables

If the random variables \( X_1, X_2, \ldots, X_n \) are independent, their covariances are zero (\( \mathrm{Cov}(X_i, X_j) = 0 \) for \( i \ne j \)), and the Bienaymé Formula reduces to:

$$
\mathrm{Var}\left(\sum_{i=1}^n X_i\right) = \sum_{i=1}^n \mathrm{Var}(X_i)
$$

### Examples

1. **Sum of Independent Variables**:
   Let \( X_1, X_2, \ldots, X_n \) be independent random variables with variances \( \sigma_1^2, \sigma_2^2, \ldots, \sigma_n^2 \). Then:

$$
\mathrm{Var}\left(\sum_{i=1}^n X_i\right) = \sum_{i=1}^n \sigma_i^2
$$

2. **Sum of Correlated Variables**:
   Suppose \( X_1 \) and \( X_2 \) are random variables with variances \( \sigma_1^2 \) and \( \sigma_2^2 \), and covariance \( \mathrm{Cov}(X_1, X_2) = \sigma_{12} \). Then:

$$
\mathrm{Var}(X_1 + X_2) = \sigma_1^2 + \sigma_2^2 + 2\sigma_{12}
$$

### Applications

The Bienaymé Formula is widely used in:

- **Risk Management**: In finance, it helps in the calculation of the risk (variance) of a portfolio of assets, taking into account the variances of individual assets and their covariances.
- **Error Analysis**: In experimental physics and engineering, it is used to propagate errors when combining multiple measurements.
- **Statistical Analysis**: It is essential in understanding the distribution of the sum of random variables, which is a common scenario in various statistical models.

### References

- Ross, S. M. (2006). *A First Course in Probability*. Pearson.
- Casella, G., & Berger, R. L. (2002). *Statistical Inference*. Duxbury.

The Bienaymé Formula is thus a fundamental tool in probability and statistics, providing a clear and concise way to compute the variance of the sum of random variables, accounting for their individual variances and covariances.


---

preceding: [[karhunen–loève theorem]]  


following: [[empirical orthogonal functions (eofs)]]

See also: [[probability theory]]


See also: [[random variables]]