---
date: '2025-06-22T08:56:48.991332Z'
tags:
- autogenerated
- source__claude
- conversation_id__9b94ea9a-2594-48f2-b5a8-86ebb44d387a
- msg_b9d7395b-cc51-49b8-86f4-05a2c2918fde
- msg_48e6d62e-5992-4d90-9447-03ff4c793d0d
- colorclass/decision-making-processes
title: formal decision tools
---

**USER**

formal decision tools


**ASSISTANT**

The user is asking for a wiki entry on "formal decision tools." This refers to structured, mathematical, and systematic approaches to decision-making that help overcome cognitive biases and provide more rational decision outcomes. This would include decision theory, multi-criteria decision analysis, game theory applications, optimization methods, etc.

I need to provide a comprehensive technical wiki entry covering:

1. Definition and mathematical formalization
2. Various types of formal decision tools (decision trees, utility theory, MCDM, etc.)
3. Mathematical foundations
4. Information-theoretic perspectives
5. Game-theoretic applications
6. Computational implementations
7. Specific methodologies
8. Applications across domains
9. Advantages and limitations
10. Integration with cognitive science

This should be highly technical with mathematical formalisms throughout, suitable for someone with graduate degrees in applied mathematics, statistics, and philosophy of science.# Formal Decision Tools

## Definition

**Formal decision tools** are [[mathematical frameworks]], [[computational algorithms]], and [[structured methodologies]] designed to support [[rational decision-making]] by providing systematic approaches to [[alternative evaluation]], [[uncertainty quantification]], [[preference elicitation]], and [[choice optimization]]. These tools transform [[subjective decision problems]] into [[objective mathematical formulations]] that can be analyzed, computed, and validated through [[rigorous analytical methods]].

## Mathematical Foundations

### Decision Problem Formalization

A [[formal decision problem]] is defined as a tuple $\mathcal{D} = (\mathcal{A}, \mathcal{S}, \mathcal{U}, \mathcal{P})$ where:
- $\mathcal{A} = \{a_1, a_2, \ldots, a_n\}$ is the [[action space]]
- $\mathcal{S} = \{s_1, s_2, \ldots, s_m\}$ is the [[state space]]
- $\mathcal{U}: \mathcal{A} \times \mathcal{S} \rightarrow \mathbb{R}$ is the [[utility function]]
- $\mathcal{P}: \mathcal{S} \rightarrow [0,1]$ is the [[probability distribution]] over states

### Expected Utility Maximization

The [[optimal decision]] under uncertainty follows:

$$a^* = \arg\max_{a \in \mathcal{A}} \mathbb{E}_{\mathcal{P}}[\mathcal{U}(a, s)] = \arg\max_{a \in \mathcal{A}} \sum_{s \in \mathcal{S}} \mathcal{P}(s) \mathcal{U}(a, s)$$

### Multi-Objective Optimization

For [[multiple objectives]] $\{f_1, f_2, \ldots, f_k\}$, the [[Pareto frontier]] is:

$$\mathcal{P}_{\text{Pareto}} = \{a \in \mathcal{A} : \nexists a' \in \mathcal{A} \text{ such that } f_i(a') \geq f_i(a) \forall i \text{ and } f_j(a') > f_j(a) \text{ for some } j\}$$

The [[scalarized objective]] combines multiple criteria:

$$\mathcal{F}_{\text{combined}}(a) = \sum_{i=1}^k w_i f_i(a)$$

subject to $\sum_{i=1}^k w_i = 1, w_i \geq 0$.

## Decision Trees

### Tree Structure

A [[decision tree]] $\mathcal{T}$ consists of:
- [[Decision nodes]] $\mathcal{D} = \{d_1, d_2, \ldots\}$ 
- [[Chance nodes]] $\mathcal{C} = \{c_1, c_2, \ldots\}$
- [[Terminal nodes]] $\mathcal{L} = \{l_1, l_2, \ldots\}$
- [[Branches]] $\mathcal{B} \subseteq (\mathcal{D} \cup \mathcal{C}) \times (\mathcal{D} \cup \mathcal{C} \cup \mathcal{L})$

### Backward Induction

The [[expected value]] at each node follows [[backward induction]]:

For chance nodes:
$$V(c) = \sum_{i \in \text{children}(c)} p_i V(i)$$

For decision nodes:
$$V(d) = \max_{i \in \text{children}(d)} V(i)$$

### Decision Tree Complexity

The [[computational complexity]] grows as $O(|\mathcal{A}|^{|\mathcal{D}|} \cdot |\mathcal{S}|^{|\mathcal{C}|})$ for a tree with $|\mathcal{D}|$ decision nodes and $|\mathcal{C}|$ chance nodes.

## Multi-Criteria Decision Analysis (MCDA)

### Analytical Hierarchy Process (AHP)

The [[pairwise comparison matrix]] $\mathbf{A}$ has elements:

$$a_{ij} = \frac{w_i}{w_j}$$

The [[principal eigenvector]] provides weights:

$$\mathbf{A}\mathbf{w} = \lambda_{\max} \mathbf{w}$$

[[Consistency index]]:
$$CI = \frac{\lambda_{\max} - n}{n - 1}$$

[[Consistency ratio]]:
$$CR = \frac{CI}{RI}$$

where $RI$ is the [[random index]].

### TOPSIS Method

The [[Technique for Order Preference by Similarity to Ideal Solution]] uses:

**Positive Ideal Solution:**
$$\mathbf{A}^+ = \{(\max_i a_{ij} | j \in J^+), (\min_i a_{ij} | j \in J^-)\}$$

**Negative Ideal Solution:**
$$\mathbf{A}^- = \{(\min_i a_{ij} | j \in J^+), (\max_i a_{ij} | j \in J^-)\}$$

**Relative Closeness:**
$$RC_i = \frac{d_i^-}{d_i^+ + d_i^-}$$

where $d_i^+$ and $d_i^-$ are [[Euclidean distances]] to ideal solutions.

### ELECTRE Method

The [[outranking relation]] $aSb$ (a outranks b) requires:

**Concordance condition:**
$$C(a,b) = \frac{\sum_{j \in J^+} w_j}{\sum_{j=1}^n w_j} \geq c$$

**Non-discordance condition:**
$$D_j(a,b) = \frac{|v_j(b) - v_j(a)|}{\max\{v_j\} - \min\{v_j\}} \leq d \quad \forall j$$

where $c$ is the [[concordance threshold]] and $d$ is the [[discordance threshold]].

## Utility Theory

### Von Neumann-Morgenstern Axioms

1. **Completeness:** $\forall L_1, L_2: L_1 \succeq L_2 \text{ or } L_2 \succeq L_1$
2. **Transitivity:** $L_1 \succeq L_2 \text{ and } L_2 \succeq L_3 \Rightarrow L_1 \succeq L_3$
3. **Independence:** $L_1 \succeq L_2 \Leftrightarrow \alpha L_1 + (1-\alpha)L_3 \succeq \alpha L_2 + (1-\alpha)L_3$
4. **Continuity:** If $L_1 \succeq L_2 \succeq L_3$, then $\exists \alpha, \beta$ such that $\alpha L_1 + (1-\alpha)L_3 \succeq L_2 \succeq \beta L_1 + (1-\beta)L_3$

### Risk Attitudes

**Risk-averse:** $u''(x) < 0$ (concave utility)
**Risk-neutral:** $u''(x) = 0$ (linear utility)  
**Risk-seeking:** $u''(x) > 0$ (convex utility)

### Arrow-Pratt Risk Aversion

**Absolute risk aversion:**
$$A(x) = -\frac{u''(x)}{u'(x)}$$

**Relative risk aversion:**
$$R(x) = -\frac{xu''(x)}{u'(x)}$$

### Multi-Attribute Utility

For independent attributes, the [[additive form]]:

$$U(x_1, x_2, \ldots, x_n) = \sum_{i=1}^n w_i u_i(x_i)$$

For dependent attributes, the [[multiplicative form]]:

$$1 + kU(\mathbf{x}) = \prod_{i=1}^n [1 + kk_i u_i(x_i)]$$

where $k$ and $k_i$ are [[scaling constants]].

## Game Theory Applications

### Strategic Form Games

A [[strategic game]] $\Gamma = (N, S, u)$ where:
- $N = \{1, 2, \ldots, n\}$ is the set of players
- $S = S_1 \times S_2 \times \cdots \times S_n$ is the [[strategy space]]
- $u = (u_1, u_2, \ldots, u_n)$ are [[payoff functions]]

### Nash Equilibrium

A [[strategy profile]] $s^* \in S$ is a Nash equilibrium if:

$$u_i(s_i^*, s_{-i}^*) \geq u_i(s_i, s_{-i}^*) \quad \forall s_i \in S_i, \forall i \in N$$

### Mixed Strategy Equilibrium

For [[mixed strategies]] $\sigma_i \in \Delta(S_i)$:

$$\sum_{s_i \in S_i} \sigma_i(s_i) u_i(s_i, s_{-i}^*) \geq \sum_{s_i \in S_i} \sigma_i'(s_i) u_i(s_i, s_{-i}^*) \quad \forall \sigma_i' \in \Delta(S_i)$$

### Cooperative Game Theory

The [[Shapley value]] for player $i$:

$$\phi_i(v) = \sum_{S \subseteq N \setminus \{i\}} \frac{|S|!(n-|S|-1)!}{n!} [v(S \cup \{i\}) - v(S)]$$

where $v$ is the [[characteristic function]].

## Optimization Methods

### Linear Programming

Standard form:
$$\begin{align}
\min_{\mathbf{x}} \quad & \mathbf{c}^T \mathbf{x} \\
\text{s.t.} \quad & \mathbf{A}\mathbf{x} = \mathbf{b} \\
& \mathbf{x} \geq \mathbf{0}
\end{align}$$

The [[simplex method]] iterates through [[basic feasible solutions]].

### Integer Programming

For [[binary variables]] $x_i \in \{0,1\}$:

$$\begin{align}
\min_{\mathbf{x}} \quad & \mathbf{c}^T \mathbf{x} \\
\text{s.t.} \quad & \mathbf{A}\mathbf{x} \leq \mathbf{b} \\
& x_i \in \{0,1\} \quad \forall i
\end{align}$$

### Dynamic Programming

The [[Bellman equation]]:

$$V(s) = \max_{a \in \mathcal{A}(s)} \left[R(s,a) + \gamma \sum_{s'} P(s'|s,a) V(s')\right]$$

where $\gamma \in [0,1]$ is the [[discount factor]].

### Stochastic Programming

Two-stage formulation:
$$\min_{x \in X} \left[c^T x + \mathbb{E}_{\xi}[Q(x, \xi)]\right]$$

where $Q(x, \xi) = \min_{y} \{q^T y : Wy = h - Tx, y \geq 0\}$ is the [[recourse function]].

## Markov Decision Processes

### MDP Formulation

An MDP is defined as $(\mathcal{S}, \mathcal{A}, \mathcal{P}, \mathcal{R}, \gamma)$ where:
- $\mathcal{S}$ is the [[state space]]
- $\mathcal{A}$ is the [[action space]]  
- $\mathcal{P}(s'|s,a)$ are [[transition probabilities]]
- $\mathcal{R}(s,a,s')$ is the [[reward function]]
- $\gamma$ is the [[discount factor]]

### Value Functions

**State value function:**
$$V^{\pi}(s) = \mathbb{E}_{\pi}\left[\sum_{t=0}^{\infty} \gamma^t R_{t+1} \mid S_0 = s\right]$$

**Action value function:**
$$Q^{\pi}(s,a) = \mathbb{E}_{\pi}\left[\sum_{t=0}^{\infty} \gamma^t R_{t+1} \mid S_0 = s, A_0 = a\right]$$

### Policy Iteration

**Policy Evaluation:**
$$V^{\pi}_{k+1}(s) = \sum_{a} \pi(a|s) \sum_{s',r} p(s',r|s,a)[r + \gamma V^{\pi}_k(s')]$$

**Policy Improvement:**
$$\pi'(s) = \arg\max_a \sum_{s',r} p(s',r|s,a)[r + \gamma V^{\pi}(s')]$$

## Robust Decision Making

### Minimax Approach

$$a^* = \arg\min_{a \in \mathcal{A}} \max_{s \in \mathcal{S}} L(a, s)$$

where $L(a, s)$ is the [[loss function]].

### Minimax Regret

**Regret matrix:**
$$R(a, s) = \max_{a' \in \mathcal{A}} U(a', s) - U(a, s)$$

**Minimax regret decision:**
$$a^* = \arg\min_{a \in \mathcal{A}} \max_{s \in \mathcal{S}} R(a, s)$$

### Robust Optimization

For [[uncertainty set]] $\mathcal{U}$:

$$\begin{align}
\min_{\mathbf{x}} \quad & \max_{\mathbf{u} \in \mathcal{U}} \mathbf{c}(\mathbf{u})^T \mathbf{x} \\
\text{s.t.} \quad & \mathbf{A}(\mathbf{u})\mathbf{x} \leq \mathbf{b}(\mathbf{u}) \quad \forall \mathbf{u} \in \mathcal{U}
\end{align}$$

## Information-Theoretic Decision Tools

### Value of Information

The [[expected value of perfect information]]:

$$EVPI = \mathbb{E}_s[\max_a U(a,s)] - \max_a \mathbb{E}_s[U(a,s)]$$

### Mutual Information and Decisions

The [[mutual information]] between action and outcome:

$$I(A; S) = \sum_{a,s} p(a,s) \log \frac{p(a,s)}{p(a)p(s)}$$

### Entropy-Based Criteria

**Maximum entropy principle:**
$$p^* = \arg\max_{p \in \mathcal{P}} H(p) = \arg\max_{p \in \mathcal{P}} -\sum_i p_i \log p_i$$

subject to constraints.

### Information Gain

The [[expected information gain]] from experiment $e$:

$$IG(e) = H(S) - \mathbb{E}_{o}[H(S|o)]$$

where $o$ are possible outcomes.

## Computational Implementation

### Algorithmic Complexity

| Method | Time Complexity | Space Complexity |
|--------|----------------|------------------|
| Decision Tree | $O(b^d)$ | $O(bd)$ |
| Linear Programming | $O(n^3)$ | $O(n^2)$ |
| Dynamic Programming | $O(|\mathcal{S}|^2|\mathcal{A}|)$ | $O(|\mathcal{S}|)$ |
| Monte Carlo Tree Search | $O(n \log n)$ per iteration | $O(n)$ |

### Parallel Algorithms

**Map-Reduce for MCDA:**
```
Map: (criterion, alternatives) → (alternative, score)
Reduce: (alternative, scores) → (alternative, weighted_sum)
```

**Distributed MDP Solving:**
$$V_i^{(k+1)}(s) = \max_a \left[R_i(s,a) + \gamma \sum_{j \in \mathcal{N}(i)} \sum_{s'} P_{ij}(s'|s,a) V_j^{(k)}(s')\right]$$

### Machine Learning Integration

**Neural Decision Networks:**
```python
class DecisionNetwork(nn.Module):
    def __init__(self, state_dim, action_dim, hidden_dim):
        super().__init__()
        self.feature_net = nn.Sequential(
            nn.Linear(state_dim, hidden_dim),
            nn.ReLU(),
            nn.Linear(hidden_dim, hidden_dim),
            nn.ReLU()
        )
        self.value_head = nn.Linear(hidden_dim, 1)
        self.policy_head = nn.Linear(hidden_dim, action_dim)
    
    def forward(self, state):
        features = self.feature_net(state)
        value = self.value_head(features)
        policy = F.softmax(self.policy_head(features), dim=-1)
        return value, policy
```

## Uncertainty Quantification

### Probability Elicitation

**Beta Distribution Fitting:**
For expert assessments with mode $m$ and confidence interval $[a, b]$:

$$\alpha = \frac{(m-a)(2m-a-b)}{(b-a)(m-\frac{a+b}{2})} + 1$$

$$\beta = \frac{(b-m)(2m-a-b)}{(b-a)(\frac{a+b}{2}-m)} + 1$$

### Monte Carlo Methods

**Latin Hypercube Sampling:**
For $n$ samples in $d$ dimensions, divide each dimension into $n$ equiprobable intervals and sample once from each.

**Importance Sampling:**
$$\mathbb{E}[f(X)] = \int f(x) p(x) dx = \int f(x) \frac{p(x)}{q(x)} q(x) dx \approx \frac{1}{n} \sum_{i=1}^n f(x_i) w_i$$

where $w_i = \frac{p(x_i)}{q(x_i)}$ are [[importance weights]].

### Sensitivity Analysis

**Morris Method:**
$$\mu_i^* = \frac{1}{r} \sum_{j=1}^r \left|\frac{Y(x_1, \ldots, x_i + \Delta, \ldots, x_k) - Y(x_1, \ldots, x_i, \ldots, x_k)}{\Delta}\right|$$

**Sobol Indices:**
$$S_i = \frac{V_{X_i}(E_{X_{\sim i}}(Y|X_i))}{V(Y)}$$

## Real Options Theory

### Black-Scholes for Real Options

The [[option value]] follows:

$$\frac{\partial V}{\partial t} + \frac{1}{2}\sigma^2 S^2 \frac{\partial^2 V}{\partial S^2} + rS\frac{\partial V}{\partial S} - rV = 0$$

### Binomial Trees

**Risk-neutral probability:**
$$p = \frac{e^{r\Delta t} - d}{u - d}$$

where $u$ and $d$ are [[up and down factors]].

**Option value:**
$$V_0 = e^{-r\Delta t}[pV_u + (1-p)V_d]$$

### American Options

Using [[dynamic programming]]:
$$V(S,t) = \max\{\text{exercise value}, \text{continuation value}\}$$

## Behavioral Integration

### Prospect Theory Integration

**Value function:**
$$v(x) = \begin{cases}
x^{\alpha} & \text{if } x \geq 0 \\
-\lambda(-x)^{\beta} & \text{if } x < 0
\end{cases}$$

**Decision weights:**
$$w^+(p) = \frac{p^{\gamma}}{(p^{\gamma} + (1-p)^{\gamma})^{1/\gamma}}$$

### Bounded Rationality

**Satisficing with [[aspiration levels]]:**
$$a^* = \arg\max_{a \in \mathcal{A}} \mathbf{1}[U(a) \geq \bar{U}] \cdot \text{simplicity}(a)$$

**Limited search:**
$$\mathcal{A}_{\text{considered}} = \{a \in \mathcal{A} : \text{search cost}(a) \leq \text{budget}\}$$

### Cognitive Load Adaptation

**Effort-accuracy tradeoff:**
$$\text{Strategy}^* = \arg\max_s \left[\text{accuracy}(s) - \kappa \cdot \text{effort}(s)\right]$$

## Multi-Stakeholder Decisions

### Social Choice Theory

**Arrow's Impossibility Theorem conditions:**
1. Universal domain
2. Pareto efficiency  
3. Independence of irrelevant alternatives
4. Non-dictatorship

**Condorcet winner:**
Alternative $a$ is a Condorcet winner if $a$ defeats every other alternative in pairwise majority comparison.

### Mechanism Design

**Revelation principle:** Any mechanism can be replaced by an equivalent [[direct mechanism]] that is [[incentive compatible]].

**Vickrey-Clarke-Groves mechanism:**
$$t_i(\mathbf{v}) = \sum_{j \neq i} v_j(a^*(\mathbf{v}_{-i})) - \sum_{j \neq i} v_j(a^*(\mathbf{v}))$$

### Voting Systems

**Borda count:**
$$\text{Score}(a) = \sum_{i} (n - \text{rank}_i(a))$$

**Approval voting:**
Each voter approves any subset of candidates; candidate with most approvals wins.

## Applications

### Financial Portfolio Optimization

**Markowitz Model:**
$$\begin{align}
\min_{\mathbf{w}} \quad & \mathbf{w}^T \boldsymbol{\Sigma} \mathbf{w} \\
\text{s.t.} \quad & \boldsymbol{\mu}^T \mathbf{w} \geq \mu_{\min} \\
& \mathbf{1}^T \mathbf{w} = 1 \\
& \mathbf{w} \geq \mathbf{0}
\end{align}$$

**Black-Litterman Model:**
$$\boldsymbol{\mu}_{\text{BL}} = \left[(\tau\boldsymbol{\Sigma})^{-1} + \mathbf{P}^T\boldsymbol{\Omega}^{-1}\mathbf{P}\right]^{-1}\left[(\tau\boldsymbol{\Sigma})^{-1}\boldsymbol{\pi} + \mathbf{P}^T\boldsymbol{\Omega}^{-1}\mathbf{Q}\right]$$

### Healthcare Decision Analysis

**Quality-Adjusted Life Years:**
$$\text{QALY} = \sum_{t=0}^T (1+r)^{-t} Q_t L_t$$

**Cost-effectiveness ratio:**
$$\text{ICER} = \frac{\Delta \text{Cost}}{\Delta \text{Effectiveness}}$$

### Supply Chain Optimization

**Newsvendor problem:**
$$Q^* = F^{-1}\left(\frac{p - c}{p - v}\right)$$

where $p$ is price, $c$ is cost, $v$ is salvage value.

**Bullwhip effect mitigation:**
$$\text{Order}_t = \alpha \cdot \text{Demand}_t + (1-\alpha) \cdot \text{Order}_{t-1}$$

### Environmental Decision Making

**Cost-benefit analysis with discounting:**
$$\text{NPV} = \sum_{t=0}^T \frac{B_t - C_t}{(1+r)^t}$$

**Real options for climate policy:**
$$V_{\text{wait}} = \max\{V_{\text{invest now}}, e^{-r\Delta t} \mathbb{E}[V_{\text{next period}}]\}$$

## Software Tools and Platforms

### Decision Analysis Software

**TreeAge Pro:** Decision tree construction and analysis
**@RISK:** Monte Carlo simulation for Excel
**Crystal Ball:** Predictive modeling and optimization
**Expert Choice:** AHP-based decision support

### Programming Libraries

**Python:**
```python
# scikit-learn for ML-based decisions
from sklearn.ensemble import RandomForestClassifier
from sklearn.model_selection import cross_val_score

# PuLP for optimization
from pulp import *
prob = LpProblem("Decision", LpMaximize)

# NetworkX for game theory
import networkx as nx
G = nx.Graph()
```

**R:**
```r
# AHP package
library(ahp)
ahp_model <- Load("decision_hierarchy.yml")

# MCMCpack for Bayesian decisions  
library(MCMCpack)
posterior <- MCMCregress(y ~ x, data=data)
```

**MATLAB:**
```matlab
% Optimization Toolbox
options = optimoptions('linprog','Display','iter');
[x,fval] = linprog(f,A,b,Aeq,beq,lb,ub,options);

% Statistics and Machine Learning Toolbox
tree = fitctree(X,Y,'OptimizeHyperparameters','auto');
```

## Validation and Testing

### Cross-Validation

**k-fold validation for decision models:**
$$\text{CV Error} = \frac{1}{k} \sum_{i=1}^k L(\hat{f}^{(-i)}, D_i)$$

### Backtesting

**Sharpe ratio for portfolio decisions:**
$$SR = \frac{\mathbb{E}[R_p] - R_f}{\sigma(R_p)}$$

**Maximum drawdown:**
$$MDD = \max_{t \in [0,T]} \left[\max_{\tau \in [0,t]} X(\tau) - X(t)\right]$$

### Stress Testing

**Value-at-Risk:**
$$\text{VaR}_{\alpha} = \inf\{x : P(L \leq x) \geq \alpha\}$$

**Expected Shortfall:**
$$\text{ES}_{\alpha} = \mathbb{E}[L | L \geq \text{VaR}_{\alpha}]$$

## Limitations and Challenges

### Computational Complexity

Many formal decision problems are [[NP-hard]]:
- Multi-objective optimization with constraints
- Stochastic programming with integer variables
- Game-theoretic equilibrium computation

### Model Uncertainty

**Ambiguity aversion:**
$$\min_{\pi \in \Pi} \mathbb{E}_{\pi}[u(a, s)]$$

where $\Pi$ is a set of probability distributions.

### Dynamic Inconsistency

**Hyperbolic discounting:**
$$U_t = u(c_t) + \beta \sum_{s=t+1}^T \delta^{s-t} u(c_s)$$

leads to time-inconsistent preferences.

### Preference Elicitation

**Cognitive burden of preference assessment:**
- Number of pairwise comparisons: $\binom{n}{2}$
- Transitivity checking complexity: $O(n^3)$
- Consistency maintenance overhead

## Related Concepts

- [[Expected Utility Theory]]
- [[Game Theory]]
- [[Optimization Theory]]
- [[Decision Trees]]
- [[Multi-Criteria Decision Analysis]]
- [[Behavioral Economics]]
- [[Risk Management]]
- [[Operations Research]]
- [[Systems Analysis]]
- [[Computational Intelligence]]
- [[Cognitive Biases]]
- [[Availability Heuristic]]
- [[Bounded Rationality]]
- [[Satisficing]]
- [[Prospect Theory]]
- [[Social Choice Theory]]


---

preceding: [[availability heuristic]]  


following: [[multi-criteria decision analysis]]

See also: [[decision analysis]]


See also: [[decision theory]]


See also: [[game theory]]


See also: [[utility theory]]