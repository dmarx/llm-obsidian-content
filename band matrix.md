---
date: 1717827084.151371
tags:
- autogenerated
- source__oai
- conversation_id__ac8c3253-a669-4989-bf7c-c535d52c8140
- msg_aaa296cf-7069-4a7a-86b8-313daad9a6df
- msg_471e0951-54c0-4429-9dc0-87a4745f6c03
- colorclass/vector-space
title: band matrix
---

**USER**

Band Matrix


**ASSISTANT**

A band matrix is a sparse matrix whose non-zero elements are confined to a diagonal band, comprising the main diagonal and a certain number of diagonals on either side. This structure significantly reduces the number of non-zero elements, which leads to more efficient storage and computational operations.

## Definition

A matrix \( A \in \mathbb{R}^{n \times n} \) is called a band matrix if its elements satisfy:

$$
a_{ij} = 0 \quad \text{if} \quad |i - j| > k
$$

where \( k \) is a non-negative integer known as the bandwidth. The bandwidth \( k \) defines the width of the band around the main diagonal where non-zero elements are allowed.

### Lower and Upper Bandwidths

- **Lower Bandwidth \( l \)**: The number of subdiagonals containing non-zero elements.
- **Upper Bandwidth \( u \)**: The number of superdiagonals containing non-zero elements.

For a general band matrix, we can define it with both lower and upper bandwidths. A matrix with lower bandwidth \( l \) and upper bandwidth \( u \) has non-zero elements \( a_{ij} \) only if:

$$
i - l \leq j \leq i + u
$$

### Special Cases

1. **Diagonal Matrix**: A band matrix with \( l = 0 \) and \( u = 0 \).
2. **Tridiagonal Matrix**: A band matrix with \( l = 1 \) and \( u = 1 \).
3. **Pentadiagonal Matrix**: A band matrix with \( l = 2 \) and \( u = 2 \).

### Example

A band matrix with \( l = 2 \) and \( u = 1 \) (two subdiagonals and one superdiagonal):

$$
A = \begin{pmatrix}
a_{11} & a_{12} & 0 & 0 & 0 \\
a_{21} & a_{22} & a_{23} & 0 & 0 \\
a_{31} & a_{32} & a_{33} & a_{34} & 0 \\
a_{41} & a_{42} & a_{43} & a_{44} & a_{45} \\
0 & a_{52} & a_{53} & a_{54} & a_{55}
\end{pmatrix}
$$

## Storage and Efficiency

### Compact Storage

Due to the large number of zeros outside the band, band matrices are typically stored using a more compact format to save space and improve computational efficiency. One common storage scheme is to store only the diagonals of the matrix in a two-dimensional array:

- A band matrix with lower bandwidth \( l \) and upper bandwidth \( u \) is stored in an array of dimensions \((l + u + 1) \times n\).

### Computational Efficiency

1. **Matrix-Vector Multiplication**:
   - The multiplication of a band matrix \( A \) with a vector \( x \) can be performed in \( O(n(l + u)) \) operations, compared to \( O(n^2) \) for a general dense matrix.
   
2. **Solving Linear Systems**:
   - Direct methods like Gaussian elimination can be adapted to exploit the band structure, reducing the complexity significantly. For instance, the LU decomposition of a band matrix can be performed in \( O(n(l + u)^2) \) operations.

### Applications

1. **Finite Difference Methods**: In numerical solutions of partial differential equations, discretizing the equations often leads to band matrices. For example, the discretization of the Laplace equation on a grid results in a tridiagonal or pentadiagonal matrix.
2. **Structural Engineering**: Stiffness matrices in finite element analysis often have a band structure, reflecting local interactions between elements.
3. **Quantum Mechanics**: Hamiltonian matrices in quantum mechanics frequently exhibit band structures, especially in models with local interactions.

## Example: Solving a Band Matrix System

Consider solving a linear system \( Ax = b \) where \( A \) is a tridiagonal matrix (a special case of a band matrix with \( l = 1 \) and \( u = 1 \)):

$$
A = \begin{pmatrix}
2 & -1 & 0 & 0 \\
-1 & 2 & -1 & 0 \\
0 & -1 & 2 & -1 \\
0 & 0 & -1 & 2
\end{pmatrix}, \quad b = \begin{pmatrix} 1 \\ 2 \\ 2 \\ 1 \end{pmatrix}
$$

### Thomas Algorithm (Tridiagonal Matrix Algorithm)

1. **Forward Elimination**:

   For \( i = 1 \) to \( n-1 \):

   $$ c_i = \frac{a_{i,i+1}}{a_{i,i}} $$
   $$ a_{i+1,i+1} = a_{i+1,i+1} - c_i a_{i+1,i} $$
   $$ b_{i+1} = b_{i+1} - c_i b_i $$

   Steps:

   - \( c_1 = \frac{-1}{2} = -0.5 \)
   - Update: \( a_{22} = 2 - (-0.5)(-1) = 1.5 \), \( b_2 = 2 - (-0.5)(1) = 1.5 \)
   - \( c_2 = \frac{-1}{1.5} \approx -0.6667 \)
   - Update: \( a_{33} = 2 - (-0.6667)(-1) \approx 1.3333 \), \( b_3 = 2 - (-0.6667)(1.5) \approx 1.0 \)
   - \( c_3 = \frac{-1}{1.3333} \approx -0.75 \)
   - Update: \( a_{44} = 2 - (-0.75)(-1) = 1.25 \), \( b_4 = 1 - (-0.75)(1) = 0.25 \)

2. **Back Substitution**:

   Solve \( x_n \) and then \( x_i \) for \( i = n-1, n-2, \ldots, 1 \):

   $$ x_n = \frac{b_n}{a_{n,n}} $$
   $$ x_i = \frac{b_i - a_{i,i+1} x_{i+1}}{a_{i,i}} $$

   Steps:

   - \( x_4 = \frac{0.25}{1.25} = 0.2 \)
   - \( x_3 = \frac{1 - (-1)(0.2)}{1.3333} \approx 0.9 \)
   - \( x_2 = \frac{1.5 - (-1)(0.9)}{1.5} = 1.6 \)
   - \( x_1 = \frac{1 - (-1)(1.6)}{2} = 1.3 \)

   Solution: \( x = \begin{pmatrix} 1.3 \\ 1.6 \\ 0.9 \\ 0.2 \end{pmatrix} \)

## Related Topics

- [[Sparse Matrix]]: General concept of matrices with mostly zero elements.
- [[Tridiagonal Matrix]]: Special case of band matrices with \( l = 1 \) and \( u = 1 \).
- [[Pentadiagonal Matrix]]: Band matrix with \( l = 2 \) and \( u = 2 \).
- [[Thomas Algorithm]]: Efficient algorithm for solving tridiagonal systems.

Band matrices are fundamental in numerical analysis, providing a balance between computational efficiency and storage requirements, making them essential for solving large-scale scientific and engineering problems.


---

preceding: [[tridiagonal structure]]  


following: [[spectral properties of band matrices]]

See also: [[partial differential equations]]