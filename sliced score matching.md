---
date: '2025-06-15T21:05:08.495603Z'
tags:
- autogenerated
- source__claude
- conversation_id__bb2ab335-44e5-43a4-9a65-dc690439b655
- msg_a4aa7a22-4821-4112-ad65-58dc7adbd25b
- msg_65d601c7-1e56-46d1-b011-f19530f85e85
- colorclass/information-theory
title: sliced score matching
---

**USER**

sliced score matching


**ASSISTANT**

The user is asking for a dedicated entry on sliced score matching. In my previous response, I covered sliced score matching as a subsection within the broader score matching article, but now they want a focused, detailed treatment of just sliced score matching. This should be a standalone wiki entry that goes much deeper into the mathematical details, computational aspects, and theoretical properties of sliced score matching specifically.

I should provide:
1. The motivation for sliced score matching (computational efficiency)
2. The mathematical foundation and derivation
3. The connection to random projections and the Hutchinson trace estimator
4. Theoretical properties
5. Practical implementation details
6. Variants and extensions
7. Applications

I need to be very careful with the mathematical notation and make sure all symbols are properly defined. I should also establish clear symbolic notation for this specific method.# Sliced Score Matching

[[Sliced score matching]] (SSM) is a computationally efficient variant of [[score matching]] that reduces the complexity of score function estimation by leveraging [[random projections]]. The method circumvents the expensive computation of the [[divergence]] term in [[explicit score matching]] while maintaining theoretical guarantees.

## Mathematical Foundation

### Motivation and Complexity Analysis

The [[explicit score matching]] objective requires computing the divergence $\nabla_x \cdot s_\theta(x) = \sum_{i=1}^d \frac{\partial s_{\theta,i}(x)}{\partial x_i}$, which involves $d$ second-order derivatives when $s_\theta$ is a neural network. For high-dimensional problems, this becomes computationally prohibitive, scaling as $O(d \cdot \text{complexity}(s_\theta))$.

Sliced score matching reduces this to $O(\text{complexity}(s_\theta))$ by projecting the score matching problem onto random one-dimensional subspaces.

### Random Projection Framework

The core insight is to replace the full-dimensional score matching with a collection of one-dimensional problems. For a random unit vector $v \in \mathbb{S}^{d-1}$ drawn from some distribution $p_v$, consider the [[projected score]]:

$$s_v(x) = v^T s(x) = v^T \nabla_x \log p(x)$$

where $s(x) = \nabla_x \log p(x)$ is the true score function.

### One-Dimensional Score Matching Identity

For the one-dimensional projection, we can derive a score matching identity. The [[projected Fisher divergence]] is:

$$\mathcal{J}_v[s_\theta, s] = \frac{1}{2} \mathbb{E}_{p(x)}\left[(v^T s_\theta(x) - v^T s(x))^2\right]$$

Expanding this expression:

$$\mathcal{J}_v[s_\theta, s] = \frac{1}{2} \mathbb{E}_{p(x)}\left[(v^T s_\theta(x))^2\right] - \mathbb{E}_{p(x)}\left[v^T s_\theta(x) \cdot v^T s(x)\right] + \frac{1}{2} \mathbb{E}_{p(x)}\left[(v^T s(x))^2\right]$$

The cross-term can be transformed using [[integration by parts]]. Since $v^T s(x) = v^T \nabla_x \log p(x)$:

$$\mathbb{E}_{p(x)}\left[v^T s_\theta(x) \cdot v^T \nabla_x \log p(x)\right] = \int v^T s_\theta(x) \cdot v^T \frac{\nabla_x p(x)}{p(x)} \cdot p(x) dx$$

$$= \int v^T s_\theta(x) \cdot v^T \nabla_x p(x) dx$$

Applying integration by parts to this integral:

$$= -\int v^T \nabla_x (v^T s_\theta(x)) \cdot p(x) dx = -\mathbb{E}_{p(x)}\left[v^T \nabla_x (v^T s_\theta(x))\right]$$

### Sliced Score Matching Objective

The [[sliced score matching objective]] for a single projection direction $v$ is:

$$\mathcal{L}_{\text{SSM},v}(\theta) = \frac{1}{2} \mathbb{E}_{p(x)}\left[(v^T s_\theta(x))^2\right] + \mathbb{E}_{p(x)}\left[v^T \nabla_x (v^T s_\theta(x))\right]$$

To obtain a robust estimator, we average over multiple random projections:

$$\mathcal{L}_{\text{SSM}}(\theta) = \mathbb{E}_{v \sim p_v}\left[\mathcal{L}_{\text{SSM},v}(\theta)\right]$$

$$= \mathbb{E}_{v \sim p_v, x \sim p(x)}\left[\frac{1}{2}(v^T s_\theta(x))^2 + v^T \nabla_x (v^T s_\theta(x))\right]$$

## Hutchinson Trace Estimator

### Directional Derivative Computation

The term $v^T \nabla_x (v^T s_\theta(x))$ represents a [[directional second derivative]]. Let $f(x) = v^T s_\theta(x)$, then:

$$v^T \nabla_x (v^T s_\theta(x)) = v^T \nabla_x f(x) = \sum_{i=1}^d v_i \frac{\partial f(x)}{\partial x_i}$$

This can be written in terms of the [[Hessian]] of a scalar function. Define:

$$h_v(x) = \frac{1}{2}(v^T s_\theta(x))^2$$

Then:
$$v^T \nabla_x (v^T s_\theta(x)) = v^T H_{h_v}(x) v$$

where $H_{h_v}(x)$ is the Hessian of $h_v$ with respect to $x$.

### Hutchinson's Method

The [[Hutchinson trace estimator]] provides an unbiased estimate of the trace of a matrix using random vectors. For a matrix $A$ and random vector $v$ with $\mathbb{E}[vv^T] = I$:

$$\text{tr}(A) = \mathbb{E}[v^T A v]$$

In our context, we use this to estimate $v^T H_{h_v}(x) v$ efficiently using [[automatic differentiation]].

### Implementation via Automatic Differentiation

The directional derivative can be computed using [[forward-mode automatic differentiation]]:

```python
def sliced_score_matching_loss(score_fn, x, v):
    # Compute v^T s_theta(x)
    score = score_fn(x)
    projected_score = torch.sum(v * score, dim=-1)
    
    # Compute v^T âˆ‡_x (v^T s_theta(x)) using forward-mode AD
    directional_derivative = torch.autograd.grad(
        projected_score, x, 
        grad_outputs=torch.ones_like(projected_score),
        create_graph=True
    )[0]
    
    hvp = torch.sum(v * directional_derivative, dim=-1)
    
    return 0.5 * projected_score**2 + hvp
```

## Theoretical Properties

### Consistency

Under appropriate regularity conditions, the sliced score matching estimator is [[Fisher consistent]]. As the number of projections $K \to \infty$ and sample size $N \to \infty$:

$$\lim_{K,N \to \infty} \arg\min_\theta \hat{\mathcal{L}}_{\text{SSM}}(\theta) = \arg\min_\theta \mathcal{J}[s_\theta, s_{\text{true}}]$$

### Convergence Analysis

The convergence rate depends on both the number of data points $N$ and the number of projection directions $K$. For a finite number of projections $K$, there is an additional [[projection error]]:

$$\mathbb{E}[\mathcal{J}[s_{\hat{\theta}}, s_{\text{true}}]] \leq \underbrace{O(N^{-\alpha})}_{\text{estimation error}} + \underbrace{O(K^{-\beta})}_{\text{projection error}}$$

where $\alpha$ and $\beta$ depend on the smoothness and dimensionality of the problem.

### Projection Distribution Choice

The choice of projection distribution $p_v$ affects the convergence properties. Common choices include:

1. **Uniform on the sphere**: $v \sim \text{Uniform}(\mathbb{S}^{d-1})$
2. **Gaussian projections**: $v \sim \mathcal{N}(0, I)/\|\mathcal{N}(0, I)\|$
3. **Rademacher projections**: $v_i \sim \text{Uniform}(\{-1, +1\})$

The [[spherical symmetry]] property ensures that all directions are treated equally.

## Symbolic Notation Framework

**Projection Operators:**
- $\Pi_v: \mathbb{R}^d \to \mathbb{R}$, $(\Pi_v s)(x) = v^T s(x)$
- $\mathcal{P}_v = \{g: \mathbb{R}^d \to \mathbb{R} \mid g(x) = v^T f(x), f: \mathbb{R}^d \to \mathbb{R}^d\}$

**Directional Derivatives:**
- $D_v f(x) = \lim_{h \to 0} \frac{f(x + hv) - f(x)}{h} = v^T \nabla_x f(x)$
- $D_v^2 f(x) = v^T H_f(x) v$ where $H_f$ is the Hessian

**Random Projection Measures:**
- $\mu_{\mathbb{S}^{d-1}}$: Uniform measure on the unit sphere
- $\mathbb{E}_v[\cdot] = \int_{\mathbb{S}^{d-1}} (\cdot) d\mu_{\mathbb{S}^{d-1}}(v)$

**Sliced Divergences:**
- $D_{\text{sliced}}(s_1, s_2) = \mathbb{E}_v[D_{\text{Fisher}}(\Pi_v s_1, \Pi_v s_2)]$
- $\mathcal{J}_{\text{sliced}}[s_\theta, s] = \mathbb{E}_v[\mathcal{J}_v[s_\theta, s]]$

## Practical Considerations

### Computational Complexity

For a neural network score function with $P$ parameters:
- **Explicit score matching**: $O(P + d \cdot P)$ per sample
- **Sliced score matching**: $O(P)$ per sample (independent of $d$)

This makes SSM particularly attractive for high-dimensional problems.

### Number of Projections

The number of projection directions $K$ involves a bias-variance tradeoff:
- **Small $K$**: Lower computational cost but higher projection bias
- **Large $K$**: Better approximation but higher variance

Typical choices are $K = 1$ (most efficient) or $K = O(\log d)$ (better accuracy).

### Mini-batch Implementation

For mini-batch training with batch size $B$ and $K$ projections per sample:

```python
def sliced_score_matching_batch(score_fn, x_batch, num_projections=1):
    B, d = x_batch.shape
    losses = []
    
    for _ in range(num_projections):
        # Sample projection directions
        v = torch.randn(B, d)
        v = v / torch.norm(v, dim=1, keepdim=True)
        
        # Compute sliced loss
        loss = sliced_score_matching_loss(score_fn, x_batch, v)
        losses.append(loss)
    
    return torch.mean(torch.stack(losses))
```

## Extensions and Variants

### Multi-Scale Sliced Score Matching

Combining sliced score matching with [[denoising score matching]]:

$$\mathcal{L}_{\text{multi-scale}}(\theta) = \sum_{l=1}^L \lambda_l \mathbb{E}_{v, x_0, \epsilon} \left[\mathcal{L}_{\text{SSM},v}(s_\theta(x_0 + \sigma_l \epsilon, \sigma_l))\right]$$

### Adaptive Projection Selection

Rather than using random projections, some methods learn optimal projection directions:

$$\mathcal{L}_{\text{adaptive}}(\theta, \phi) = \mathbb{E}_{x \sim p(x)} \left[\mathcal{L}_{\text{SSM},v_\phi(x)}(\theta)\right]$$

where $v_\phi(x)$ is a parameterized projection direction.

### Stein Variational Gradient Descent Connection

Sliced score matching connects to [[Stein variational gradient descent]] through the [[Stein operator]]. The sliced Stein discrepancy is:

$$\mathcal{S}_{\text{sliced}}(q, p) = \mathbb{E}_{v \sim p_v} \left[\mathbb{E}_{x \sim q} [v^T \phi(x)]^2\right]$$

where $\phi(x) = \nabla_x \log p(x) + \nabla_x \log q(x)$.

## Applications

### High-Dimensional Density Estimation

Sliced score matching is particularly effective for:
- **Image modeling**: Natural images in $\mathbb{R}^{d}$ where $d \sim 10^4 - 10^6$
- **Point cloud processing**: 3D geometric data
- **Molecular dynamics**: High-dimensional configuration spaces

### Computational Biology

Applications in [[computational biology]] include:
- **Protein folding**: Score-based sampling of conformational space
- **Genomics**: Modeling high-dimensional genetic variation
- **Neuroscience**: Analyzing high-dimensional neural activity patterns

The method's computational efficiency makes it a preferred choice when explicit score matching becomes prohibitively expensive, while maintaining the theoretical advantages of score-based approaches over likelihood-based methods.This is a comprehensive entry on sliced score matching. I've covered:

1. Mathematical foundation and motivation
2. The derivation of the sliced score matching objective
3. Connection to Hutchinson trace estimator
4. Theoretical properties
5. Symbolic notation framework
6. Practical implementation details
7. Extensions and variants
8. Applications

I made sure to:
- Clearly define all notation
- Provide step-by-step mathematical derivations
- Include practical implementation considerations
- Connect to broader theoretical frameworks
- Use appropriate wiki-style linking with double brackets

The user specifically asked about sliced score matching after I had covered it briefly in the broader score matching article, so this provides a much more detailed and focused treatment.


---

preceding: [[whats $tilde{p}$ i think you may have introduced some notation without clearly explaining it.]]  


following: [[momentum as information]]

See also: [[bias-variance tradeoff]]


See also: [[projection bias]]


See also: [[score matching]]