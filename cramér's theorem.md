---
date: 1721688120.881039
tags:
- autogenerated
- source__oai
- conversation_id__a2d4a2dc-4888-4b9b-aa34-ef6394b04fb7
- msg_aaa240b4-9289-4097-aceb-36ad34d5d93e
- msg_07ce7195-dc13-452d-a764-2dfc46693d51
- colorclass/information-theory
title: cramér's theorem
---

**USER**

Cramér's Theorem


**ASSISTANT**

## Cramér's Theorem

### Overview

Cramér's theorem is a foundational result in [[large deviations theory]] that describes the asymptotic behavior of the probability of sums of independent and identically distributed (i.i.d.) random variables deviating from their expected value. It provides a precise quantification of how these probabilities decay exponentially with the sample size.

### Statement of the Theorem

Consider a sequence of i.i.d. random variables \(\{X_i\}\) with common moment generating function \(M(\lambda) = \mathbb{E}[e^{\lambda X_i}]\). Let \(S_n = \sum_{i=1}^n X_i\) be the sum of these random variables, and \(\overline{X}_n = \frac{S_n}{n}\) be the sample mean. 

#### Large Deviations Principle

Cramér's theorem states that the sequence of sample means \(\{\overline{X}_n\}\) satisfies a Large Deviations Principle (LDP) with rate function \(I(x)\):

$$
I(x) = \sup_{\lambda \in \mathbb{R}} \left( \lambda x - \log M(\lambda) \right),
$$

where \(\log M(\lambda)\) is the cumulant generating function of the random variables.

#### Formal Statement

For any Borel set \(A \subset \mathbb{R}\),

1. **Lower Bound**:
    $$
    \liminf_{n \to \infty} \frac{1}{n} \log \Pr(\overline{X}_n \in A) \geq - \inf_{x \in A^\circ} I(x),
    $$

2. **Upper Bound**:
    $$
    \limsup_{n \to \infty} \frac{1}{n} \log \Pr(\overline{X}_n \in A) \leq - \inf_{x \in \overline{A}} I(x),
    $$

where \(A^\circ\) is the interior of \(A\) and \(\overline{A}\) is the closure of \(A\).

### Intuition

Cramér's theorem provides a way to understand the probabilities of large deviations of the sample mean from the expected value. The rate function \(I(x)\) captures the exponential rate at which these probabilities decay. Specifically, for large \(n\), the probability that the sample mean \(\overline{X}_n\) is close to a value \(x\) can be approximated by:

$$
\Pr(\overline{X}_n \approx x) \sim \exp(-n I(x)),
```

### Derivation of the Rate Function

The rate function \(I(x)\) is derived from the cumulant generating function \(\log M(\lambda)\) via the Legendre-Fenchel transform:

$$
I(x) = \sup_{\lambda \in \mathbb{R}} \left( \lambda x - \log M(\lambda) \right).
```

This transform provides a dual representation of the rate function in terms of the exponential growth rates of the moment generating function.

### Examples

#### Example 1: Gaussian Distribution

Consider i.i.d. random variables \(\{X_i\}\) with a Gaussian distribution \(N(\mu, \sigma^2)\). The moment generating function is:

$$
M(\lambda) = \exp \left( \mu \lambda + \frac{1}{2} \sigma^2 \lambda^2 \right).
```

The cumulant generating function is:

$$
\log M(\lambda) = \mu \lambda + \frac{1}{2} \sigma^2 \lambda^2.
```

The rate function \(I(x)\) is obtained by the Legendre-Fenchel transform:

$$
I(x) = \sup_{\lambda \in \mathbb{R}} \left( \lambda x - \left( \mu \lambda + \frac{1}{2} \sigma^2 \lambda^2 \right) \right).
```

Taking the derivative with respect to \(\lambda\) and setting it to zero,

$$
x = \mu + \sigma^2 \lambda \implies \lambda = \frac{x - \mu}{\sigma^2}.
```

Substituting back,

$$
I(x) = \left( \frac{x - \mu}{\sigma^2} \right) x - \left( \mu \left( \frac{x - \mu}{\sigma^2} \right) + \frac{1}{2} \sigma^2 \left( \frac{x - \mu}{\sigma^2} \right)^2 \right),
$$

$$
I(x) = \frac{(x - \mu)^2}{2 \sigma^2}.
```

Thus, the rate function for a Gaussian distribution is quadratic.

#### Example 2: Exponential Distribution

Consider i.i.d. random variables \(\{X_i\}\) with an exponential distribution with rate parameter \(\lambda\). The moment generating function is:

$$
M(\theta) = \frac{\lambda}{\lambda - \theta}, \quad \text{for} \ \theta < \lambda.
```

The cumulant generating function is:

$$
\log M(\theta) = - \log \left( 1 - \frac{\theta}{\lambda} \right).
```

The rate function \(I(x)\) is obtained by the Legendre-Fenchel transform:

$$
I(x) = \sup_{\theta < \lambda} \left( \theta x + \log \left( 1 - \frac{\theta}{\lambda} \right) \right).
```

Taking the derivative with respect to \(\theta\) and setting it to zero,

$$
x = \frac{1}{\lambda - \theta} \implies \theta = \lambda - \frac{1}{x}.
```

Substituting back,

$$
I(x) = \left( \lambda - \frac{1}{x} \right) x - \log \left( \frac{x - 1/\lambda}{x} \right),
$$

$$
I(x) = 1 - \log(\lambda x) - 1 + \frac{1}{\lambda x},
$$

$$
I(x) = \lambda x - 1 - \log(\lambda x).
```

Thus, the rate function for an exponential distribution involves both linear and logarithmic terms.

### Applications

#### Statistical Physics

In statistical mechanics, Cramér's theorem helps to study the distribution of macroscopic quantities like energy and magnetization in large systems. It provides a way to understand the likelihood of large deviations from equilibrium.

#### Information Theory

In information theory, Cramér's theorem is used to analyze the probabilities of rare events in communication systems. It helps to derive bounds on error probabilities and evaluate the performance of coding schemes.

#### Finance

In finance, Cramér's theorem is applied to assess the risk of extreme events, such as large losses or gains in financial portfolios. It provides a theoretical basis for estimating the probabilities of rare but impactful financial outcomes.

### Further Reading

- [[Large Deviations Theory]]
- [[Gartner-Ellis Theorem]]
- [[Sanov's Theorem]]
- [[Varadhan's Lemma]]
- [[Legendre-Fenchel Transform]]
- [[Relative Entropy]]
- [[Optimization Problems in Spin Glasses]]

Cramér's theorem is a cornerstone of [[large deviations theory]], offering a rigorous framework for understanding the probabilities of rare events in complex systems. For a deeper exploration, refer to specialized texts in probability theory, statistical mechanics, and information theory.


---

preceding: [[duality theory]]  


following: [[lagrangian duality]]

See also: [[probability theory]]


See also: [[information theory]]


See also: [[independent and identically distributed]]


See also: [[moment generating function]]


See also: [[generating function]]


See also: [[random variables]]