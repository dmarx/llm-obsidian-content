---
date: 1726520973.65755
tags:
- autogenerated
- source__oai
- conversation_id__66e89e8d-53f4-8008-9244-5cb47b71732f
- msg_aaa2faf3-478a-4c50-b797-87adc2668083
- msg_27ab53c7-d515-4096-b01d-ebdde0a04424
- colorclass/game-theory
title: differential privacy
---

**USER**

differential privacy


**ASSISTANT**

**Differential privacy (DP)** is a rigorous framework designed to ensure that statistical analyses or machine learning models provide insights about a population without revealing sensitive information about any individual in the dataset. The core idea is that the inclusion or exclusion of a single data point (i.e., one individual's data) should not significantly affect the outcome of the analysis.

### Formal Definition

At its core, differential privacy relies on the concept of indistinguishability. Given two neighboring datasets, $D$ and $D'$, which differ by at most one individual’s data, a mechanism $\mathcal{M}$ is said to be $(\epsilon, \delta)$-differentially private if for all measurable subsets $S$ of the output space:

$$
\Pr[\mathcal{M}(D) \in S] \leq e^{\epsilon} \Pr[\mathcal{M}(D') \in S] + \delta
$$

Here, the parameters $\epsilon$ and $\delta$ control the strength of privacy:
- $\epsilon$ (the **privacy loss parameter**) is typically a small positive number that limits how much the probabilities can differ between neighboring datasets. Smaller $\epsilon$ means stronger privacy guarantees.
- $\delta$ allows for a small probability that the mechanism might violate the $\epsilon$-bound, and is often set to be close to zero (or exactly zero in the case of **pure differential privacy**).

If $\delta = 0$, we have **pure differential privacy**. If $\delta > 0$, it is called **approximate differential privacy**.

### Intuition Behind Differential Privacy

The goal is to ensure that the presence or absence of any single individual in the dataset does not significantly affect the results. Thus, an observer seeing the outcome of $\mathcal{M}$ cannot infer whether any individual’s data was used. 

More concretely, if two datasets differ by only one record (say, an individual's data), the outputs of the differentially private mechanism on both datasets will be similar, making it difficult to distinguish which dataset was used.

### Sensitivity and Noise Addition

The key challenge in differential privacy is how to balance utility (usefulness of the output) with privacy. The standard approach involves adding noise to the results of a query or analysis. The amount of noise depends on the **sensitivity** of the query, which is a measure of how much a single individual's data can affect the result.

- **Global sensitivity** is the maximum change in the query's output when one individual's data is added or removed from the dataset. For a function $f: D \to \mathbb{R}^k$, its global sensitivity is:
  $$
  \Delta f = \max_{D, D'} \| f(D) - f(D') \|_1
  $$
  where $D$ and $D'$ are neighboring datasets differing by one individual's data, and $\| \cdot \|_1$ is the $\ell_1$-norm, though other norms can be used depending on the privacy mechanism.

- **Laplace mechanism**: One common technique to achieve $(\epsilon, 0)$-differential privacy is to add noise drawn from the Laplace distribution to the output of a query. The magnitude of this noise is calibrated to the sensitivity of the query:
  $$
  \mathcal{M}(D) = f(D) + \text{Lap}\left(\frac{\Delta f}{\epsilon}\right)
  $$
  where $\text{Lap}(b)$ represents a Laplace distribution with mean zero and scale parameter $b$. The noise is centered around zero, and its variance increases as the sensitivity of the query or the strength of privacy ($\epsilon$) increases.

- **Gaussian mechanism**: For $(\epsilon, \delta)$-differential privacy, noise drawn from a Gaussian distribution can also be used. The amount of noise added is proportional to the sensitivity and depends on both $\epsilon$ and $\delta$:
  $$
  \mathcal{M}(D) = f(D) + \mathcal{N}(0, \sigma^2)
  $$
  where $\sigma$ is determined based on the desired values of $\epsilon$ and $\delta$ and the sensitivity of the function.

### Composition and Privacy Loss

Differential privacy mechanisms enjoy **composition** properties, meaning that applying differentially private mechanisms multiple times over a dataset results in cumulative privacy loss. Specifically:
- **Sequential composition**: If two mechanisms $\mathcal{M}_1$ and $\mathcal{M}_2$ are applied to the same dataset, where $\mathcal{M}_1$ provides $(\epsilon_1, \delta_1)$-DP and $\mathcal{M}_2$ provides $(\epsilon_2, \delta_2)$-DP, the combined mechanism provides $(\epsilon_1 + \epsilon_2, \delta_1 + \delta_2)$-DP.
  
- **Parallel composition**: If mechanisms are applied to disjoint subsets of the data, the total privacy loss is determined by the worst mechanism, i.e., the maximum of the $\epsilon$ and $\delta$ parameters for the individual subsets.

### Advanced Techniques in Differential Privacy

- **Exponential mechanism**: This is useful for non-numeric queries, where the goal is to output a result with a high utility score while maintaining privacy. It selects an output from a range based on a utility function, with probabilities weighted by privacy constraints.

- **Private Aggregation**: Techniques like **Private Multiplicative Weights** or **Stochastic Gradient Descent** can be modified to ensure that machine learning models trained on sensitive data can remain differentially private.

### Applications

Differential privacy has been applied in many real-world contexts where privacy is a concern:
- **Census data**: The U.S. Census Bureau adopted differential privacy for the 2020 census to release aggregate population statistics without compromising individual privacy.
- **Machine learning**: [[Differentially Private SGD]] and other methods ensure that machine learning models do not reveal information about any single training point.
- **Data sharing**: Companies like Apple and Google have used differential privacy to collect usage data from users while protecting individual identities.

### Conclusion

Differential privacy provides a robust mathematical guarantee for privacy, which makes it a widely adopted tool in the analysis of sensitive data. The balance between the strength of privacy (controlled by $\epsilon$ and $\delta$) and the utility of the data is central to its practical application, and the noise addition mechanisms (Laplace, Gaussian) allow for this balance to be fine-tuned in various contexts. 

For more complex algorithms or applications of differential privacy, other specialized techniques such as the [[Exponential Mechanism]] or [[Private Multiplicative Weights]] may also be used.


---


following: [[exponential mechanism]]

See also: [[ethical implications of artificial intelligence balancing innovation and responsibility]]