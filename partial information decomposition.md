---
date: '2025-04-19T16:44:29.771223Z'
tags:
- autogenerated
- source__claude
- conversation_id__a632fe1f-d15e-4da4-814d-fdcec2bbd584
- msg_af8431e7-2243-40e1-baec-192043b87bb0
- msg_65420f6e-0ffd-4bae-a834-f2072588aa55
- colorclass/information-theory
title: partial information decomposition
---

**USER**

partial information decomposition


**ASSISTANT**

# Partial Information Decomposition

## Definition

[[Partial Information Decomposition]] (PID) is a theoretical framework for decomposing multivariate information into distinct and interpretable components representing different forms of information sharing. First proposed by Williams and Beer in 2010, PID addresses the limitations of traditional [[information theory]] measures by providing a more complete characterization of how information is distributed across multiple variables.

For a target variable $Y$ and a set of predictor variables $X = \{X_1, X_2, ..., X_n\}$, PID decomposes the total mutual information $I(Y; X)$ into:

$$I(Y; X) = \sum_{A \subseteq \{1,2,...,n\}} \Pi_Y(A)$$

where $\Pi_Y(A)$ represents the unique information contribution from the subset of variables indexed by $A$.

## Core Components

The PID framework identifies four fundamental types of information for the simplest case of two predictors $X_1$ and $X_2$ and one target $Y$:

1. **[[Unique Information]]** ($UI_1$, $UI_2$): Information that one predictor provides about the target that cannot be obtained from the other predictor
   
   $$UI_1 = I(Y; X_1 | X_2) - SI$$
   $$UI_2 = I(Y; X_2 | X_1) - SI$$

2. **[[Redundant Information]]** ($RI$ or $SI$): Information about the target that is shared by both predictors
   
   $$RI = I(Y; X_1) + I(Y; X_2) - I(Y; X_1, X_2) - SI$$

3. **[[Synergistic Information]]** ($SI$): Information about the target that can only be obtained by considering both predictors together
   
   $$SI = I(Y; X_1, X_2) - I(Y; X_1) - I(Y; X_2) + RI$$

4. **[[Total Mutual Information]]**: Sum of the above components
   
   $$I(Y; X_1, X_2) = UI_1 + UI_2 + RI + SI$$

## Mathematical Challenges

The PID framework faces several mathematical challenges:

1. **[[Redundancy Measure]]**: No universally accepted definition exists for quantifying redundancy between variables
2. **[[Negative Components]]**: Some proposed measures can yield negative values for certain components
3. **[[Lattice Structure]]**: For more than two predictors, the redundancy structure forms a complex lattice
4. **[[Uniqueness of Decomposition]]**: The decomposition is not uniquely defined without additional constraints

## Proposed Measures

Several measures have been proposed to operationalize PID:

1. **[[Specific Information]]** (Williams and Beer): 
   
   $$I_{spec}(y; X) = \sum_{x} p(x|y) \log \frac{p(y|x)}{p(y)}$$

2. **[[Minimum Mutual Information]]** (Barrett):
   
   $$I_{min}(Y; X_1, X_2) = \min \{I(Y; X_1), I(Y; X_2)\}$$

3. **[[Maximum Entropy]]** (Bertschinger et al.):
   
   $$UI_{Bertschinger}(Y; X_1 \backslash X_2) = \min_{Q \in \Delta_p} I_{Q}(Y; X_1 | X_2)$$
   
   where $\Delta_p$ is the set of all distributions $Q$ matching the marginals of $P_{Y,X_2}$.

4. **[[Common Change in Surprisal]]** (Ince):
   
   $$I_{\cap}(Y; X_1, X_2) = \sum_{y,x_1,x_2} p(y,x_1,x_2) \min \{i(y;x_1), i(y;x_2)\}$$

5. **[[Dependency Constraints]]** (James et al.):
   
   Uses copula-based methods to measure unique and redundant information.

## Lattice Structure

For systems with more than two predictors, PID organizes information components into a [[redundancy lattice]]:

- Each node represents a collection of predictor subsets
- Parent nodes contain redundant information common to all child nodes
- The lattice is ordered by inclusion relations
- The number of nodes grows super-exponentially with the number of predictors

The general formula for the number of nodes in an n-predictor lattice is:

$$|\mathcal{L}_n| = \sum_{k=1}^{n} \binom{n}{k} \cdot \text{Stirling2}(k)$$

where $\text{Stirling2}(k)$ is the [[Stirling number of the second kind]].

## Symbolic Notation

For clear representation of PID components, we can adopt the following notation:

- $I_\cap(Y; X_1, X_2, ..., X_n)$: Redundant information shared by all predictors
- $I_\backslash(Y; X_i \backslash X_{-i})$: Unique information provided only by $X_i$
- $I_\Delta(Y; X_1, X_2, ..., X_n)$: Synergistic information available only from all predictors together
- $I_\partial(Y; \alpha)$: Partial information contribution from predictor subset $\alpha$

For specific subsets of three variables:

$$I_\partial(Y; \{1\}\{2\}\{3\})$$: Three-way synergy
$$I_\partial(Y; \{1\}\{2,3\})$$: Unique information in $X_1$ combined with synergy between $X_2$ and $X_3$

## Applications

PID has been applied across multiple fields:

1. **[[Neuroscience]]**:
   - Analyzing neural coding strategies
   - Quantifying information processing in neural networks
   - Studying sensory integration mechanisms

2. **[[Complex Systems]]**:
   - Characterizing emergent behaviors
   - Analyzing self-organization principles
   - Measuring collective information processing

3. **[[Machine Learning]]**:
   - Feature selection based on unique information contribution
   - Understanding deep neural network representations
   - Analyzing ensemble methods through redundancy and synergy

4. **[[Systems Biology]]**:
   - Gene regulatory network analysis
   - Protein-protein interaction mapping
   - Metabolic pathway information flow

5. **[[Social Network Analysis]]**:
   - Quantifying information spread in social networks
   - Identifying influential nodes through unique information
   - Measuring group dynamics through synergy

## Extensions

Several extensions to the basic PID framework have been proposed:

1. **[[Continuous Variables]]**: Extensions beyond discrete variables
2. **[[Dynamic PID]]**: Incorporating temporal aspects and causal information
3. **[[Gaussian PID]]**: Closed-form solutions for multivariate Gaussian distributions
4. **[[Integrated Information Decomposition]]**: Combining with integrated information theory
5. **[[Quantum PID]]**: Extensions to quantum information theory

## Relationship to Other Measures

PID connects to several information-theoretic and statistical measures:

- **[[Interaction Information]]**: $I(X_1; X_2; Y) = SI - RI$
- **[[Co-Information]]**: Generalization of mutual information to multiple variables
- **[[Total Correlation]]**: Measures total dependence between variables
- **[[Dual Total Correlation]]**: Alternative multivariate dependency measure
- **[[Transfer Entropy]]**: Can be decomposed using PID to identify information transfer types
- **[[Conditional Mutual Information]]**: $I(Y; X_1 | X_2) = UI_1 + SI$
- **[[Mutual Information]]**: $I(Y; X_1) = UI_1 + RI$

## Computational Methods

Several computational approaches exist for estimating PID components:

1. **[[Binning Methods]]**: 
   - Discretize continuous data
   - Estimate probabilities from binned frequencies
   - Simple but introduces binning artifacts

2. **[[Kernel Density Estimation]]**:
   - Use kernel functions to estimate probability densities
   - More accurate for continuous data
   - Bandwidth selection impacts results

3. **[[k-Nearest Neighbors]]**:
   - Estimate densities using distances to k-nearest neighbors
   - Adapts to local data density
   - Similar to [[Kraskov estimator]] for mutual information

4. **[[Maximum Entropy Optimization]]**:
   - Define constraints based on observed statistics
   - Find maximum entropy distribution satisfying constraints
   - Computationally intensive for complex constraints

## Open Challenges

Several open challenges remain in PID research:

1. **[[Axiomatic Foundations]]**: Developing consistent axioms for redundancy measures
2. **[[Computational Complexity]]**: Efficient algorithms for high-dimensional systems
3. **[[Statistical Significance]]**: Determining significance thresholds for PID components
4. **[[Causal Interpretation]]**: Relating PID components to causal mechanisms
5. **[[Large-Scale Systems]]**: Scalable approximations for many variables

## References

- Williams, P. L., & Beer, R. D. (2010). Nonnegative decomposition of multivariate information. arXiv preprint arXiv:1004.2515.
- Bertschinger, N., Rauh, J., Olbrich, E., Jost, J., & Ay, N. (2014). Quantifying unique information. Entropy, 16(4), 2161-2183.
- Griffith, V., & Koch, C. (2014). Quantifying synergistic mutual information. In Guided Self-Organization: Inception (pp. 159-190). Springer.
- Timme, N., Alford, W., Flecker, B., & Beggs, J. M. (2014). Synergy, redundancy, and multivariate information measures: an experimentalist's perspective. Journal of Computational Neuroscience, 36(2), 119-140.


---

preceding: [[transfer entropy]]

See also: [[sensory integration]]


See also: [[mutual information]]


See also: [[information decomposition]]