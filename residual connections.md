---
date: 1713751990.687493
tags:
- autogenerated
- source__oai
- conversation_id__5105adc8-e608-40d7-b875-84f3846ef325
- msg_aaa29381-a711-4bee-b3c6-9efd67de940e
- msg_bf0046ff-278b-409e-859d-a39cb1fad8cf
- colorclass/vector-space
title: residual connections
---

**USER**

Residual Connections


**ASSISTANT**

Residual connections, also known as skip connections, are a significant architectural innovation in deep neural networks, particularly in the design of deep convolutional networks. They were first introduced in the context of ResNet (Residual Network) by Kaiming He et al. in their seminal paper "Deep Residual Learning for Image Recognition" (2015). This innovation has since become a fundamental component in many state-of-the-art neural architectures due to its effectiveness in addressing issues related to training very deep networks.

### Purpose of Residual Connections

The primary purpose of residual connections is to alleviate the problem of [[vanishing gradients]]—a common issue in training deep neural networks, where the gradients of the loss function decrease exponentially with the depth of the network, making lower layers very hard to train. Residual connections help maintain a strong gradient flow across layers, thereby enabling the training of networks that are substantially deeper than those that were previously feasible.

### How Residual Connections Work

In a standard neural network layer, the input to a layer is transformed through a function \( F(x) \), typically comprising convolution operations, batch normalization, and activation functions. In a network with residual connections, instead of trying to learn this desired underlying mapping directly, the layer is designed to learn a residual function defined as \( F(x) + x \). Mathematically, this can be expressed as:

$$
\text{output} = \text{activation}(F(x) + x)
$$

Here, \( x \) is the input to the layer, and \( F(x) \) represents the transformation that the network needs to learn. The term \( x \) is added directly to the output of the layer transformation \( F(x) \) before the activation function, effectively allowing the original input \( x \) to "skip" one or more layers.

### Architectural Impact

1. **Easier Optimization**: By adding the input \( x \) back into the output of the layer, the network only needs to learn the residual mapping \( F(x) \). If the identity mapping is optimal (i.e., the input does not need any modification), learning the residual function becomes inherently easier, as \( F(x) \) can approximate zero. This simplification generally leads to faster training and better convergence properties.

2. **Improved Gradient Flow**: During backpropagation, gradients can flow directly through the skip connections across layers without undergoing transformation by the weights and activations of the layers they skip. This direct path prevents the gradients from becoming too small (vanishing) as they propagate back through the network, which is crucial for training deep networks effectively.

3. **Capacity to Train Deeper Networks**: With the help of residual connections, it becomes feasible to train networks that are much deeper than was previously practical, leading to improved model performance on complex tasks due to the network’s increased representational capacity.

### Applications and Extensions

Residual connections have been widely adopted and extended in various ways across many architectures beyond ResNet, such as DenseNet (where each layer connects to every other layer in a feed-forward fashion) and in domains beyond image processing, including natural language processing and audio analysis. Their use has also inspired similar concepts in non-convolutional architectures, like Transformer models, which use residuals around each sub-layer of their encoder and decoder modules.

In conclusion, residual connections represent a simple yet powerful technique to address some of the fundamental challenges in training deep neural networks. By enhancing gradient flow and simplifying the learning objectives of individual layers, they enable the development of deeper and more powerful models across a wide range of applications.


---

preceding: [[geometry of neural network loss landscapes]]  


following: [[relationship between residual connections and the identity function]]

See also: [[relationship between residual connections and the identity function]]


See also: [[neural network gradient flow as a wave propagation through an excitable medium]]


See also: [[theories and philosophies of tero karras]]


See also: [[batch norm]]