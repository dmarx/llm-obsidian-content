---
date: 1715966954.595021
tags:
- autogenerated
- source__oai
- conversation_id__e536633b-8eed-4822-967e-58113d1a33c2
- msg_aaa25338-42b9-4a2d-bfb8-e0dfbc8b95c7
- msg_b85a6446-801f-4ec3-9fa4-1a60bdda929f
- colorclass/vector-space
title: representer theorems
---

**USER**

representer theorems


**ASSISTANT**

## Representer Theorems

Representer theorems are foundational results in the field of [[Reproducing Kernel Hilbert Spaces]] (RKHS) and [[Machine Learning]], particularly in the context of kernel methods. These theorems provide crucial insights into the structure of solutions to certain optimization problems, indicating that optimal solutions can be expressed as finite linear combinations of kernel functions evaluated at the training data points.

### General Statement

The general form of the Representer Theorem can be stated as follows:

*Consider a Hilbert space $\mathcal{H}$ with a reproducing kernel $k: \mathcal{X} \times \mathcal{X} \to \mathbb{R}$, a loss function $L: \mathbb{R}^N \to \mathbb{R}$, and a strictly monotonically increasing regularizer $\Omega: \mathbb{R} \to \mathbb{R}$. If $f \in \mathcal{H}$ minimizes the regularized risk functional:*

$$
J(f) = L(f(x_1), \ldots, f(x_N)) + \Omega(\|f\|_{\mathcal{H}})
$$

*then $f$ can be represented as:*

$$
f(x) = \sum_{i=1}^N \alpha_i k(x, x_i)
$$

*for some coefficients $\alpha_i \in \mathbb{R}$.*

### Detailed Explanation

#### Setting and Notation

- $\mathcal{X}$: Input space.
- $\mathcal{H}$: RKHS with kernel $k(x, z)$.
- $\{(x_i, y_i)\}_{i=1}^N$: Training data points.
- $f \in \mathcal{H}$: Function in the RKHS.
- $L$: Loss function measuring the discrepancy between predictions and actual values.
- $\Omega$: Regularization function ensuring smoothness or penalizing complexity of $f$.

#### Optimization Problem

The optimization problem seeks to find a function $f$ that minimizes the regularized risk functional:

$$
J(f) = L(f(x_1), \ldots, f(x_N)) + \Omega(\|f\|_{\mathcal{H}})
$$

where $\|f\|_{\mathcal{H}}$ denotes the norm of $f$ in the RKHS.

#### Representer Theorem Insight

The representer theorem states that the minimizer $f^*$ of $J(f)$ lies in the span of the kernel functions evaluated at the training points $\{x_i\}_{i=1}^N$. This significantly reduces the complexity of the problem from finding an optimal function in an infinite-dimensional space to finding a finite-dimensional vector $\alpha = (\alpha_1, \ldots, \alpha_N)$.

### Special Cases

#### Ridge Regression

For ridge regression in an RKHS, the problem is:

$$
\min_{f \in \mathcal{H}} \sum_{i=1}^N (y_i - f(x_i))^2 + \lambda \|f\|_{\mathcal{H}}^2
$$

According to the representer theorem, the solution $f^*$ can be written as:

$$
f^*(x) = \sum_{i=1}^N \alpha_i k(x, x_i)
$$

where $\alpha_i$ are coefficients determined by solving the finite-dimensional optimization problem.

#### Support Vector Machines

In the context of SVMs, the optimization problem is:

$$
\min_{f \in \mathcal{H}} \sum_{i=1}^N \max(0, 1 - y_i f(x_i)) + \lambda \|f\|_{\mathcal{H}}^2
$$

Here, the solution $f^*$ again takes the form:

$$
f^*(x) = \sum_{i=1}^N \alpha_i k(x, x_i)
$$

with $\alpha_i$ derived from the dual form of the SVM optimization problem.

### Implications and Applications

The representer theorem underlies many kernel-based learning algorithms, simplifying their implementation by reducing the problem to a finite-dimensional optimization. This includes algorithms such as:

- [[Kernel Ridge Regression]]
- [[Support Vector Machines]]
- [[Gaussian Processes]]

### References

- Sch√∂lkopf, B., Herbrich, R., & Smola, A. J. (2001). *A Generalized Representer Theorem*. In Computational Learning Theory (pp. 416-426). Springer.
- Wahba, G. (1990). *Spline Models for Observational Data*. SIAM.

The representer theorems provide a crucial theoretical foundation for understanding and implementing kernel methods in machine learning, ensuring that solutions to complex optimization problems remain computationally tractable.


---

preceding: [[cover's theorem]]  


following: [[mercer's theorem]]