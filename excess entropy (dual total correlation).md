---
date: '2025-03-06T20:47:04.271784Z'
tags:
- autogenerated
- source__claude
- conversation_id__8f40d55e-8a33-4669-848d-e36693d979f1
- msg_b74f8cc2-2688-42ac-b48c-379f2b1bf93e
- msg_44654452-c0b8-4261-b321-68c9dd97a542
- colorclass/information-theory
title: excess entropy (dual total correlation)
---

**USER**

Excess entropy (dual total correlation)


**ASSISTANT**

# Excess Entropy (Dual Total Correlation)

[[Excess entropy]], also known as [[dual total correlation]] or [[redundancy]], is an information-theoretic measure that quantifies the total amount of multivariate dependence in a set of random variables. It represents an extension of [[mutual information]] to multivariate settings and provides insights into the structure of dependencies that cannot be captured by pairwise measures.

## Mathematical Definition

For a set of random variables $X_1, X_2, \ldots, X_n$, the excess entropy is defined as:

$$E(X_1, X_2, \ldots, X_n) = \sum_{i=1}^{n} H(X_i) - H(X_1, X_2, \ldots, X_n)$$

where:
- $H(X_i)$ is the [[Shannon entropy]] of the $i$-th variable
- $H(X_1, X_2, \ldots, X_n)$ is the [[joint entropy]] of all variables

Alternatively, the excess entropy can be expressed using [[Kullback-Leibler divergence]]:

$$E(X_1, X_2, \ldots, X_n) = D_{KL}\left(p(x_1, x_2, \ldots, x_n) \| \prod_{i=1}^{n} p(x_i)\right)$$

This formulation highlights that excess entropy measures the distance between the joint distribution and the product of the marginal distributions, capturing all forms of statistical dependence.

## Relationship to Other Information Measures

### Total Correlation

The [[total correlation]] (also known as [[multivariate mutual information]] or [[multi-information]]) is identical to excess entropy:

$$C(X_1, X_2, \ldots, X_n) = \sum_{i=1}^{n} H(X_i) - H(X_1, X_2, \ldots, X_n) = E(X_1, X_2, \ldots, X_n)$$

### Mutual Information

For two variables, excess entropy reduces to mutual information:

$$E(X_1, X_2) = H(X_1) + H(X_2) - H(X_1, X_2) = I(X_1; X_2)$$

### Interaction Information

The [[interaction information]] (or [[co-information]]) is related to excess entropy through a recursive relationship:

$$I(X_1; X_2; \ldots; X_n) = \sum_{T \subseteq \{1,2,\ldots,n\}} (-1)^{n-|T|} H(X_T)$$

where $X_T$ represents the joint variable of all $X_i$ with $i \in T$.

### Dual Total Correlation

While "dual total correlation" is often used as a synonym for excess entropy, there is also a distinct measure called the [[dual total correlation]] (or [[binding information]]), defined as:

$$B(X_1, X_2, \ldots, X_n) = H(X_1, X_2, \ldots, X_n) - \sum_{i=1}^{n} H(X_i | X_{\{1,2,\ldots,n\} \setminus \{i\}})$$

This measures the total amount of information that each variable provides about the others collectively.

## Properties

1. **Non-negativity**: $E(X_1, X_2, \ldots, X_n) \geq 0$, with equality if and only if the variables are mutually independent.

2. **Monotonicity**: Adding variables cannot decrease excess entropy:
   $$E(X_1, X_2, \ldots, X_n) \leq E(X_1, X_2, \ldots, X_n, X_{n+1})$$

3. **Chain Rule**: Excess entropy satisfies:
   $$E(X_1, X_2, \ldots, X_n) = \sum_{i=2}^{n} I(X_i; X_1, X_2, \ldots, X_{i-1})$$
   where $I(X_i; X_1, X_2, \ldots, X_{i-1})$ is the mutual information between $X_i$ and the joint variable $(X_1, X_2, \ldots, X_{i-1})$.

4. **Additivity**: For independent sets of variables:
   $$E(X_1, \ldots, X_n, Y_1, \ldots, Y_m) = E(X_1, \ldots, X_n) + E(Y_1, \ldots, Y_m)$$
   if $(X_1, \ldots, X_n)$ is independent of $(Y_1, \ldots, Y_m)$.

5. **Invariance**: Excess entropy is invariant under invertible transformations of individual variables.

## Applications

### Complex Systems Analysis

Excess entropy provides a measure of complexity in various systems:

1. **[[Time Series Analysis]]**: For a stationary process, the excess entropy between past and future represents the predictable information:
   $$E = \lim_{n \to \infty} [H(X_1, X_2, \ldots, X_n) - n h_\mu]$$
   where $h_\mu$ is the [[entropy rate]].

2. **[[Cellular Automata]]**: Excess entropy quantifies the complexity of patterns generated by cellular automata, distinguishing between ordered, complex, and chaotic regimes.

3. **[[Statistical Mechanics]]**: In spin systems, excess entropy measures the information shared across spatial regions, characterizing phase transitions.

### Information Decomposition

Excess entropy plays a key role in the [[partial information decomposition]] (PID) framework, which aims to decompose multivariate information into unique, redundant, and synergistic components:

1. **[[Redundant Information]]**: Information shared among multiple variables
2. **[[Unique Information]]**: Information available only in specific variables
3. **[[Synergistic Information]]**: Information that emerges from the combination of variables

The excess entropy provides constraints on these decompositions.

### Network Theory

In [[complex networks]], excess entropy quantifies:

1. **[[Functional connectivity]]**: Statistical dependencies between network nodes
2. **[[Network complexity]]**: Overall organization beyond pairwise correlations
3. **[[Information flow]]**: Efficiency of information transmission through networks

### Machine Learning

Applications in machine learning include:

1. **[[Feature Selection]]**: Identifying sets of features with minimal redundancy and maximal relevance
2. **[[Representation Learning]]**: Evaluating the information content of learned representations
3. **[[Clustering Evaluation]]**: Assessing the quality of clustering algorithms by measuring dependence structure

## Estimation Methods

### Parametric Estimation

For parametric models:
1. **[[Maximum Likelihood Estimation]]**: Fit distribution parameters and compute entropies analytically
2. **[[Bayesian Estimation]]**: Incorporate prior knowledge of distribution parameters

### Non-parametric Estimation

For empirical data without assuming specific distributions:

1. **[[k-Nearest Neighbors Estimators]]**:
   $$\hat{H}(X) \approx \frac{1}{N} \sum_{i=1}^{N} \log(N \cdot V_{k,i}) + \log 2 + \gamma + \frac{k-1}{k}$$
   where $V_{k,i}$ is the volume of the sphere containing the $k$ nearest neighbors of point $i$.

2. **[[Kernel Density Estimation]]**:
   $$\hat{p}(x) = \frac{1}{N} \sum_{i=1}^{N} K_h(x - x_i)$$
   where $K_h$ is a kernel function with bandwidth $h$.

3. **[[Binning Methods]]**:
   Discretize continuous variables and estimate entropies from empirical frequencies.

### Bias Correction

Excess entropy estimators often suffer from systematic bias, especially for small sample sizes. Methods to address this include:

1. **[[Miller-Madow Correction]]**: Adjusts for the bias in maximum likelihood entropy estimation
2. **[[Jackknife Estimators]]**: Reduce bias through leave-one-out estimates
3. **[[Shrinkage Estimators]]**: Combine empirical estimates with structured models

## Generalizations

### Rényi Excess Entropy

The [[Rényi excess entropy]] of order $\alpha$ generalizes the standard excess entropy:

$$E_\alpha(X_1, \ldots, X_n) = \sum_{i=1}^{n} H_\alpha(X_i) - H_\alpha(X_1, \ldots, X_n)$$

where $H_\alpha$ is the [[Rényi entropy]] of order $\alpha$.

### Tsallis Excess Entropy

Similarly, the [[Tsallis excess entropy]] is defined using [[Tsallis entropy]]:

$$E_q(X_1, \ldots, X_n) = \sum_{i=1}^{n} S_q(X_i) - S_q(X_1, \ldots, X_n)$$

### Quantum Excess Entropy

In [[quantum information theory]], the [[quantum excess entropy]] is:

$$E(\rho_{1,2,\ldots,n}) = \sum_{i=1}^{n} S(\rho_i) - S(\rho_{1,2,\ldots,n})$$

where $S(\rho)$ is the [[von Neumann entropy]] of the density matrix $\rho$.

## Formal Mathematical Framework

For a rigorous treatment of excess entropy, we can define:

- $(\Omega, \mathcal{F}, P)$: Probability space
- $X_i: \Omega \rightarrow \mathcal{X}_i$: Random variables
- $\mathbf{X} = (X_1, X_2, \ldots, X_n)$: Joint random variable
- $p_{\mathbf{X}}$: Joint probability distribution
- $p_{X_i}$: Marginal distributions

Then, excess entropy is:

$$E(\mathbf{X}) = \sum_{i=1}^{n} H(X_i) - H(\mathbf{X}) = \int_{\mathcal{X}_1 \times \cdots \times \mathcal{X}_n} p_{\mathbf{X}}(\mathbf{x}) \log \frac{p_{\mathbf{X}}(\mathbf{x})}{\prod_{i=1}^{n} p_{X_i}(x_i)} d\mathbf{x}$$

For discrete variables, this becomes:

$$E(\mathbf{X}) = \sum_{\mathbf{x} \in \mathcal{X}_1 \times \cdots \times \mathcal{X}_n} p_{\mathbf{X}}(\mathbf{x}) \log \frac{p_{\mathbf{X}}(\mathbf{x})}{\prod_{i=1}^{n} p_{X_i}(x_i)}$$

This formulation emphasizes excess entropy as a measure of the overall statistical dependence in a multivariate system.


---

preceding: [[ising-type models]]

See also: [[information decomposition]]


See also: [[information theory]]


See also: [[random variables]]


See also: [[cellular automata]]