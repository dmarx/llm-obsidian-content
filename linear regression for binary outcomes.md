---
date: '2025-03-09T07:25:36.108516Z'
tags:
- autogenerated
- source__claude
- conversation_id__f5278ef4-a0f7-4b09-b57a-d1033ff3a25c
- msg_c99c7911-5ddc-4e8f-a55b-a1db9f1bb14c
- msg_4eeb7fe1-480d-4143-9834-877f75bbe0b6
- colorclass/philosophy-of-language
title: linear regression for binary outcomes
---

**USER**




**ASSISTANT**

# Linear Regression for Binary Outcomes

## Overview

Linear regression, also known as the linear probability model (LPM), is a statistical approach for analyzing binary outcomes that offers several advantages over nonlinear alternatives like logistic or probit regression when estimating causal effects in experimental settings.

While statisticians have traditionally recommended [[logistic regression]] or [[probit regression]] for binary outcomes, compelling evidence supports linear regression as the optimal approach for estimating [[average treatment effects]] (ATEs) on binary outcomes in experimental and quasi-experimental research.

## Mathematical Formulation

For a binary outcome $Y_i \in \{0,1\}$ and treatment indicator $D_i \in \{0,1\}$, the linear probability model is specified as:

$$Y_i = \beta_0 + \beta_1 D_i + \varepsilon_i$$

Where:
- $Y_i$ is the binary outcome for individual $i$
- $\beta_0$ is the intercept (baseline probability)
- $\beta_1$ represents the ATE (in probability units)
- $\varepsilon_i$ is the error term

In this formulation, $\beta_1$ directly gives the change in probability of $Y_i = 1$ due to treatment.

## Key Advantages

1. **Direct Interpretability**: Coefficients represent probability changes, allowing immediate understanding of effect sizes.

2. **Interaction Term Clarity**: Interaction terms maintain straightforward interpretations, unlike in logistic regression where interactions can vary with other covariates.

3. **Fixed Effects Compatibility**: Linear regression performs reliably with fixed effects for nested data structures, while logistic regression can drop observations and produce biased estimates.

4. **Statistical Equivalence**: Linear and logistic regression yield comparable significance testing results (p-values).

5. **Analytical Simplicity**: Linear regression requires fewer computational steps and assumptions than logistic regression when converting to interpretable probabilities.

## Theoretical Justification

Under the [[Neyman-Rubin causal model]], which frames treatment effects in terms of potential outcomes, the causal effect for individual $i$ can be expressed as:

$$\tau_i = E[Y_{1i} | D_i = 1] - E[Y_{0i} | D_i = 1]$$

Where $Y_{1i}$ is the potential outcome under treatment and $Y_{0i}$ is the potential outcome under control.

In experimental designs with random assignment, $D_i$ and $Y_{0i}$ are independent, making the ATE:

$$\tau_i = E[Y_{1i} - Y_{0i} | D_i = 1] = E[Y_{1i} - Y_{0i}]$$

This demonstrates that for binary outcomes, the causal effect is unbiased and consistent even when estimated through linear regression.

## Common Objections and Responses

### 1. Out-of-Bounds Predictions

**Objection**: Linear regression can predict probabilities outside the [0,1] interval.

**Response**: 
- In simple experimental designs with binary treatments, predictions will never fall outside [0,1].
- In saturated models (fully interacted models with binary/categorical covariates), the underlying structure is inherently linear, preventing out-of-bounds predictions.
- For models with continuous covariates, linear approximation near the center of the distribution is generally adequate.

### 2. Heteroskedasticity

**Objection**: Binary outcomes inherently violate the homoskedasticity assumption of OLS.

**Response**:
- Heteroskedasticity is common even with non-binary outcomes.
- Using [[heteroskedasticity-robust standard errors]] effectively addresses this concern.
- The conditional variance of a binary outcome follows: $Var(Y|X) = X\beta(1 - X\beta)$

## Empirical Evidence

Simulation studies and analyses of real experimental data consistently show that:

1. Both linear and logistic regression estimators are unbiased and consistent for the ATE.
2. Linear and logistic regression produce nearly identical p-values.
3. The correlation between p-values from both approaches exceeds 0.999.

## Best Practices

1. **For Experimental Data**: Use linear regression as the primary analysis method.
2. **For Multiple Regression**: Use robust standard errors to address heteroskedasticity.
3. **With Fixed Effects**: Prefer linear regression over logistic regression.
4. **For Unsaturated Models**: Consider supplementary sensitivity analyses with nonlinear methods.

## Implementation Example

```python
# Simple linear regression for binary outcome
import statsmodels.api as sm

# Fit linear probability model
X = sm.add_constant(treatment)  # treatment is binary 0/1 vector
model = sm.OLS(binary_outcome, X)
results = model.fit(cov_type='HC1')  # Using robust standard errors

# Get average treatment effect and its significance
ate = results.params[1]
p_value = results.pvalues[1]
```

## Conclusion

While nonlinear models remain useful for prediction tasks, linear regression offers significant advantages for estimating causal effects on binary outcomes in experimental psychology research. The direct interpretability of coefficients as probability changes, combined with reliable performance across various model specifications, makes linear regression the recommended approach for analyzing binary outcomes in causal inference contexts.


---


following: [[heteroskedasticity-robust standard errors]]

See also: [[causal effect]]


See also: [[effect size]]