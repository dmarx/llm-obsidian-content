---
date: 1732951090.686
tags:
- autogenerated
- source__oai
- conversation_id__674ab96e-0030-8008-811b-df1a12b1e598
- msg_aaa23f99-0a80-4970-a19d-019da839e46d
- msg_f39fee43-06b7-4812-adf8-e09b64c9d706
- colorclass/information-theory
title: kraft's inequality
---

**USER**

kraft's inequality


**ASSISTANT**

### Kraft's Inequality

**Kraft's Inequality** is a fundamental result in information theory and coding theory that provides a necessary and sufficient condition for the existence of a prefix-free code (or prefix code) with given codeword lengths. It is named after Leon Kraft, who introduced it in 1949.

---

### Statement of Kraft's Inequality

Let \( \{l_1, l_2, \dots, l_n\} \) be the lengths of \( n \) codewords in a prefix-free code over a \( D \)-ary alphabet (where \( D = 2 \) for binary codes). The lengths must satisfy:
$$
\sum_{i=1}^n D^{-l_i} \leq 1.
$$

For binary codes (\( D = 2 \)), this simplifies to:
$$
\sum_{i=1}^n 2^{-l_i} \leq 1.
$$

#### Prefix-Free Codes
A code is **prefix-free** if no codeword is a prefix of another. For example:
- **Valid** prefix-free code: \( \{0, 10, 110\} \)
- **Invalid** prefix-free code: \( \{0, 01, 011\} \) (since \( 0 \) is a prefix of \( 01 \)).

---

### Interpretation

1. **Necessity**
   If a prefix-free code exists with codeword lengths \( \{l_1, l_2, \dots, l_n\} \), then Kraft's inequality must hold. This ensures that the codewords can fit within the constraints of a prefix-free structure.

2. **Sufficiency**
   If \( \{l_1, l_2, \dots, l_n\} \) satisfy Kraft's inequality, there exists a prefix-free code with those lengths. The inequality guarantees that there is enough "space" in the coding tree to assign all the codewords.

---

### Proof (Sketch for Binary Case)

1. **Necessity**
   In a binary tree representing the codewords:
   - Each level corresponds to binary sequences of length \( l \), containing \( 2^l \) possible nodes.
   - A codeword of length \( l_i \) occupies \( 2^{l - l_i} \) nodes at level \( l \).
   - Prefix-free property ensures that no two codewords share nodes.
   - Summing over all codewords:
     $$
     \sum_{i=1}^n 2^{-l_i} \leq 1.
     $$

2. **Sufficiency**
   If \( \sum_{i=1}^n 2^{-l_i} \leq 1 \):
   - Construct a binary tree.
   - Assign codewords iteratively, ensuring that no codeword is a prefix of another.
   - The sum \( 2^{-l_i} \) ensures the tree has enough "capacity" to assign all codewords.

---

### Example: Binary Codes

#### Case 1: Valid Codeword Lengths
Suppose \( \{l_1, l_2, l_3\} = \{1, 2, 3\} \):
$$
\sum_{i=1}^3 2^{-l_i} = 2^{-1} + 2^{-2} + 2^{-3} = \frac{1}{2} + \frac{1}{4} + \frac{1}{8} = \frac{7}{8} \leq 1.
$$
A prefix-free code can be constructed, e.g., \( \{0, 10, 110\} \).

#### Case 2: Invalid Codeword Lengths
Suppose \( \{l_1, l_2, l_3\} = \{1, 2, 2\} \):
$$
\sum_{i=1}^3 2^{-l_i} = 2^{-1} + 2^{-2} + 2^{-2} = \frac{1}{2} + \frac{1}{4} + \frac{1}{4} = 1.
$$
While this satisfies Kraft's inequality, it implies the tree is fully filled, leaving no space for further codewords.

---

### Applications

1. **Shannon Coding**
   Kraft's inequality is used to verify the feasibility of codeword assignments when designing optimal or near-optimal codes based on probability distributions.

2. **Huffman Coding**
   Huffman coding assigns codeword lengths such that they satisfy Kraft's inequality, ensuring prefix-free property and minimum expected length.

3. **Prefix Complexity**
   In [[Kolmogorov Complexity]], Kraft's inequality is implicitly used in defining prefix-free codes, ensuring valid encoding of descriptions.

4. **Data Compression**
   Kraft's inequality underpins the design of compression schemes where prefix-free codes are required for unambiguous decoding.

---

### Generalization to \( D \)-Ary Codes

For \( D \)-ary codes (where the alphabet has \( D \) symbols):
$$
\sum_{i=1}^n D^{-l_i} \leq 1.
$$
This result generalizes binary codes to arbitrary alphabets, providing flexibility for applications in non-binary coding systems.

---

Kraft's inequality elegantly balances the trade-offs in prefix-free coding, forming a cornerstone of both theoretical and practical coding theory.


---

preceding: [[martin-l√∂f randomness]]  


following: [[minimum description length (mdl) principle]]

See also: [[information theory]]


See also: [[coding theory]]